{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytextrank in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (2.0.2)\n",
      "Requirement already satisfied: networkx in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from pytextrank) (2.4)\n",
      "Requirement already satisfied: coverage in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from pytextrank) (5.2)\n",
      "Requirement already satisfied: graphviz in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from pytextrank) (0.14.1)\n",
      "Requirement already satisfied: spacy in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from pytextrank) (2.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from networkx->pytextrank) (4.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (4.46.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (0.7.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (42.0.2.post20191203)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy->pytextrank) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->pytextrank) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pytextrank) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pytextrank) (8.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (42.0.2.post20191203)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.46.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (8.0.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/trevor/anaconda3/envs/NU/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rake_nltk\n",
    "# import pytextrank\n",
    "# import spacy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hluth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "hluth_folder = './data/abstracts/Hulth2003/untitled folder/Training/'\n",
    "hluth_files = os.listdir(hluth_folder)\n",
    "\n",
    "hluth_abstract_files = [x for x in hluth_files if '.abs' in x]\n",
    "hluth_keyword_files = [x for x in hluth_files if '.uncon' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mentors; role models; gender issues; computing; women retention; women\\n\\trecruitment; computer science\\n'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(os.path.join(hluth_folder, hluth_keyword_files[1]))\n",
    "file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "hluth_data = dict()\n",
    "\n",
    "for filenum in [x.split('.')[0] for x in hluth_abstract_files]:\n",
    "    abs_file = [x for x in hluth_abstract_files if (filenum + '.abstr') == x][0]\n",
    "    key_file = [x for x in hluth_keyword_files if (filenum + '.uncontr') == x][0]\n",
    "    \n",
    "    hluth_data[filenum] = dict()\n",
    "    hluth_data[filenum]['abstr'] = open(os.path.join(hluth_folder, abs_file)).read().replace('\\n',' ').replace('\\t', ' ').replace('  ', ' ')\n",
    "    hluth_data[filenum]['keys'] = open(os.path.join(hluth_folder, key_file)).read().replace('\\n',' ').replace('\\t', ' ').replace('  ', ' ').split(';')\n",
    "    \n",
    "#    hluth_data[filenum]['abstr'] = ' '.join([x for x in hluth_data[filenum]['abstr'].lower().split(' ') if x not in stopwords.words('english')])\n",
    "    \n",
    "    while '' in hluth_data[filenum]['keys']:\n",
    "        hluth_data[filenum]['keys'].remove('')\n",
    "        \n",
    "    while ' ' in hluth_data[filenum]['keys']:\n",
    "        hluth_data[filenum]['keys'].remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/abstracts/hluth.data', 'wb') as f:\n",
    "    pickle.dump(hluth_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marujo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "marujo_folder = './data/abstracts/Marujo2012/Training/'\n",
    "marujo_files = os.listdir(marujo_folder)\n",
    "\n",
    "marujo_abstract_files = [x for x in marujo_files if '.txt' in x]\n",
    "marujo_key_files = [x for x in marujo_files if '.key' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marujo_abstract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US',\n",
       " 'lying',\n",
       " 'Mexico',\n",
       " 'more than 16 years',\n",
       " '670pound 300kilogram marijuana shipment',\n",
       " 'kidnapping',\n",
       " 'intercepted',\n",
       " 'third man ObregonOrtiz kidnapped Saucedo under orders of',\n",
       " 'dead',\n",
       " 'wanted to become',\n",
       " 'Prosecutors say ObregonReyes Vega',\n",
       " 'near El Paso two years ago',\n",
       " 'Sept 3 2009',\n",
       " 'prosecutor',\n",
       " \"my cousin Vega's cousin Maria Biddlestone\",\n",
       " 'convicted two men',\n",
       " 'West Texas Federal Court jury found Cesar ObregonReyes',\n",
       " 'assumed name',\n",
       " 'angry',\n",
       " 'American drug trafficker',\n",
       " 'admitted on',\n",
       " 'drug trafficker',\n",
       " 'Mexican drug cartel',\n",
       " 'witness stand that',\n",
       " 'Border Patrol',\n",
       " 'Saucedo',\n",
       " 'jury',\n",
       " 'abducting Sergio Saucedo from his home',\n",
       " 'Defense attorneys argued',\n",
       " 'distributing drugs',\n",
       " 'defend innocent people',\n",
       " \"Saucedo's wife\",\n",
       " 'Rafael Vega guilty',\n",
       " 'defense attorney',\n",
       " 'husband was',\n",
       " 'owners of',\n",
       " 'east of El Paso',\n",
       " 'convicted drug trafficker testifying',\n",
       " 'EL PASO Texas',\n",
       " 'to prison for life',\n",
       " 'appeal',\n",
       " 'Sergio Saucedo',\n",
       " 'US drug trafficker',\n",
       " 'attorney Robert J Perez',\n",
       " 'Juarez',\n",
       " '300kilogram marijuana shipment',\n",
       " 'two men',\n",
       " \"Vega's cousin\",\n",
       " 'Rafael Vega',\n",
       " 'ObregonOrtiz kidnapped Saucedo',\n",
       " 'Cesar ObregonReyes',\n",
       " 'drug war',\n",
       " 'two years ago',\n",
       " '16 years']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(os.path.join(marujo_folder, marujo_key_files[3]))\n",
    "file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "marujo_data = dict()\n",
    "\n",
    "for filname in [x.split('.')[0] for x in marujo_abstract_files]:\n",
    "    abs_file = [x for x in marujo_abstract_files if (filname + '.txt') == x][0]\n",
    "    key_file = [x for x in marujo_key_files if (filname + '.key') == x][0]\n",
    "    \n",
    "    marujo_data[filname] = dict()\n",
    "    marujo_data[filname]['abstr'] = open(os.path.join(marujo_folder, abs_file)).read().replace('\\n',' ').replace('\\t', ' ').replace('  ', ' ')\n",
    "    marujo_data[filname]['keys'] = open(os.path.join(marujo_folder, key_file)).read().replace('  ', ' ').split('\\n')\n",
    "    \n",
    "#     marujo_data[filname]['abstr'] = ' '.join([x for x in marujo_data[filname]['abstr'].lower().split(' ') if x not in stopwords.words('english')])\n",
    "    \n",
    "    while '' in marujo_data[filname]['keys']:\n",
    "        marujo_data[filname]['keys'].remove('')\n",
    "        \n",
    "    while ' ' in marujo_data[filname]['keys']:\n",
    "        marujo_data[filname]['keys'].remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./data/abstracts/marujo.data', 'wb') as f:\n",
    "    pickle.dump(marujo_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "www_folder = './data/abstracts/WWW/WWW/'\n",
    "www_files = os.listdir(www_folder)\n",
    "\n",
    "www_abstract_files = [x for x in www_files if '.txt' in x]\n",
    "www_key_files = [x for x in www_files if '.key' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(www_abstract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miscellaneous', 'social network', 'user interaction', 'web mining', '']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(os.path.join(www_folder, www_key_files[3]))\n",
    "file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "www_data = dict()\n",
    "\n",
    "for filname in [x.split('.')[0] for x in www_abstract_files]:\n",
    "    abs_file = [x for x in www_abstract_files if (filname + '.txt') == x][0]\n",
    "    key_file = [x for x in www_key_files if (filname + '.key') == x][0]\n",
    "    \n",
    "    www_data[filname] = dict()\n",
    "    www_data[filname]['abstr'] = open(os.path.join(www_folder, abs_file)).read().replace('\\n',' ').replace('\\t', ' ').replace('  ', ' ')\n",
    "    www_data[filname]['keys'] = open(os.path.join(www_folder, key_file)).read().replace('  ', ' ').split('\\n')\n",
    "    \n",
    "#     www_data[filname]['abstr'] = ' '.join([x for x in www_data[filname]['abstr'].lower().split(' ') if x not in stopwords.words('english')])\n",
    "    \n",
    "    \n",
    "    while '' in www_data[filname]['keys']:\n",
    "        www_data[filname]['keys'].remove('')\n",
    "        \n",
    "    while ' ' in www_data[filname]['keys']:\n",
    "        www_data[filname]['keys'].remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/abstracts/www.data', 'wb') as f:\n",
    "    pickle.dump(www_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_folder = './data/abstracts/nlm/documents/'\n",
    "nlm_files = os.listdir(nlm_folder)\n",
    "\n",
    "nlm_abstract_files = [x for x in nlm_files if '.txt' in x]\n",
    "nlm_key_files = [x for x in nlm_files if '.key' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlm_abstract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12777364.txt\n",
      "16269586.txt\n",
      "10933267.txt\n",
      "12419768.txt\n",
      "15353412.txt\n"
     ]
    }
   ],
   "source": [
    "nlm_data = dict()\n",
    "\n",
    "for filname in [x.split('.')[0] for x in nlm_abstract_files]:\n",
    "    try:\n",
    "        abs_file = [x for x in nlm_abstract_files if (filname + '.txt') == x][0]\n",
    "        key_file = [x for x in nlm_key_files if (filname + '.key') == x][0]\n",
    "\n",
    "        nlm_data[filname] = dict()\n",
    "\n",
    "        nlm_data[filname]['abstr'] = open(os.path.join(nlm_folder, abs_file)).read().replace('\\n',' ').replace('\\t', ' ').replace('  ', ' ')\n",
    "        nlm_data[filname]['keys'] = open(os.path.join(nlm_folder, key_file)).read().replace('  ', ' ').split('\\n')\n",
    "\n",
    "    #     www_data[filname]['abstr'] = ' '.join([x for x in www_data[filname]['abstr'].lower().split(' ') if x not in stopwords.words('english')])\n",
    "\n",
    "\n",
    "        while '' in nlm_data[filname]['keys']:\n",
    "            nlm_data[filname]['keys'].remove('')\n",
    "\n",
    "        while ' ' in nlm_data[filname]['keys']:\n",
    "            nlm_data[filname]['keys'].remove('')\n",
    "    except:\n",
    "        del nlm_data[filname]\n",
    "        print(abs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/abstracts/hluth.data', 'rb') as file:   \n",
    "    hluth_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/abstracts/www.data', 'rb') as file:   \n",
    "    www_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/abstracts/marujo.data', 'rb') as file:   \n",
    "    marujo_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Control of a heavy-duty robotic excavator using time delay control with integral sliding surface The control of a robotic excavator is difficult from the standpoint of the following problems: parameter variations in mechanical structures, various nonlinearities in hydraulic actuators and disturbance due to the contact with the ground. In addition, the more the size of robotic excavators increases, the more the length and mass of the excavator links; the more the parameters of a heavy-duty excavator vary. A time-delay control with switching action (TDCSA) using an integral sliding surface is proposed in this paper for the control of a 21-ton robotic excavator. Through analysis and experiments, we show that using an integral sliding surface for the switching action of TDCSA is better than using a PD-type sliding surface. The proposed controller is applied to straight-line motions of a 21-ton robotic excavator with a speed level at which skillful operators work. Experiments, which were designed for surfaces with various inclinations and over broad ranges of joint motions, show that the proposed controller exhibits good performance '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hluth_data[list(hluth_data.keys())[0]]['abstr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'control heavy-duty robotic excavator using time delay control integral sliding surface control robotic excavator difficult standpoint following problems: parameter variations mechanical structures, various nonlinearities hydraulic actuators disturbance due contact ground. addition, size robotic excavators increases, length mass excavator links; parameters heavy-duty excavator vary. time-delay control switching action (tdcsa) using integral sliding surface proposed paper control 21-ton robotic excavator. analysis experiments, show using integral sliding surface switching action tdcsa better using pd-type sliding surface. proposed controller applied straight-line motions 21-ton robotic excavator speed level skillful operators work. experiments, designed surfaces various inclinations broad ranges joint motions, show proposed controller exhibits good performance '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([x for x in hluth_data[list(hluth_data.keys())[0]]['abstr'].lower().split(' ') if x not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'control heavy-duty robotic excavator using time delay control integral sliding surface control robotic excavator difficult standpoint following problems: parameter variations mechanical structures, various nonlinearities hydraulic actuators disturbance due contact ground. addition, size robotic excavators increases, length mass excavator links; parameters heavy-duty excavator vary. time-delay control switching action (tdcsa) using integral sliding surface proposed paper control 21-ton robotic excavator. analysis experiments, show using integral sliding surface switching action tdcsa better using pd-type sliding surface. proposed controller applied straight-line motions 21-ton robotic excavator speed level skillful operators work. experiments, designed surfaces various inclinations broad ranges joint motions, show proposed controller exhibits good performance '"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hluth_data[list(hluth_data.keys())[0]]['abstr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type sliding surface',\n",
       " 'skillful operators work',\n",
       " 'integral sliding surface',\n",
       " 'robotic excavators increases',\n",
       " 'duty excavator vary',\n",
       " 'ton robotic excavator',\n",
       " 'robotic excavator',\n",
       " 'excavator links',\n",
       " 'various nonlinearities',\n",
       " 'various inclinations',\n",
       " 'switching action',\n",
       " 'speed level',\n",
       " 'parameter variations',\n",
       " 'mechanical structures',\n",
       " 'line motions',\n",
       " 'joint motions',\n",
       " 'hydraulic actuators',\n",
       " 'following problems',\n",
       " 'disturbance due',\n",
       " 'broad ranges',\n",
       " 'proposed controller',\n",
       " 'delay control',\n",
       " 'proposed',\n",
       " 'control',\n",
       " 'using',\n",
       " 'time',\n",
       " 'tdcsa',\n",
       " 'surfaces',\n",
       " 'straight',\n",
       " 'standpoint',\n",
       " 'size',\n",
       " 'show',\n",
       " 'pd',\n",
       " 'parameters',\n",
       " 'paper',\n",
       " 'mass',\n",
       " 'length',\n",
       " 'heavy',\n",
       " 'ground',\n",
       " 'experiments',\n",
       " 'difficult',\n",
       " 'designed',\n",
       " 'contact',\n",
       " 'better',\n",
       " 'applied',\n",
       " 'analysis',\n",
       " 'addition',\n",
       " '21']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = rake_nltk.Rake()\n",
    "r.max_length = 4\n",
    "r.extract_keywords_from_sentences([hluth_data[list(hluth_data.keys())[0]]['abstr']])\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_textrank(tr, phrases, keys):\n",
    "    \"\"\"This is recall\"\"\"\n",
    "    keys = deepcopy(keys)\n",
    "    num_keyphrases = round(len(tr.ranks) * .3)\n",
    "    \n",
    "    # Some quick cleaning...\n",
    "    keys = [x.replace('-', ' ') for x in keys]\n",
    "    keys = [x.lower() for x in keys]\n",
    "    phrases = [str(x).replace('-', ' ') for x in phrases]\n",
    "    phrases = [x for x in phrases if len(x) > 3]\n",
    "    \n",
    "    num_actual = len(keys)\n",
    "    correct_guesses = 0\n",
    "    term_to_check = 0\n",
    "    \n",
    "    result = dict()\n",
    "    prev_found = list()\n",
    "    \n",
    "    for j in range(num_keyphrases):\n",
    "        result[phrases[j]] = list()\n",
    "        for i in range(len(keys)):\n",
    "            if np.any([phrases[j] in keys[i], phrases[j][:len(phrases[j])-1] in keys[i]]):\n",
    "                result[phrases[j]].append(keys[i])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    found = [v for k, v in result.items()]\n",
    "    unrolled = list()\n",
    "    for outer in found:\n",
    "        for inner in outer:\n",
    "            unrolled.append(inner)\n",
    "            \n",
    "    recall = len(set(unrolled)) / num_actual\n",
    "        \n",
    "    return recall, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rake(phrases, keys):\n",
    "    keys = deepcopy(keys)\n",
    "    \n",
    "    phrases = [str(x).replace('-', ' ') for x in phrases]\n",
    "    phrases = [x for x in phrases if len(x) > 3]\n",
    "    \n",
    "    num_keyphrases = round(len(phrases) * .3)\n",
    "    # Some quick cleaning...\n",
    "    keys = [x.replace('-', ' ') for x in keys]\n",
    "    keys = [x.lower() for x in keys]\n",
    "\n",
    "    \n",
    "    num_actual = len(keys)\n",
    "    correct_guesses = 0\n",
    "    term_to_check = 0\n",
    "    \n",
    "    result = dict()\n",
    "    prev_found = list()\n",
    "    \n",
    "    for j in range(num_keyphrases):\n",
    "        result[phrases[j]] = list()\n",
    "        for i in range(len(keys)):\n",
    "            if np.any([phrases[j] in keys[i], phrases[j][:len(phrases[j])-1] in keys[i]]):\n",
    "                result[phrases[j]].append(keys[i])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    found = [v for k, v in result.items()]\n",
    "    unrolled = list()\n",
    "    for outer in found:\n",
    "        for inner in outer:\n",
    "            unrolled.append(inner)\n",
    "            \n",
    "    recall = len(set(unrolled)) / num_actual\n",
    "        \n",
    "    return recall, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:00<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071979921205412\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(hluth_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(hluth_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=hluth_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(hluth_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 838.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3179080663425533\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for k in tqdm(hluth_data.keys()):\n",
    "    r = rake_nltk.Rake()\n",
    "    r.max_length = 3\n",
    "    r.extract_keywords_from_sentences([hluth_data[k]['abstr']])\n",
    "    \n",
    "    try:\n",
    "        r, details = check_rake(r.get_ranked_phrases(), hluth_data[k]['keys'])\n",
    "    except:\n",
    "        print(r.get_ranked_phrases())\n",
    "        break\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(hluth_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [14:36<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33973678898558707\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(www_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(www_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=www_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(www_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:01<00:00, 1099.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12534480298903375\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for k in tqdm(www_data.keys()):\n",
    "    r = rake_nltk.Rake()\n",
    "    r.max_length = 3\n",
    "    r.extract_keywords_from_sentences([www_data[k]['abstr']])\n",
    "    \n",
    "    try:\n",
    "        r, details = check_rake(r.get_ranked_phrases(), www_data[k]['keys'])\n",
    "    except:\n",
    "        print(r.get_ranked_phrases())\n",
    "        break\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(www_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [06:18<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2643512688048823\n",
      "0.76\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(marujo_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(marujo_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=marujo_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(marujo_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:03<00:00, 133.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15290599275547145\n",
      "0.3333333333333333\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for k in tqdm(marujo_data.keys()):\n",
    "    r = rake_nltk.Rake()\n",
    "    r.max_length = 1\n",
    "    r.extract_keywords_from_sentences([marujo_data[k]['abstr']])\n",
    "    \n",
    "    try:\n",
    "        r, details = check_rake(r.get_ranked_phrases(), marujo_data[k]['keys'])\n",
    "    except:\n",
    "        print(r.get_ranked_phrases())\n",
    "        break\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(marujo_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [23:10<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43611650405166186\n",
      "0.8888888888888888\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlm_data\n",
    "\n",
    "results = list()\n",
    "\n",
    "for k in tqdm(nlm_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(nlm_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=nlm_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(nlm_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [00:07<00:00, 64.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32734494819847976\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for k in tqdm(nlm_data.keys()):\n",
    "    r = rake_nltk.Rake()\n",
    "    r.max_length = 1\n",
    "    r.extract_keywords_from_sentences([nlm_data[k]['abstr']])\n",
    "    \n",
    "    try:\n",
    "        r, details = check_rake(r.get_ranked_phrases(), nlm_data[k]['keys'])\n",
    "    except:\n",
    "        print(r.get_ranked_phrases())\n",
    "        break\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "#         print(doc._.phrases)\n",
    "        print(nlm_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [06:15<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11656195699741018\n",
      "0.4166666666666667\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(marujo_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(marujo_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=marujo_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(marujo_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [14:19<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33973678898558707\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(www_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(www_data[k]['abstr'])\n",
    "    r, details = check_textrank(tr=tr,\n",
    "                       phrases=doc._.phrases,\n",
    "                       keys=www_data[k]['keys'])\n",
    "    \n",
    "    if r > 1:\n",
    "        print(k)\n",
    "        print(doc._.phrases)\n",
    "        print(www_data[k]['keys'])\n",
    "        print(r)\n",
    "        print(details)\n",
    "        \n",
    "        break\n",
    "    results.append(r)\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27.98333333333333, 'duty robotic excavator using time delay control'),\n",
       " (21.166666666666664, 'proposed controller exhibits good performance'),\n",
       " (11.4, 'duty excavator vary'),\n",
       " (10.15, 'ton robotic excavator'),\n",
       " (9.75, 'robotic excavators increases'),\n",
       " (9.0, 'type sliding surface'),\n",
       " (9.0, 'skillful operators work'),\n",
       " (9.0, 'integral sliding surface'),\n",
       " (7.833333333333334, 'delay control'),\n",
       " (7.15, 'robotic excavator'),\n",
       " (6.166666666666666, 'proposed controller'),\n",
       " (5.4, 'excavator links'),\n",
       " (4.0, 'various nonlinearities'),\n",
       " (4.0, 'various inclinations'),\n",
       " (4.0, 'using'),\n",
       " (4.0, 'time'),\n",
       " (4.0, 'switching action'),\n",
       " (4.0, 'speed level'),\n",
       " (4.0, 'parameter variations'),\n",
       " (4.0, 'mechanical structures'),\n",
       " (4.0, 'line motions'),\n",
       " (4.0, 'joint motions'),\n",
       " (4.0, 'hydraulic actuators'),\n",
       " (4.0, 'following problems'),\n",
       " (4.0, 'disturbance due'),\n",
       " (4.0, 'broad ranges'),\n",
       " (3.3333333333333335, 'control'),\n",
       " (2.6666666666666665, 'proposed'),\n",
       " (1.0, 'tdcsa'),\n",
       " (1.0, 'surfaces'),\n",
       " (1.0, 'straight'),\n",
       " (1.0, 'standpoint'),\n",
       " (1.0, 'size'),\n",
       " (1.0, 'show'),\n",
       " (1.0, 'pd'),\n",
       " (1.0, 'parameters'),\n",
       " (1.0, 'paper'),\n",
       " (1.0, 'mass'),\n",
       " (1.0, 'length'),\n",
       " (1.0, 'heavy'),\n",
       " (1.0, 'ground'),\n",
       " (1.0, 'experiments'),\n",
       " (1.0, 'difficult'),\n",
       " (1.0, 'designed'),\n",
       " (1.0, 'contact'),\n",
       " (1.0, 'better'),\n",
       " (1.0, 'applied'),\n",
       " (1.0, 'analysis'),\n",
       " (1.0, 'addition'),\n",
       " (1.0, '21')]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = rake_nltk.Rake()\n",
    "r.extract_keywords_from_sentences([hluth_data[list(hluth_data.keys())[0]]['abstr']])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time-delay control',\n",
       " ' robust control',\n",
       " ' robotic excavator',\n",
       " ' integral sliding surface',\n",
       " ' motion control',\n",
       " ' trajectory control',\n",
       " ' dynamics',\n",
       " ' tracking',\n",
       " ' pressure control ']"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hluth_data[list(hluth_data.keys())[0]]['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'want'),\n",
       " (1.0, 'tasks'),\n",
       " (1.0, 'robots'),\n",
       " (1.0, 'perform'),\n",
       " (1.0, 'know'),\n",
       " (1.0, 'excavation')]"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = rake_nltk.Rake()\n",
    "r.extract_keywords_from_sentences('I want to know more about how robots perform excavation tasks'.split(' '))\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pytextrank.TextRank()\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "doc = nlp('I want to know more about how robots perform excavation tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[excavation tasks, robots, i]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pytextrank.TextRank()\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "doc = nlp(marujo_data['fashion-20872365']['abstr'])\n",
    "r, details = check_textrank(tr=tr,\n",
    "                   phrases=doc._.phrases,\n",
    "                   keys=marujo_data['fashion-20872365']['keys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amazing post baby bikini body denise van outen': [],\n",
       " 'denise van outen': [],\n",
       " 'van outen': [],\n",
       " 'miss van outen': [],\n",
       " 'last may': [],\n",
       " 'last week': [],\n",
       " 'pre pregnancy shape': [],\n",
       " 'denise': [],\n",
       " 'her amazing post baby bikini body': [],\n",
       " 'odd outfit days': [],\n",
       " 'le royal meriden hotel': [],\n",
       " 'tired new mums': [],\n",
       " 'amazing genes': ['amazing genes'],\n",
       " 'le royal meriden': [],\n",
       " 'daughter betsey': [],\n",
       " 'lee mead': [],\n",
       " 'birth': [],\n",
       " 'shape': ['shape'],\n",
       " 'black and white pussycat': [],\n",
       " 'betsy last may. she': [],\n",
       " 'pure package': [],\n",
       " 'yummy mummy': [],\n",
       " 'bow blouse': [],\n",
       " 'healthy meals': [],\n",
       " 'pictures': ['pictures emerged'],\n",
       " 'stumpy legs': ['stumpy legs'],\n",
       " 'gisele bundchen': [],\n",
       " 'a matching white blazer': [],\n",
       " 'dubai': [],\n",
       " 'time': ['time'],\n",
       " 'paulette': [],\n",
       " 'a loud black and white print': [],\n",
       " 'to actor lee mead': [],\n",
       " 'betsey': [],\n",
       " 'others': ['motherofone'],\n",
       " 'envy': [],\n",
       " 'a skimpy paisley print bikini': [],\n",
       " 'the cropped length': [],\n",
       " 'gisele': [],\n",
       " 'today': [],\n",
       " 'peep toe booties': [],\n",
       " 'the odd look': [],\n",
       " 'a black handbag': [],\n",
       " 'musical legally blonde': [],\n",
       " 'a personal trainer': [],\n",
       " 'matchy matchy': []}"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dubai',\n",
       " 'paisleyprint bikini',\n",
       " '36yearold',\n",
       " 'London wearing',\n",
       " 'amazing postbaby bikini body',\n",
       " 'ankles',\n",
       " 'Miss Van Outen',\n",
       " 'Gisele Bundchen',\n",
       " 'Denise Van',\n",
       " 'TV presenter',\n",
       " 'Yummy Mummy',\n",
       " 'Paulette',\n",
       " 'pregnancy weight',\n",
       " 'West End musical Legally Blonde',\n",
       " 'matchymatchy',\n",
       " 'Denise Van Outen',\n",
       " 'motherofone',\n",
       " 'amazing genes',\n",
       " 'daughter Betsey',\n",
       " 'blessed',\n",
       " 'pictures emerged',\n",
       " 'time',\n",
       " 'handbag',\n",
       " 'pregnant',\n",
       " 'look',\n",
       " 'ensemble',\n",
       " 'gave',\n",
       " 'daughter Betsy',\n",
       " 'Outen',\n",
       " 'week',\n",
       " 'pussycat bow',\n",
       " 'white',\n",
       " 'Mead',\n",
       " 'flaunting',\n",
       " 'unflattering outfit',\n",
       " 'Package',\n",
       " 'delivered',\n",
       " 'Denise lapping',\n",
       " 'illusion',\n",
       " 'Denise',\n",
       " 'pregnancy',\n",
       " 'highwaisted trousers',\n",
       " 'prepregnancy',\n",
       " 'workout',\n",
       " 'the chic look',\n",
       " 'postbaby bikini body',\n",
       " 'length',\n",
       " 'peeptoe booties',\n",
       " 'ankle',\n",
       " 'described',\n",
       " 'shape',\n",
       " 'chose',\n",
       " 'loud',\n",
       " 'matching',\n",
       " 'wore',\n",
       " 'lifesaver',\n",
       " 'stumpy legs',\n",
       " 'equivalent',\n",
       " 'skimpy paisleyprint bikini Denise',\n",
       " 'like Gisele Bundchen',\n",
       " 'the sunshine at Le Royal Meriden',\n",
       " 'Betsey',\n",
       " 'mums']"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marujo_data['fashion-20872365']['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:21<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(hluth_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(hluth_data[k]['abstr'])\n",
    "    \n",
    "    results.append(check_textrank(tr=tr,\n",
    "                                  phrases=doc._.phrases,\n",
    "                                  keys=hluth_data[k]['keys']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9718071404931281\n",
      "5.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6409440800979498\n",
      "5.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for k in tqdm(hluth_data.keys()):\n",
    "    tr = pytextrank.TextRank()\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "    doc = nlp(hluth_data[k]['abstr'])\n",
    "    \n",
    "    results.append(check_textrank(tr=tr,\n",
    "                                  phrases=doc._.phrases,\n",
    "                                  keys=hluth_data[k]['keys']))\n",
    "    \n",
    "print(np.mean(results))\n",
    "print(np.max(results))\n",
    "print(np.min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['power system restoration',\n",
       " ' standing phase angle reduction approach',\n",
       " ' circuit breaker closing',\n",
       " ' synchrocheck relay',\n",
       " ' power line connection',\n",
       " ' sensitivity factors ']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hluth_data[list(hluth_data.keys())[1]]['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[basins,\n",
       " a real electrical power system,\n",
       " massively complex electrical power systems,\n",
       " an electrical power system,\n",
       " an electric power system,\n",
       " pathological states,\n",
       " the power system,\n",
       " numerical simulation,\n",
       " basin configuration,\n",
       " particular values,\n",
       " initial conditions,\n",
       " contacts,\n",
       " attractors,\n",
       " this system,\n",
       " the global geometric structure,\n",
       " view,\n",
       " a specific numerical example,\n",
       " part,\n",
       " a six-dimensional dynamical system,\n",
       " a global map]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.phrases[:round(len(tr.ranks) * .3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a new co-operative web caching architecture'.find('co-operative Web caching architecture'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['power system restoration',\n",
       " ' standing phase angle reduction approach',\n",
       " ' circuit breaker closing',\n",
       " ' synchrocheck relay',\n",
       " ' power line connection',\n",
       " ' sensitivity factors ']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hluth_data[list(hluth_data.keys())[1]]['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing phase angle reduction\n",
      "large standing phase angle\n",
      "power system restoration\n",
      "selected buses\n",
      "active power generations\n",
      "excessive spa difference\n",
      "the standing phase angle difference\n",
      "sensitivity factors\n",
      "selected units\n",
      "spa\n",
      "circuit breakers\n",
      "the phase angle\n",
      "the proposed method reschedule generation\n",
      "two specific buses\n",
      "this excessive spa\n",
      "these angles\n",
      "terms\n",
      "closing\n",
      "two buses\n",
      "consumption\n",
      "a new and fast method\n",
      "a tie line\n",
      "the synchro-check relay\n",
      "two systems\n",
      "case\n",
      "the breaker\n",
      "two connected subsystems\n",
      "a line\n",
      "the change\n",
      "this paper\n",
      "this purpose\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "for k in doc._.phrases:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.lemma_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc._.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a heavy-duty excavator'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytextrank.maniacal_scrubber('a heavy-duty excavator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
