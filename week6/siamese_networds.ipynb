{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.2)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.0 pytz-2020.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 92.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 109.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.14.32-py2.py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 70.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.18.0,>=1.17.32\n",
      "  Downloading botocore-1.17.32-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 83.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.32->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 95.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=117753 sha256=3ded132ac1b119b837b1939c1aab6bf323ca2781fc1020ea0f2c3994b163a6f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.14.32 botocore-1.17.32 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.4\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.48.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# from spellchecker import SpellChecker\n",
    "# from tqdm import tqdm\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SemEval-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment\\n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>9993</td>\n",
       "      <td>A door is being opened by a man</td>\n",
       "      <td>A bald man in a band is playing guitar in the ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>9997</td>\n",
       "      <td>Someone is boiling okra in a pot</td>\n",
       "      <td>The man is not playing the drums</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>9998</td>\n",
       "      <td>The man is singing heartily and playing the gu...</td>\n",
       "      <td>A bicyclist is holding a bike over his head in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>9999</td>\n",
       "      <td>A man in blue has a yellow ball in the mitt</td>\n",
       "      <td>A man is jumping rope outside</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>10000</td>\n",
       "      <td>Three dogs are resting on a sidewalk</td>\n",
       "      <td>The woman with a knife is slicing a pepper</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pair_ID                                         sentence_A  \\\n",
       "4495    9993                    A door is being opened by a man   \n",
       "4496    9997                   Someone is boiling okra in a pot   \n",
       "4497    9998  The man is singing heartily and playing the gu...   \n",
       "4498    9999        A man in blue has a yellow ball in the mitt   \n",
       "4499   10000               Three dogs are resting on a sidewalk   \n",
       "\n",
       "                                             sentence_B  relatedness_score  \\\n",
       "4495  A bald man in a band is playing guitar in the ...                1.1   \n",
       "4496                   The man is not playing the drums                1.0   \n",
       "4497  A bicyclist is holding a bike over his head in...                1.0   \n",
       "4498                      A man is jumping rope outside                1.2   \n",
       "4499         The woman with a knife is slicing a pepper                1.0   \n",
       "\n",
       "     entailment_judgment\\n  \n",
       "4495             NEUTRAL\\n  \n",
       "4496             NEUTRAL\\n  \n",
       "4497             NEUTRAL\\n  \n",
       "4498             NEUTRAL\\n  \n",
       "4499             NEUTRAL\\n  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"LOADING DATA\"\"\"\n",
    "file = open('./data/semeval.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "\n",
    "file = open('./data/semeval.txt', 'r')\n",
    "data = list()\n",
    "for line in file:\n",
    "    data.append(line.split('\\t'))\n",
    "data = data[1:]\n",
    "\n",
    "data = pd.DataFrame(data, columns=headers)\n",
    "data['relatedness_score'] = pd.to_numeric(data['relatedness_score'])\n",
    "\n",
    "file = open('./data/semeval_train.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "\n",
    "file = open('./data/semeval_train.txt', 'r')\n",
    "data_train = list()\n",
    "for line in file:\n",
    "    data_train.append(line.split('\\t'))\n",
    "data_train = data_train[1:]\n",
    "\n",
    "data_train = pd.DataFrame(data_train, columns=headers)\n",
    "data_train['relatedness_score'] = pd.to_numeric(data_train['relatedness_score'])\n",
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person on a black motorbike is doing tricks with a jacket\n",
      "A person is riding the bicycle on one wheel\n"
     ]
    }
   ],
   "source": [
    "print(data_train.iloc[10]['sentence_A'])\n",
    "print(data_train.iloc[10]['sentence_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) \n",
    "\n",
    "data['normed_score'] = norm(data['relatedness_score'])\n",
    "data_train['normed_score'] = norm(data_train['relatedness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentence_A'] = [x.lower() for x in data['sentence_A']]\n",
    "data['sentence_B'] = [x.lower() for x in data['sentence_B']]\n",
    "data_train['sentence_A'] = [x.lower() for x in data_train['sentence_A']]\n",
    "data_train['sentence_B'] = [x.lower() for x in data_train['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec model, pretrained\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/enwiki_20180420_300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 32 is the longest sentence. so let's pad all with [0,..., 0] until len()==32\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(data_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in data_train['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in data_train['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 32:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 32:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TESTING DATA\"\"\"\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(data)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in data['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in data['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 32:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 32:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "    \n",
    "with open('./data/test_a_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pickle.load(open('./data/train_a_w2v300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_w2v300.data', 'rb'))\n",
    "test_a = pickle.load(open('./data/test_a_w2v300.data', 'rb'))\n",
    "test_b = pickle.load(open('./data/test_b_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=2.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0220\n",
      "Epoch 2/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0223\n",
      "Epoch 3/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0222\n",
      "Epoch 4/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0218\n",
      "Epoch 5/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0220\n",
      "Epoch 6/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0220\n",
      "Epoch 7/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0224\n",
      "Epoch 8/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0219\n",
      "Epoch 9/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0216\n",
      "Epoch 10/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0221\n",
      "Epoch 11/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0220\n",
      "Epoch 12/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0217\n",
      "Epoch 13/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0215\n",
      "Epoch 14/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0216\n",
      "Epoch 15/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0216\n",
      "Epoch 16/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0216\n",
      "Epoch 17/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0221\n",
      "Epoch 18/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0216\n",
      "Epoch 19/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0211\n",
      "Epoch 20/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0216\n",
      "Epoch 21/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0215\n",
      "Epoch 22/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0215\n",
      "Epoch 23/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0211\n",
      "Epoch 24/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0211\n",
      "Epoch 25/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0205\n",
      "Epoch 26/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0208\n",
      "Epoch 27/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0211\n",
      "Epoch 28/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0211\n",
      "Epoch 29/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0213\n",
      "Epoch 30/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0213\n",
      "Epoch 31/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0210\n",
      "Epoch 32/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0212\n",
      "Epoch 33/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0207\n",
      "Epoch 34/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0212\n",
      "Epoch 35/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0204\n",
      "Epoch 36/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0205\n",
      "Epoch 37/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0208\n",
      "Epoch 38/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0208\n",
      "Epoch 39/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0197\n",
      "Epoch 40/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0209\n",
      "Epoch 41/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0204\n",
      "Epoch 42/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0205\n",
      "Epoch 43/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0203\n",
      "Epoch 44/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0210\n",
      "Epoch 45/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0198\n",
      "Epoch 46/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0198\n",
      "Epoch 47/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0199\n",
      "Epoch 48/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0202\n",
      "Epoch 49/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0199\n",
      "Epoch 50/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0210\n",
      "Epoch 51/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0200\n",
      "Epoch 52/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0201\n",
      "Epoch 53/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0201\n",
      "Epoch 54/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0198\n",
      "Epoch 55/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0202\n",
      "Epoch 56/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0205\n",
      "Epoch 57/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0204\n",
      "Epoch 58/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0200\n",
      "Epoch 59/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0199\n",
      "Epoch 60/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 61/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0198\n",
      "Epoch 62/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0203\n",
      "Epoch 63/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 64/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0193\n",
      "Epoch 65/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 66/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0200\n",
      "Epoch 67/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0201\n",
      "Epoch 68/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0193\n",
      "Epoch 69/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 70/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 71/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 72/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0194\n",
      "Epoch 73/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0192\n",
      "Epoch 74/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0199\n",
      "Epoch 75/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 76/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0202\n",
      "Epoch 77/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 78/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 79/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0196\n",
      "Epoch 80/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 81/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 82/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0190\n",
      "Epoch 83/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 84/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0197\n",
      "Epoch 85/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0196\n",
      "Epoch 86/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0192\n",
      "Epoch 87/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0190\n",
      "Epoch 88/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0192\n",
      "Epoch 89/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0193\n",
      "Epoch 90/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 91/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0182\n",
      "Epoch 92/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0190\n",
      "Epoch 93/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0188\n",
      "Epoch 94/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 95/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0192\n",
      "Epoch 96/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0194\n",
      "Epoch 97/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0187\n",
      "Epoch 98/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0190\n",
      "Epoch 99/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0186\n",
      "Epoch 100/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0189\n",
      "Epoch 101/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0186\n",
      "Epoch 102/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0192\n",
      "Epoch 103/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 104/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0188\n",
      "Epoch 105/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0188\n",
      "Epoch 106/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0187\n",
      "Epoch 107/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0186\n",
      "Epoch 108/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0189\n",
      "Epoch 109/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0188\n",
      "Epoch 110/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0190\n",
      "Epoch 111/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0187\n",
      "Epoch 112/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0186\n",
      "Epoch 113/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0181\n",
      "Epoch 114/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0184\n",
      "Epoch 115/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0178\n",
      "Epoch 116/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 117/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0187\n",
      "Epoch 118/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0183\n",
      "Epoch 119/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0185\n",
      "Epoch 120/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0188\n",
      "Epoch 121/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0186\n",
      "Epoch 122/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0183\n",
      "Epoch 123/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0182\n",
      "Epoch 124/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0181\n",
      "Epoch 125/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0182\n",
      "Epoch 126/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 127/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0177\n",
      "Epoch 128/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0184\n",
      "Epoch 129/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0187\n",
      "Epoch 130/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 131/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0182\n",
      "Epoch 132/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0181\n",
      "Epoch 133/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 134/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0181\n",
      "Epoch 135/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0178\n",
      "Epoch 136/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0183\n",
      "Epoch 137/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 138/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0182\n",
      "Epoch 139/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0180\n",
      "Epoch 140/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0182\n",
      "Epoch 141/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0175\n",
      "Epoch 142/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0179\n",
      "Epoch 143/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0182\n",
      "Epoch 144/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0179\n",
      "Epoch 145/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0182\n",
      "Epoch 146/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 147/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0184\n",
      "Epoch 148/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0175\n",
      "Epoch 149/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 150/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 151/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 152/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 153/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0175\n",
      "Epoch 154/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0173\n",
      "Epoch 155/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0178\n",
      "Epoch 156/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0178\n",
      "Epoch 157/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0171\n",
      "Epoch 158/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 159/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0179\n",
      "Epoch 160/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0178\n",
      "Epoch 161/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0176\n",
      "Epoch 162/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0177\n",
      "Epoch 163/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0173\n",
      "Epoch 164/300\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 165/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0170\n",
      "Epoch 166/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0173\n",
      "Epoch 167/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0178\n",
      "Epoch 168/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0179\n",
      "Epoch 169/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 170/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0173\n",
      "Epoch 171/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0174\n",
      "Epoch 172/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0173\n",
      "Epoch 173/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0170\n",
      "Epoch 174/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0176\n",
      "Epoch 175/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0173\n",
      "Epoch 176/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0172\n",
      "Epoch 177/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0169\n",
      "Epoch 178/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0174\n",
      "Epoch 179/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0171\n",
      "Epoch 180/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0169\n",
      "Epoch 181/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0173\n",
      "Epoch 182/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0172\n",
      "Epoch 183/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0171\n",
      "Epoch 184/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0170\n",
      "Epoch 185/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0179\n",
      "Epoch 186/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0173\n",
      "Epoch 187/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0172\n",
      "Epoch 188/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0169\n",
      "Epoch 189/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 190/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0172\n",
      "Epoch 191/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 192/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 193/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0170\n",
      "Epoch 194/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 195/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 196/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0175\n",
      "Epoch 197/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0176\n",
      "Epoch 198/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 199/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0171\n",
      "Epoch 200/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0168\n",
      "Epoch 201/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0169\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0170\n",
      "Epoch 203/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 204/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0173\n",
      "Epoch 205/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0167\n",
      "Epoch 206/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0169\n",
      "Epoch 207/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0168\n",
      "Epoch 208/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0168\n",
      "Epoch 209/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0167\n",
      "Epoch 210/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 211/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 212/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 213/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0171\n",
      "Epoch 214/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 215/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 216/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0174\n",
      "Epoch 217/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 218/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 219/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0172\n",
      "Epoch 220/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 221/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 222/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0168\n",
      "Epoch 223/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0166\n",
      "Epoch 224/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0166\n",
      "Epoch 225/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 226/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0168\n",
      "Epoch 227/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 228/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0163\n",
      "Epoch 229/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0162\n",
      "Epoch 230/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 231/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0160\n",
      "Epoch 232/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 233/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0161\n",
      "Epoch 234/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 235/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 236/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 237/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 238/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 239/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 240/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0160\n",
      "Epoch 241/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 242/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0162\n",
      "Epoch 243/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 244/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 245/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0162\n",
      "Epoch 246/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 247/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 248/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 249/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 250/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0161\n",
      "Epoch 251/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 252/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 253/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0161\n",
      "Epoch 254/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 255/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0160\n",
      "Epoch 256/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 257/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 258/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0163\n",
      "Epoch 259/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0166\n",
      "Epoch 260/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 261/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0163\n",
      "Epoch 262/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 263/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0163\n",
      "Epoch 264/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 265/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0161\n",
      "Epoch 266/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0164\n",
      "Epoch 267/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0162\n",
      "Epoch 268/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0160\n",
      "Epoch 269/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 270/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 271/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0161\n",
      "Epoch 272/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 273/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0165\n",
      "Epoch 274/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0165\n",
      "Epoch 275/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 276/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0159\n",
      "Epoch 277/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 278/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0160\n",
      "Epoch 279/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0162\n",
      "Epoch 280/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0159\n",
      "Epoch 281/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 282/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 283/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0156\n",
      "Epoch 284/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0162\n",
      "Epoch 285/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 286/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 287/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0158\n",
      "Epoch 288/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0162\n",
      "Epoch 289/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 290/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0155\n",
      "Epoch 291/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 292/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0159\n",
      "Epoch 293/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0156\n",
      "Epoch 294/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0155\n",
      "Epoch 295/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0159\n",
      "Epoch 296/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0157\n",
      "Epoch 297/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0156\n",
      "Epoch 298/300\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0155\n",
      "Epoch 299/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0160\n",
      "Epoch 300/300\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1837ecd5f8>"
      ]
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(data_train['normed_score']), \n",
    "                epochs=300, \n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.9245687911527701\n",
      "Spearmans: 0.8779700511318327\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], data_train['normed_score'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], data_train['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.8177292527791502\n",
      "Spearmans: 0.7678779257390479\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "\n",
    "gm = loadGloveModel('./models/glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(data_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in data_train['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in data_train['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 32:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 32:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_semeval_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_semeval_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(data)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in data['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in data['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 32:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 32:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_semeval_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_images_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pickle.load(open('./data/train_a_semeval_glove300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_semeval_glove300.data', 'rb'))\n",
    "test_a = pickle.load(open('./data/test_a_semeval_glove300.data', 'rb'))\n",
    "test_b = pickle.load(open('./data/test_b_images_glove300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(256, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=1.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.3050\n",
      "Epoch 2/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.1462\n",
      "Epoch 3/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.1135\n",
      "Epoch 4/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0957\n",
      "Epoch 5/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0871\n",
      "Epoch 6/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0800\n",
      "Epoch 7/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0761\n",
      "Epoch 8/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0728\n",
      "Epoch 9/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0706\n",
      "Epoch 10/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0687\n",
      "Epoch 11/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0670\n",
      "Epoch 12/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0659\n",
      "Epoch 13/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0647\n",
      "Epoch 14/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0637\n",
      "Epoch 15/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0624\n",
      "Epoch 16/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0601\n",
      "Epoch 17/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0571\n",
      "Epoch 18/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0550\n",
      "Epoch 19/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0529\n",
      "Epoch 20/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0514\n",
      "Epoch 21/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0499\n",
      "Epoch 22/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0476\n",
      "Epoch 23/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0462\n",
      "Epoch 24/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0456\n",
      "Epoch 25/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0449\n",
      "Epoch 26/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0438\n",
      "Epoch 27/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0430\n",
      "Epoch 28/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0422\n",
      "Epoch 29/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0418\n",
      "Epoch 30/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0410\n",
      "Epoch 31/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0405\n",
      "Epoch 32/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0398\n",
      "Epoch 33/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0389\n",
      "Epoch 34/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0392\n",
      "Epoch 35/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0380\n",
      "Epoch 36/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0374\n",
      "Epoch 37/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0362\n",
      "Epoch 38/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0355\n",
      "Epoch 39/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0356\n",
      "Epoch 40/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0342\n",
      "Epoch 41/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0335\n",
      "Epoch 42/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0336\n",
      "Epoch 43/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0325\n",
      "Epoch 44/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0318\n",
      "Epoch 45/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0315\n",
      "Epoch 46/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0309\n",
      "Epoch 47/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0301\n",
      "Epoch 48/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0297\n",
      "Epoch 49/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0288\n",
      "Epoch 50/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0289\n",
      "Epoch 51/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0283\n",
      "Epoch 52/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0285\n",
      "Epoch 53/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0278\n",
      "Epoch 54/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0269\n",
      "Epoch 55/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0275\n",
      "Epoch 56/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0265\n",
      "Epoch 57/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0264\n",
      "Epoch 58/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0256\n",
      "Epoch 59/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0256\n",
      "Epoch 60/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0251\n",
      "Epoch 61/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0239\n",
      "Epoch 62/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0242\n",
      "Epoch 63/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0244\n",
      "Epoch 64/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0240\n",
      "Epoch 65/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0236\n",
      "Epoch 66/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0230\n",
      "Epoch 67/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0231\n",
      "Epoch 68/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0223\n",
      "Epoch 69/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0226\n",
      "Epoch 70/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0222\n",
      "Epoch 71/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0215\n",
      "Epoch 72/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0213\n",
      "Epoch 73/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0217\n",
      "Epoch 74/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0214\n",
      "Epoch 75/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0212\n",
      "Epoch 76/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0206\n",
      "Epoch 77/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0209\n",
      "Epoch 78/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0205\n",
      "Epoch 79/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0205\n",
      "Epoch 80/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0204\n",
      "Epoch 81/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0203\n",
      "Epoch 82/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0203\n",
      "Epoch 83/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0201\n",
      "Epoch 84/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0200\n",
      "Epoch 85/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 86/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0197\n",
      "Epoch 87/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0196\n",
      "Epoch 88/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0189\n",
      "Epoch 89/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0191\n",
      "Epoch 90/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0189\n",
      "Epoch 91/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0188\n",
      "Epoch 92/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0192\n",
      "Epoch 93/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0188\n",
      "Epoch 94/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0186\n",
      "Epoch 95/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0188\n",
      "Epoch 96/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0186\n",
      "Epoch 97/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0181\n",
      "Epoch 98/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0180\n",
      "Epoch 99/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0180\n",
      "Epoch 100/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0181\n",
      "Epoch 101/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0180\n",
      "Epoch 102/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0178\n",
      "Epoch 103/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0179\n",
      "Epoch 104/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0175\n",
      "Epoch 105/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0177\n",
      "Epoch 106/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0177\n",
      "Epoch 107/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0174\n",
      "Epoch 108/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0172\n",
      "Epoch 109/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0176\n",
      "Epoch 110/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0172\n",
      "Epoch 111/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0170\n",
      "Epoch 112/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0169\n",
      "Epoch 113/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0169\n",
      "Epoch 114/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0167\n",
      "Epoch 115/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 116/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 117/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0168\n",
      "Epoch 118/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0163\n",
      "Epoch 119/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0164\n",
      "Epoch 120/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 121/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 122/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0159\n",
      "Epoch 123/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0162\n",
      "Epoch 124/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0154\n",
      "Epoch 125/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0159\n",
      "Epoch 126/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0158\n",
      "Epoch 127/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0159\n",
      "Epoch 128/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0153\n",
      "Epoch 129/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0155\n",
      "Epoch 130/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0152\n",
      "Epoch 131/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0157\n",
      "Epoch 132/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0154\n",
      "Epoch 133/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0157\n",
      "Epoch 134/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0153\n",
      "Epoch 135/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0154\n",
      "Epoch 136/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0152\n",
      "Epoch 137/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0151\n",
      "Epoch 138/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0149\n",
      "Epoch 139/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0152\n",
      "Epoch 140/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0150\n",
      "Epoch 141/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0149\n",
      "Epoch 142/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0148\n",
      "Epoch 143/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0150\n",
      "Epoch 144/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0150\n",
      "Epoch 145/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0146\n",
      "Epoch 146/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0148\n",
      "Epoch 147/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0148\n",
      "Epoch 148/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0144\n",
      "Epoch 149/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0145\n",
      "Epoch 150/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0141\n",
      "Epoch 151/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0142\n",
      "Epoch 152/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0143\n",
      "Epoch 153/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0143\n",
      "Epoch 154/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0143\n",
      "Epoch 155/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0140\n",
      "Epoch 156/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0142\n",
      "Epoch 157/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0138\n",
      "Epoch 158/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0142\n",
      "Epoch 159/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0140\n",
      "Epoch 160/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0140\n",
      "Epoch 161/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0138\n",
      "Epoch 162/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0141\n",
      "Epoch 163/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0136\n",
      "Epoch 164/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0136\n",
      "Epoch 165/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0137\n",
      "Epoch 166/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0136\n",
      "Epoch 167/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 168/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0134\n",
      "Epoch 169/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0137\n",
      "Epoch 170/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0134\n",
      "Epoch 171/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0137\n",
      "Epoch 172/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 173/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 174/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0138\n",
      "Epoch 175/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0131\n",
      "Epoch 176/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0131\n",
      "Epoch 177/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0137\n",
      "Epoch 178/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 179/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 180/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0132\n",
      "Epoch 181/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0134\n",
      "Epoch 182/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0131\n",
      "Epoch 183/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0133\n",
      "Epoch 184/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0136\n",
      "Epoch 185/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0128\n",
      "Epoch 186/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0129\n",
      "Epoch 187/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0132\n",
      "Epoch 188/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0128\n",
      "Epoch 189/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0128\n",
      "Epoch 190/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0129\n",
      "Epoch 191/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0130\n",
      "Epoch 192/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0130\n",
      "Epoch 193/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0126\n",
      "Epoch 194/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0130\n",
      "Epoch 195/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 196/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0126\n",
      "Epoch 197/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0127\n",
      "Epoch 198/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0129\n",
      "Epoch 199/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0124\n",
      "Epoch 200/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 201/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 202/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0127\n",
      "Epoch 203/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 204/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0128\n",
      "Epoch 205/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 206/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0123\n",
      "Epoch 207/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0125\n",
      "Epoch 208/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0127\n",
      "Epoch 209/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0126\n",
      "Epoch 210/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0122\n",
      "Epoch 211/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0124\n",
      "Epoch 212/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0121\n",
      "Epoch 213/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0121\n",
      "Epoch 214/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0121\n",
      "Epoch 215/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0123\n",
      "Epoch 216/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0119\n",
      "Epoch 217/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0123\n",
      "Epoch 218/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0122\n",
      "Epoch 219/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0122\n",
      "Epoch 220/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0119\n",
      "Epoch 221/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0120\n",
      "Epoch 222/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0120\n",
      "Epoch 223/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0119\n",
      "Epoch 224/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0120\n",
      "Epoch 225/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0119\n",
      "Epoch 226/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0114\n",
      "Epoch 227/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0120\n",
      "Epoch 228/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0119\n",
      "Epoch 229/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0122\n",
      "Epoch 230/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0118\n",
      "Epoch 231/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0116\n",
      "Epoch 232/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0121\n",
      "Epoch 233/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0116\n",
      "Epoch 234/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0120\n",
      "Epoch 235/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0117\n",
      "Epoch 236/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0117\n",
      "Epoch 237/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0114\n",
      "Epoch 238/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0112\n",
      "Epoch 239/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 240/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0117\n",
      "Epoch 241/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0115\n",
      "Epoch 242/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 243/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0112\n",
      "Epoch 244/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0114\n",
      "Epoch 245/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0114\n",
      "Epoch 246/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 247/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 248/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0115\n",
      "Epoch 249/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 250/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 251/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 252/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 253/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 254/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0116\n",
      "Epoch 255/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 256/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 257/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0115\n",
      "Epoch 258/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 259/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0109\n",
      "Epoch 260/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0113\n",
      "Epoch 261/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 262/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0112\n",
      "Epoch 263/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 264/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 265/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 266/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0112\n",
      "Epoch 267/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 268/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0110\n",
      "Epoch 269/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0108\n",
      "Epoch 270/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 271/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 272/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 273/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 274/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0109\n",
      "Epoch 275/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0108\n",
      "Epoch 276/700\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0109\n",
      "Epoch 277/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0107\n",
      "Epoch 278/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0108\n",
      "Epoch 279/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0107\n",
      "Epoch 280/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0111\n",
      "Epoch 281/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0107\n",
      "Epoch 282/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0107\n",
      "Epoch 283/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 284/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 285/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 286/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 287/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 288/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 289/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 290/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0108\n",
      "Epoch 291/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 292/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0109\n",
      "Epoch 293/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 294/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 295/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 296/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 297/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0106\n",
      "Epoch 298/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 299/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0104\n",
      "Epoch 300/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 301/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 302/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0103\n",
      "Epoch 303/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0104\n",
      "Epoch 304/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 305/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0103\n",
      "Epoch 306/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 307/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 308/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0105\n",
      "Epoch 309/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0103\n",
      "Epoch 310/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 311/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 312/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 313/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0104\n",
      "Epoch 314/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 315/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0103\n",
      "Epoch 316/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 317/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 318/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 319/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 320/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 321/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0103\n",
      "Epoch 322/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 323/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 324/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 325/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 326/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 327/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 328/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 329/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 330/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 331/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 332/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 333/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 334/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0102\n",
      "Epoch 335/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 336/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 337/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 338/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0100\n",
      "Epoch 339/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 340/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0101\n",
      "Epoch 341/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 342/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 343/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 344/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 345/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 346/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 347/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 348/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 349/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 350/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 351/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 352/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 353/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 354/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 355/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 356/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 357/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 358/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 359/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 360/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 361/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0097\n",
      "Epoch 362/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 363/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 364/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 365/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 366/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 367/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 368/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0099\n",
      "Epoch 369/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0096\n",
      "Epoch 370/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0098\n",
      "Epoch 371/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 372/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 373/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 374/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 375/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 376/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 377/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 378/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 379/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 380/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 381/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 382/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 383/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 384/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 385/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 386/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 387/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0094\n",
      "Epoch 388/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0092\n",
      "Epoch 389/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0090\n",
      "Epoch 390/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 391/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 392/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 393/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0090\n",
      "Epoch 394/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 395/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 396/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 397/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 398/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 399/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 400/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0093\n",
      "Epoch 401/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 402/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 403/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 404/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 405/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0092\n",
      "Epoch 406/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0092\n",
      "Epoch 407/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 408/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0090\n",
      "Epoch 409/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0095\n",
      "Epoch 410/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 411/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 412/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 413/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0090\n",
      "Epoch 414/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 415/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 416/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0092\n",
      "Epoch 417/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 418/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 419/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 420/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 421/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 422/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 423/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 424/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 425/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 426/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 427/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 428/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 429/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 430/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 431/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 432/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 433/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0091\n",
      "Epoch 434/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 435/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 436/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 437/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 438/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 439/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0088\n",
      "Epoch 440/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 441/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 442/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 443/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 444/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0089\n",
      "Epoch 445/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 446/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 447/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 448/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 449/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 450/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 451/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 452/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 453/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 454/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 455/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 456/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 457/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 458/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 459/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 460/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 461/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 462/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 463/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0086\n",
      "Epoch 464/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 465/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 466/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 467/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 468/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 469/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 470/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 471/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 472/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 473/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 474/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 475/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 476/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0087\n",
      "Epoch 477/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 478/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 479/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 480/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 481/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 482/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 483/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 484/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 485/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0085\n",
      "Epoch 486/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 487/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 488/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 489/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 490/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 491/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 492/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 493/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 494/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 495/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 496/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 497/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 498/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 499/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0084\n",
      "Epoch 500/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 501/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 502/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 503/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 504/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 505/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 506/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 507/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0083\n",
      "Epoch 508/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 509/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 510/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 511/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 512/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 513/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 514/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 515/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 516/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 517/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 518/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 519/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 520/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 521/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0082\n",
      "Epoch 522/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 523/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 524/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 525/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 526/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 527/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 528/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 529/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 530/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 531/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 532/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 533/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 534/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 535/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 536/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 537/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 538/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 539/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 540/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0081\n",
      "Epoch 541/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 542/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 543/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 544/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 545/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 546/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 547/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0080\n",
      "Epoch 548/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 549/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 550/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 551/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 552/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 553/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 554/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 555/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0076\n",
      "Epoch 556/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 557/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 558/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 559/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 560/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0079\n",
      "Epoch 561/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0077\n",
      "Epoch 562/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 563/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0078\n",
      "Epoch 564/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0077\n",
      "Epoch 565/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 566/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0076\n",
      "Epoch 567/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 568/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 569/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 570/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 571/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 572/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 573/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 574/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 575/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 576/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 577/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 578/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 579/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 580/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 581/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 582/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 583/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 584/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 585/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 586/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 587/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 588/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 589/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 590/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 591/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 592/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 593/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 594/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 595/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0073\n",
      "Epoch 596/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0076\n",
      "Epoch 597/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0078\n",
      "Epoch 598/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 599/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 600/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0078\n",
      "Epoch 601/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 602/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 603/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 604/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 605/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0076\n",
      "Epoch 606/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 607/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0073\n",
      "Epoch 608/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 609/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 610/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 611/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 612/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0076\n",
      "Epoch 613/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 614/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0077\n",
      "Epoch 615/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 616/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 617/700\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.0074\n",
      "Epoch 618/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 619/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 620/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 621/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 622/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 623/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 624/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 625/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 626/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 627/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 628/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 629/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0075\n",
      "Epoch 630/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 631/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 632/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 633/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 634/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 635/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 636/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 637/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 638/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 639/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 640/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 641/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 642/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 643/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 644/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 645/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 646/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 647/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 648/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 649/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0074\n",
      "Epoch 650/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 651/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 652/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 653/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 654/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 655/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 656/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 657/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 658/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 659/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0073\n",
      "Epoch 660/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 661/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 662/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 663/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 664/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 665/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 666/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 667/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 668/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 669/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 670/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 671/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 672/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 673/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 674/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 675/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 676/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 677/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0068\n",
      "Epoch 678/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 679/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 680/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 681/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 682/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 683/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 684/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 685/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 686/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 687/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 688/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 689/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0072\n",
      "Epoch 690/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 691/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0067\n",
      "Epoch 692/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0067\n",
      "Epoch 693/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 694/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 695/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0067\n",
      "Epoch 696/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0071\n",
      "Epoch 697/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 698/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n",
      "Epoch 699/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0070\n",
      "Epoch 700/700\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18c92617b8>"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(data_train['normed_score']), \n",
    "                epochs=700, \n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9683738127664515\n",
      "Test: 0.8412702856082632\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], data_train['normed_score'])[0]}\")\n",
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.8412702856082632\n",
      "Spearmans: 0.7890781334473093\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")\n",
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)])\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"128, 1.5, 0.35, Adadelta(1, 0.9, 1.5), 700\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/Stsbenchmark/train.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "headers = [x.replace('\"', '').replace('\\n', '') for x in headers]\n",
    "# file = open('./data/semeval.txt', 'r')\n",
    "# data = list()\n",
    "# for line in file:\n",
    "#     data.append(line.split('\\t'))\n",
    "# data = data[1:]\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\"', '').replace('\\n', '') for x in a]\n",
    "    data.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train = pd.DataFrame(data, columns=headers + ['extra'])\n",
    "sts_train = sts_train.iloc[:len(sts_train)-1,:]\n",
    "sts_train['sim'] = [float(x) for x in sts_train['sim']]\n",
    "sts_train['normed_score'] = norm(sts_train['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/Stsbenchmark/test.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "headers = [x.replace('\"', '').replace('\\n', '') for x in headers]\n",
    "# file = open('./data/semeval.txt', 'r')\n",
    "# data = list()\n",
    "# for line in file:\n",
    "#     data.append(line.split('\\t'))\n",
    "# data = data[1:]\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\"', '').replace('\\n', '') for x in a]\n",
    "    data.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type', 'subtype', 'year', 'id', 'sim', 'sent_1', 'sent_2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_test = pd.DataFrame(data, columns=headers + ['extra', 'exta2'])\n",
    "sts_test = sts_test.iloc[:len(sts_test)-2,:]\n",
    "sts_test['sim'] = [float(x) for x in sts_test['sim']]\n",
    "sts_test['normed_score'] = norm(sts_test['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(sts_train)):\n",
    "    if len(sts_train['sent_1'][i].split(' ')) > m:\n",
    "        m = len(sts_train['sent_1'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(sts_train)):\n",
    "    if len(sts_train['sent_2'][i].split(' ')) > m:\n",
    "        m = len(sts_train['sent_2'][i].split(' '))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(sts_test)):\n",
    "    if len(sts_test['sent_1'][i].split(' ')) > m:\n",
    "        m = len(sts_test['sent_1'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(sts_test)):\n",
    "    if len(sts_test['sent_2'][i].split(' ')) > m:\n",
    "        m = len(sts_test['sent_2'][i].split(' '))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(sts_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in sts_train['sent_1'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in sts_train['sent_2'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 56:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 56:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_sts_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_sts_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(sts_test)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in sts_test['sent_1'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in sts_test['sent_2'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(gm[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 32:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 32:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_sts_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_sts_glove300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pickle.load(open('./data/train_a_sts_glove300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_sts_glove300.data', 'rb'))\n",
    "test_a = pickle.load(open('./data/test_a_sts_glove300.data', 'rb'))\n",
    "test_b = pickle.load(open('./data/test_b_sts_glove300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sts_dataset(filename):\n",
    "    \"\"\"\n",
    "     Loads a subset of the STS dataset into a DataFrame.\n",
    "     In particular both sentences and their human rated similarity score.\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sent_pairs = []\n",
    "    with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "def download_and_load_sts_data():\n",
    "    sts_dataset = tf.keras.utils.get_file(\n",
    "        fname=\"Stsbenchmark.tar.gz\",\n",
    "        origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
    "        extract=True)\n",
    "\n",
    "    sts_dev = load_sts_dataset(os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
    "    sts_train = load_sts_dataset(os.path.join(os.path.dirname(sts_dataset), 'stsbenchmark', 'sts-train.csv'))\n",
    "\n",
    "    return sts_dev, sts_test, sts_train\n",
    "\n",
    "sts_dev, sts_test, sts_train = download_and_load_sts_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sent_1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "\n",
       "                                              sent_2   sim  \n",
       "0                        An air plane is taking off.  5.00  \n",
       "1                          A man is playing a flute.  3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
       "3                         Two men are playing chess.  2.60  \n",
       "4                 A man seated is playing the cello.  4.25  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/Stsbenchmark/dev.txt')\n",
    "header = file.readline().split('\\t')\n",
    "headers = [x.replace('\"', '').replace('\\n', '') for x in header]\n",
    "\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\"', '').replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "sts_dev = pd.DataFrame(data, columns=headers)\n",
    "sts_dev = sts_dev.iloc[:len(sts_dev)-1,:]\n",
    "sts_dev = sts_dev[['sent_1', 'sent_2', 'sim']]\n",
    "sts_dev['sim'] = [float(x) for x in sts_dev['sim']]\n",
    "sts_dev['sim'] = norm(sts_dev['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>Southward Korea declares end to MERS outbreak</td>\n",
       "      <td>Frederick north Korea Delegation Meets With So...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6891</th>\n",
       "      <td>South Korean peninsula declares terminate to M...</td>\n",
       "      <td>Second earl of guilford Korea Delegation Meets...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6892</th>\n",
       "      <td>Southward Korea declares terminate to MERS out...</td>\n",
       "      <td>North Dae han min gook Deputation Meets With S...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6893</th>\n",
       "      <td>South Korean peninsula declares end to MERS ou...</td>\n",
       "      <td>Second earl of guilford Korea Delegation Meets...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6894</th>\n",
       "      <td>South Korea declares finish to MERS outbreak</td>\n",
       "      <td>Northward Korea Delegation Meets With South Ko...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent_1  \\\n",
       "6890      Southward Korea declares end to MERS outbreak   \n",
       "6891  South Korean peninsula declares terminate to M...   \n",
       "6892  Southward Korea declares terminate to MERS out...   \n",
       "6893  South Korean peninsula declares end to MERS ou...   \n",
       "6894       South Korea declares finish to MERS outbreak   \n",
       "\n",
       "                                                 sent_2  sim  \n",
       "6890  Frederick north Korea Delegation Meets With So...  0.0  \n",
       "6891  Second earl of guilford Korea Delegation Meets...  0.0  \n",
       "6892  North Dae han min gook Deputation Meets With S...  0.0  \n",
       "6893  Second earl of guilford Korea Delegation Meets...  0.0  \n",
       "6894  Northward Korea Delegation Meets With South Ko...  0.0  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug = pd.read_csv('./data/sts_train_taug.csv')\n",
    "df_aug.tail()\n",
    "\n",
    "df_test_aug = pd.read_csv('./data/sts_test_taug.csv')\n",
    "df_test_aug.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_test['sim'] = norm(sts_test['sim'])\n",
    "sts_train['sim'] = norm(sts_train['sim'])\n",
    "df_aug['sim'] = norm(df_aug['sim'])\n",
    "df_test_aug['sim'] = norm(df_test_aug['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train = sts_train.append(df_aug)\n",
    "sts_train = sts_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(model, datasets, question_cols):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']\n",
    "    questions_cols = question_cols\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    for dataset in datasets:\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            # Iterate through the text of both questions of the row\n",
    "            for question in questions_cols:\n",
    "\n",
    "                q2n = []  # q2n -> question numbers representation\n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # # Check for unwanted words\n",
    "                    if word not in model.vocab:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        q2n.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                    else:\n",
    "                        q2n.append(vocabulary[word])\n",
    "\n",
    "                # Replace questions as word to question as number representationindex, question, q2n\n",
    "                dataset.at[index, question]= q2n\n",
    "\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in model.vocab:\n",
    "            embeddings[index] = model.word_vec(word)\n",
    "\n",
    "    return embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/enwiki_20180420_300d.txt')\n",
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, embedding_dim = prepare_embeddings(model=word2vec, datasets=[sts_train, df_test_aug], question_cols=['sent_1', 'sent_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(sts_train.sent_1.map(lambda x: len(x)).max(),\n",
    "                     sts_train.sent_2.map(lambda x: len(x)).max(),\n",
    "                     df_test_aug.sent_1.map(lambda x: len(x)).max(),\n",
    "                     df_test_aug.sent_2.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {'left': sts_train.sent_1, 'right': sts_train.sent_2}\n",
    "X_test = {'left': df_test_aug.sent_1, 'right': df_test_aug.sent_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "        dataset[side] = tf.keras.preprocessing.sequence.pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length,\n",
    "                                trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_gru = tf.keras.layers.GRU(50, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "                                bias_initializer=tf.keras.initializers.Constant(4.5), dropout=0.0,\n",
    "                                kernel_regularizer=None, recurrent_dropout=0.0)\n",
    "\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "magru_distance = tf.keras.layers.Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                        output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "magru = tf.keras.Model([left_input, right_input], [magru_distance])\n",
    "optimizer=tf.keras.optimizers.Adadelta(learning_rate=1, rho=0.985, clipnorm=2.5)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipvalue=1.5)\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "def pear(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)\n",
    "\n",
    "magru.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "539/539 [==============================] - 6s 12ms/step - loss: 0.2901 - val_loss: 0.3086\n",
      "Epoch 2/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.1797 - val_loss: 0.1019\n",
      "Epoch 3/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0815 - val_loss: 0.0918\n",
      "Epoch 4/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0704 - val_loss: 0.0861\n",
      "Epoch 5/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0641 - val_loss: 0.0836\n",
      "Epoch 6/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0604 - val_loss: 0.0818\n",
      "Epoch 7/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0579 - val_loss: 0.0806\n",
      "Epoch 8/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0559 - val_loss: 0.0797\n",
      "Epoch 9/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0544 - val_loss: 0.0791\n",
      "Epoch 10/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0532 - val_loss: 0.0786\n",
      "Epoch 11/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0522 - val_loss: 0.0779\n",
      "Epoch 12/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0514 - val_loss: 0.0776\n",
      "Epoch 13/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0506 - val_loss: 0.0775\n",
      "Epoch 14/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0499 - val_loss: 0.0772\n",
      "Epoch 15/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0493 - val_loss: 0.0768\n",
      "Epoch 16/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0488 - val_loss: 0.0767\n",
      "Epoch 17/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0483 - val_loss: 0.0764\n",
      "Epoch 18/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0478 - val_loss: 0.0761\n",
      "Epoch 19/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0473 - val_loss: 0.0762\n",
      "Epoch 20/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0468 - val_loss: 0.0761\n",
      "Epoch 21/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0465 - val_loss: 0.0758\n",
      "Epoch 22/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0461 - val_loss: 0.0757\n",
      "Epoch 23/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0458 - val_loss: 0.0756\n",
      "Epoch 24/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0455 - val_loss: 0.0756\n",
      "Epoch 25/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0452 - val_loss: 0.0756\n",
      "Epoch 26/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0450 - val_loss: 0.0756\n",
      "Epoch 27/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0447 - val_loss: 0.0754\n",
      "Epoch 28/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0444 - val_loss: 0.0752\n",
      "Epoch 29/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0442 - val_loss: 0.0753\n",
      "Epoch 30/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0439 - val_loss: 0.0753\n",
      "Epoch 31/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0437 - val_loss: 0.0750\n",
      "Epoch 32/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0435 - val_loss: 0.0751\n",
      "Epoch 33/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0433 - val_loss: 0.0750\n",
      "Epoch 34/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0431 - val_loss: 0.0749\n",
      "Epoch 35/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0429 - val_loss: 0.0748\n",
      "Epoch 36/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0427 - val_loss: 0.0748\n",
      "Epoch 37/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0426 - val_loss: 0.0749\n",
      "Epoch 38/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0424 - val_loss: 0.0747\n",
      "Epoch 39/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0422 - val_loss: 0.0747\n",
      "Epoch 40/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0421 - val_loss: 0.0748\n",
      "Epoch 41/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0419 - val_loss: 0.0747\n",
      "Epoch 42/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0417 - val_loss: 0.0748\n",
      "Epoch 43/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0416 - val_loss: 0.0745\n",
      "Epoch 44/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0414 - val_loss: 0.0747\n",
      "Epoch 45/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0413 - val_loss: 0.0747\n",
      "Epoch 46/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0412 - val_loss: 0.0746\n",
      "Epoch 47/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0410 - val_loss: 0.0746\n",
      "Epoch 48/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0409 - val_loss: 0.0745\n",
      "Epoch 49/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0408 - val_loss: 0.0745\n",
      "Epoch 50/50\n",
      "539/539 [==============================] - 6s 11ms/step - loss: 0.0407 - val_loss: 0.0744\n"
     ]
    }
   ],
   "source": [
    "hist = magru.fit([X_train['left'], X_train['right']], \n",
    "                np.array(sts_train['sim']), \n",
    "                epochs=50, \n",
    "                batch_size=64,\n",
    "               validation_data=([X_test['left'], X_test['right']], df_test_aug['sim'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc964639518>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.731542145210667\n",
      "Test: 0.46455478523922145\n"
     ]
    }
   ],
   "source": [
    "preds = magru.predict([X_train['left'], X_train['right']])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], sts_train['sim'])[0]}\")\n",
    "preds = magru.predict([X_test['left'], X_test['right']])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], df_test_aug['sim'])[0]}\")\n",
    "# mse = np.mean([(preds[x] - sts_test['sim'][x])**2 for x in range(len(sts_test))])\n",
    "# print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = magru.predict([X_test['left'], X_test['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_idx = 0\n",
    "e_idx = b_idx + 5\n",
    "\n",
    "p_avg = list()\n",
    "while e_idx <= len(preds):\n",
    "    p_avg.append(np.mean(preds[b_idx:e_idx]))\n",
    "    b_idx += 5\n",
    "    e_idx += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5371504718912663, 6.472534327535879e-104)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(p_avg, sts_test['sim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>Little girl is blowing a big bubble</td>\n",
       "      <td>There is no girl in pink twirling a ribbon</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>Dog in colored coat is running across the yard</td>\n",
       "      <td>The flute is being played by one man</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>Boy happily playing the piano</td>\n",
       "      <td>A white bird lands quickly in the water</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>The girl, who is little, is combing her hair i...</td>\n",
       "      <td>Two people wearing helmets are driving over th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>A man in the parking lot playing tennis on a l...</td>\n",
       "      <td>The snowboarder is leaping fearlessly over whi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent_1  \\\n",
       "4922                Little girl is blowing a big bubble   \n",
       "4923     Dog in colored coat is running across the yard   \n",
       "4924                      Boy happily playing the piano   \n",
       "4925  The girl, who is little, is combing her hair i...   \n",
       "4926  A man in the parking lot playing tennis on a l...   \n",
       "\n",
       "                                                 sent_2  sim  \n",
       "4922         There is no girl in pink twirling a ribbon  2.1  \n",
       "4923               The flute is being played by one man  1.0  \n",
       "4924            A white bird lands quickly in the water  1.0  \n",
       "4925  Two people wearing helmets are driving over th...  1.0  \n",
       "4926  The snowboarder is leaping fearlessly over whi...  1.0  "
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/semeval_train_ko.csv')\n",
    "df_train_norm = pd.read_csv('./data/semeval_train.csv')\n",
    "df_test = pd.read_csv('./data/semeval_test_ko.csv')\n",
    "\n",
    "# df_test = df_test.iloc[:len(df_test)-1, :]\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>The man is opening the door</td>\n",
       "      <td>A bald man in a band is playing guitar in the ...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>Someone is boiling okra in a pot</td>\n",
       "      <td>Man does not play drums</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>Man singing sincerely and playing guitar</td>\n",
       "      <td>A bicyclist is holding a bike over his head in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>The man in blue has a yellow ball in his fist</td>\n",
       "      <td>A man is jumping rope outside</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>Three dogs are resting on the sidewalk</td>\n",
       "      <td>The woman with a knife is slicing a pepper</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent_1  \\\n",
       "4495                    The man is opening the door   \n",
       "4496               Someone is boiling okra in a pot   \n",
       "4497       Man singing sincerely and playing guitar   \n",
       "4498  The man in blue has a yellow ball in his fist   \n",
       "4499         Three dogs are resting on the sidewalk   \n",
       "\n",
       "                                                 sent_2  sim  \n",
       "4495  A bald man in a band is playing guitar in the ...  1.1  \n",
       "4496                            Man does not play drums  1.0  \n",
       "4497  A bicyclist is holding a bike over his head in...  1.0  \n",
       "4498                      A man is jumping rope outside  1.2  \n",
       "4499         The woman with a knife is slicing a pepper  1.0  "
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm['sent_1'] = df_train_norm['sentence_A']\n",
    "df_train_norm['sent_2'] = df_train_norm['sentence_B']\n",
    "df_train_norm['sim'] = df_train_norm['relatedness_score']\n",
    "df_train_norm = df_train_norm[['sent_1', 'sent_2', 'sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>The young girl is blowing a bubble that is huge</td>\n",
       "      <td>There is no girl in pink twirling a ribbon</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>A dog in a colored coat is running across the ...</td>\n",
       "      <td>The flute is being played by one man</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>A boy is happily playing the piano</td>\n",
       "      <td>A white bird is landing swiftly in the water</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>The girl, who is little, is combing her hair i...</td>\n",
       "      <td>Two people wearing helmets are driving over th...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>A man is in a parking lot and is playing tenni...</td>\n",
       "      <td>The snowboarder is leaping fearlessly over whi...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent_1  \\\n",
       "4922    The young girl is blowing a bubble that is huge   \n",
       "4923  A dog in a colored coat is running across the ...   \n",
       "4924                 A boy is happily playing the piano   \n",
       "4925  The girl, who is little, is combing her hair i...   \n",
       "4926  A man is in a parking lot and is playing tenni...   \n",
       "\n",
       "                                                 sent_2    sim  \n",
       "4922         There is no girl in pink twirling a ribbon  0.275  \n",
       "4923               The flute is being played by one man  0.000  \n",
       "4924       A white bird is landing swiftly in the water  0.000  \n",
       "4925  Two people wearing helmets are driving over th...  0.000  \n",
       "4926  The snowboarder is leaping fearlessly over whi...  0.000  "
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_norm = pd.read_csv('./data/semeval_test.csv')\n",
    "df_test_norm['sim'] = norm(df_test_norm['relatedness_score'])\n",
    "df_test_norm['sent_1'] =df_test_norm['sentence_A']\n",
    "df_test_norm['sent_2'] = df_test_norm['sentence_B']\n",
    "df_test_norm = df_test_norm[['sent_1', 'sent_2', 'sim']]\n",
    "df_test_norm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sim'] = norm(df_train['sim'])\n",
    "df_test['sim'] = norm(df_test['sim'])\n",
    "df_train_norm['sim'] = norm(df_train_norm['sim'])\n",
    "df_test_norm['sim'] = norm(df_test_norm['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>Little girl is blowing a big bubble</td>\n",
       "      <td>There is no girl in pink twirling a ribbon</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>Dog in colored coat is running across the yard</td>\n",
       "      <td>The flute is being played by one man</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>Boy happily playing the piano</td>\n",
       "      <td>A white bird lands quickly in the water</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>The girl, who is little, is combing her hair i...</td>\n",
       "      <td>Two people wearing helmets are driving over th...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>A man in the parking lot playing tennis on a l...</td>\n",
       "      <td>The snowboarder is leaping fearlessly over whi...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent_1  \\\n",
       "4922                Little girl is blowing a big bubble   \n",
       "4923     Dog in colored coat is running across the yard   \n",
       "4924                      Boy happily playing the piano   \n",
       "4925  The girl, who is little, is combing her hair i...   \n",
       "4926  A man in the parking lot playing tennis on a l...   \n",
       "\n",
       "                                                 sent_2    sim  \n",
       "4922         There is no girl in pink twirling a ribbon  0.275  \n",
       "4923               The flute is being played by one man  0.000  \n",
       "4924            A white bird lands quickly in the water  0.000  \n",
       "4925  Two people wearing helmets are driving over th...  0.000  \n",
       "4926  The snowboarder is leaping fearlessly over whi...  0.000  "
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sent_1'] = df_test['sentence_A']\n",
    "df_test['sent_2'] = df_test['sentence_B']\n",
    "\n",
    "# df_train['sent_1'] = df_train['sentence_A']\n",
    "# df_train['sent_2'] = df_train['sentence_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.append(df_train_norm)\n",
    "df_test = df_test.append(df_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(model, datasets, question_cols):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']\n",
    "    questions_cols = question_cols\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    for dataset in datasets:\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            # Iterate through the text of both questions of the row\n",
    "            for question in questions_cols:\n",
    "\n",
    "                q2n = []  # q2n -> question numbers representation\n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # # Check for unwanted words\n",
    "                    if word not in model.vocab:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        q2n.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                    else:\n",
    "                        q2n.append(vocabulary[word])\n",
    "\n",
    "                # Replace questions as word to question as number representationindex, question, q2n\n",
    "                dataset.at[index, question]= q2n\n",
    "\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in model.vocab:\n",
    "            embeddings[index] = model.word_vec(word)\n",
    "\n",
    "    return embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/enwiki_20180420_300d.txt')\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, embedding_dim = prepare_embeddings(model=word2vec, datasets=[df_train, df_test], question_cols=['sent_1', 'sent_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(df_train.sent_1.map(lambda x: len(x)).max(),\n",
    "                     df_train.sent_2.map(lambda x: len(x)).max(),\n",
    "                     df_test.sent_1.map(lambda x: len(x)).max(),\n",
    "                     df_test.sent_2.map(lambda x: len(x)).max())\n",
    "\n",
    "X_train = {'left': df_train.sent_1, 'right': df_train.sent_2}\n",
    "X_test = {'left': df_test.sent_1, 'right': df_test.sent_2}\n",
    "\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "        dataset[side] = tf.keras.preprocessing.sequence.pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length,\n",
    "                                trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "# shared_gru = tf.keras.layers.GRU(100, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "#                                 bias_initializer=tf.keras.initializers.Constant(2.5), dropout=0.0,\n",
    "#                                 kernel_regularizer=None, recurrent_dropout=0.0)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(100, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "                                                               bias_initializer=tf.keras.initializers.Constant(2.5), dropout=0.0,\n",
    "                                                               kernel_regularizer=None, recurrent_dropout=0.0))\n",
    "\n",
    "\n",
    "shared_dense = tf.keras.layers.Dense(50, activation='relu')\n",
    "dp = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "# left_output_den = shared_dense(left_output)\n",
    "# right_output_den = shared_dense(right_output)\n",
    "\n",
    "# left_output_dp = dp(left_output_den)\n",
    "# right_output_dp = dp(right_output_den)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "magru_distance = tf.keras.layers.Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                        output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "magru = tf.keras.Model([left_input, right_input], [magru_distance])\n",
    "optimizer=tf.keras.optimizers.Adadelta(learning_rate=1, rho=0.985, clipvalue=2.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipvalue=1.5)\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "def pear(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)\n",
    "\n",
    "magru.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.0558 - val_loss: 0.0484\n",
      "Epoch 2/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0455 - val_loss: 0.0427\n",
      "Epoch 3/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0402 - val_loss: 0.0390\n",
      "Epoch 4/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0367 - val_loss: 0.0365\n",
      "Epoch 5/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0348\n",
      "Epoch 6/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0323 - val_loss: 0.0335\n",
      "Epoch 7/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0310 - val_loss: 0.0326\n",
      "Epoch 8/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0298 - val_loss: 0.0318\n",
      "Epoch 9/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0289 - val_loss: 0.0312\n",
      "Epoch 10/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0282 - val_loss: 0.0308\n",
      "Epoch 11/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0275 - val_loss: 0.0303\n",
      "Epoch 12/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0270 - val_loss: 0.0300\n",
      "Epoch 13/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0264 - val_loss: 0.0296\n",
      "Epoch 14/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0259 - val_loss: 0.0293\n",
      "Epoch 15/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0255 - val_loss: 0.0290\n",
      "Epoch 16/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0251 - val_loss: 0.0288\n",
      "Epoch 17/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0247 - val_loss: 0.0286\n",
      "Epoch 18/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0243 - val_loss: 0.0284\n",
      "Epoch 19/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0240 - val_loss: 0.0282\n",
      "Epoch 20/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0237 - val_loss: 0.0280\n",
      "Epoch 21/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0234 - val_loss: 0.0278\n",
      "Epoch 22/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0231 - val_loss: 0.0277\n",
      "Epoch 23/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0228 - val_loss: 0.0275\n",
      "Epoch 24/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0226 - val_loss: 0.0274\n",
      "Epoch 25/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0224 - val_loss: 0.0273\n",
      "Epoch 26/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0221 - val_loss: 0.0271\n",
      "Epoch 27/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0219 - val_loss: 0.0270\n",
      "Epoch 28/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0217 - val_loss: 0.0269\n",
      "Epoch 29/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0215 - val_loss: 0.0268\n",
      "Epoch 30/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0213 - val_loss: 0.0267\n",
      "Epoch 31/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0211 - val_loss: 0.0266\n",
      "Epoch 32/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0209 - val_loss: 0.0265\n",
      "Epoch 33/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0208 - val_loss: 0.0265\n",
      "Epoch 34/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0206 - val_loss: 0.0264\n",
      "Epoch 35/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0204 - val_loss: 0.0263\n",
      "Epoch 36/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0203 - val_loss: 0.0262\n",
      "Epoch 37/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0201 - val_loss: 0.0262\n",
      "Epoch 38/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0200 - val_loss: 0.0261\n",
      "Epoch 39/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0199 - val_loss: 0.0260\n",
      "Epoch 40/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0197 - val_loss: 0.0260\n",
      "Epoch 41/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0196 - val_loss: 0.0259\n",
      "Epoch 42/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0195 - val_loss: 0.0258\n",
      "Epoch 43/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0194 - val_loss: 0.0258\n",
      "Epoch 44/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0192 - val_loss: 0.0257\n",
      "Epoch 45/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0191 - val_loss: 0.0257\n",
      "Epoch 46/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0190 - val_loss: 0.0256\n",
      "Epoch 47/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0189 - val_loss: 0.0256\n",
      "Epoch 48/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0188 - val_loss: 0.0255\n",
      "Epoch 49/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0187 - val_loss: 0.0255\n",
      "Epoch 50/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0186 - val_loss: 0.0255\n",
      "Epoch 51/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0185 - val_loss: 0.0254\n",
      "Epoch 52/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0184 - val_loss: 0.0254\n",
      "Epoch 53/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0183 - val_loss: 0.0253\n",
      "Epoch 54/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0182 - val_loss: 0.0253\n",
      "Epoch 55/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0181 - val_loss: 0.0253\n",
      "Epoch 56/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0181 - val_loss: 0.0252\n",
      "Epoch 57/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0180 - val_loss: 0.0252\n",
      "Epoch 58/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0179 - val_loss: 0.0251\n",
      "Epoch 59/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0178 - val_loss: 0.0251\n",
      "Epoch 60/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0177 - val_loss: 0.0251\n",
      "Epoch 61/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0177 - val_loss: 0.0250\n",
      "Epoch 62/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0176 - val_loss: 0.0250\n",
      "Epoch 63/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0175 - val_loss: 0.0250\n",
      "Epoch 64/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0174 - val_loss: 0.0250\n",
      "Epoch 65/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0174 - val_loss: 0.0249\n",
      "Epoch 66/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0173 - val_loss: 0.0249\n",
      "Epoch 67/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0172 - val_loss: 0.0249\n",
      "Epoch 68/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0172 - val_loss: 0.0248\n",
      "Epoch 69/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0171 - val_loss: 0.0248\n",
      "Epoch 70/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0170 - val_loss: 0.0248\n",
      "Epoch 71/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0170 - val_loss: 0.0247\n",
      "Epoch 72/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0169 - val_loss: 0.0247\n",
      "Epoch 73/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0169 - val_loss: 0.0247\n",
      "Epoch 74/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0168 - val_loss: 0.0247\n",
      "Epoch 75/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0168 - val_loss: 0.0246\n",
      "Epoch 76/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0167 - val_loss: 0.0246\n",
      "Epoch 77/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0166 - val_loss: 0.0246\n",
      "Epoch 78/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0166 - val_loss: 0.0246\n",
      "Epoch 79/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0166 - val_loss: 0.0246\n",
      "Epoch 80/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0165 - val_loss: 0.0245\n",
      "Epoch 81/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0164 - val_loss: 0.0245\n",
      "Epoch 82/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0164 - val_loss: 0.0245\n",
      "Epoch 83/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0163 - val_loss: 0.0245\n",
      "Epoch 84/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0163 - val_loss: 0.0244\n",
      "Epoch 85/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0162 - val_loss: 0.0244\n",
      "Epoch 86/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0162 - val_loss: 0.0244\n",
      "Epoch 87/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0162 - val_loss: 0.0244\n",
      "Epoch 88/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0161 - val_loss: 0.0244\n",
      "Epoch 89/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0161 - val_loss: 0.0244\n",
      "Epoch 90/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 91/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 92/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0159 - val_loss: 0.0243\n",
      "Epoch 93/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0159 - val_loss: 0.0243\n",
      "Epoch 94/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0159 - val_loss: 0.0243\n",
      "Epoch 95/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0158 - val_loss: 0.0243\n",
      "Epoch 96/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0158 - val_loss: 0.0242\n",
      "Epoch 97/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0157 - val_loss: 0.0242\n",
      "Epoch 98/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0157 - val_loss: 0.0242\n",
      "Epoch 99/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0156 - val_loss: 0.0242\n",
      "Epoch 100/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0156 - val_loss: 0.0242\n",
      "Epoch 101/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0156 - val_loss: 0.0241\n",
      "Epoch 102/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0155 - val_loss: 0.0241\n",
      "Epoch 103/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0155 - val_loss: 0.0241\n",
      "Epoch 104/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0155 - val_loss: 0.0241\n",
      "Epoch 105/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0154 - val_loss: 0.0241\n",
      "Epoch 106/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0154 - val_loss: 0.0241\n",
      "Epoch 107/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0153 - val_loss: 0.0240\n",
      "Epoch 108/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0153 - val_loss: 0.0240\n",
      "Epoch 109/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0153 - val_loss: 0.0240\n",
      "Epoch 110/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0152 - val_loss: 0.0240\n",
      "Epoch 111/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0152 - val_loss: 0.0240\n",
      "Epoch 112/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0152 - val_loss: 0.0240\n",
      "Epoch 113/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0151 - val_loss: 0.0240\n",
      "Epoch 114/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0151 - val_loss: 0.0239\n",
      "Epoch 115/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0151 - val_loss: 0.0239\n",
      "Epoch 116/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0150 - val_loss: 0.0239\n",
      "Epoch 117/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0150 - val_loss: 0.0239\n",
      "Epoch 118/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0150 - val_loss: 0.0239\n",
      "Epoch 119/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0149 - val_loss: 0.0238\n",
      "Epoch 120/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0149 - val_loss: 0.0238\n",
      "Epoch 121/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0149 - val_loss: 0.0238\n",
      "Epoch 122/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0149 - val_loss: 0.0238\n",
      "Epoch 123/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0148 - val_loss: 0.0238\n",
      "Epoch 124/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0148 - val_loss: 0.0238\n",
      "Epoch 125/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0148 - val_loss: 0.0238\n",
      "Epoch 126/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0148 - val_loss: 0.0238\n",
      "Epoch 127/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0147 - val_loss: 0.0238\n",
      "Epoch 128/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0147 - val_loss: 0.0237\n",
      "Epoch 129/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0147 - val_loss: 0.0237\n",
      "Epoch 130/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0146 - val_loss: 0.0237\n",
      "Epoch 131/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0146 - val_loss: 0.0237\n",
      "Epoch 132/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0146 - val_loss: 0.0237\n",
      "Epoch 133/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0146 - val_loss: 0.0237\n",
      "Epoch 134/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0145 - val_loss: 0.0237\n",
      "Epoch 135/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0145 - val_loss: 0.0236\n",
      "Epoch 136/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0145 - val_loss: 0.0237\n",
      "Epoch 137/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 138/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 139/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 140/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 141/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0144 - val_loss: 0.0236\n",
      "Epoch 142/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0143 - val_loss: 0.0236\n",
      "Epoch 143/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0143 - val_loss: 0.0236\n",
      "Epoch 144/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0143 - val_loss: 0.0236\n",
      "Epoch 145/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0143 - val_loss: 0.0235\n",
      "Epoch 146/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0142 - val_loss: 0.0235\n",
      "Epoch 147/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0142 - val_loss: 0.0235\n",
      "Epoch 148/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0142 - val_loss: 0.0235\n",
      "Epoch 149/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0142 - val_loss: 0.0235\n",
      "Epoch 150/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0141 - val_loss: 0.0235\n",
      "Epoch 151/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0141 - val_loss: 0.0235\n",
      "Epoch 152/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0141 - val_loss: 0.0235\n",
      "Epoch 153/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0141 - val_loss: 0.0235\n",
      "Epoch 154/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 155/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 156/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 157/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 158/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0140 - val_loss: 0.0234\n",
      "Epoch 160/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0139 - val_loss: 0.0234\n",
      "Epoch 161/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0139 - val_loss: 0.0234\n",
      "Epoch 162/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0139 - val_loss: 0.0234\n",
      "Epoch 163/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0139 - val_loss: 0.0234\n",
      "Epoch 164/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 165/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 166/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 167/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 168/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0138 - val_loss: 0.0233\n",
      "Epoch 169/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0137 - val_loss: 0.0233\n",
      "Epoch 170/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0137 - val_loss: 0.0233\n",
      "Epoch 171/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0137 - val_loss: 0.0232\n",
      "Epoch 172/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0137 - val_loss: 0.0232\n",
      "Epoch 173/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0137 - val_loss: 0.0233\n",
      "Epoch 174/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 175/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 176/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 177/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 178/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0232\n",
      "Epoch 179/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0136 - val_loss: 0.0231\n",
      "Epoch 180/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0135 - val_loss: 0.0232\n",
      "Epoch 181/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0135 - val_loss: 0.0231\n",
      "Epoch 182/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0135 - val_loss: 0.0232\n",
      "Epoch 183/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0135 - val_loss: 0.0231\n",
      "Epoch 184/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0135 - val_loss: 0.0231\n",
      "Epoch 185/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 186/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 187/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 188/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 189/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 190/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 191/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0231\n",
      "Epoch 192/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0231\n",
      "Epoch 193/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0230\n",
      "Epoch 194/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0230\n",
      "Epoch 195/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0230\n",
      "Epoch 196/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0230\n",
      "Epoch 197/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0133 - val_loss: 0.0230\n",
      "Epoch 198/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 199/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 200/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 201/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 202/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 203/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 204/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0132 - val_loss: 0.0230\n",
      "Epoch 205/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0230\n",
      "Epoch 206/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0230\n",
      "Epoch 207/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 208/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 209/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 210/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 211/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 212/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 213/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 214/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 215/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 216/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 217/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 218/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 219/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0229\n",
      "Epoch 220/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 221/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 222/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 223/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 224/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 225/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 226/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 227/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0129 - val_loss: 0.0228\n",
      "Epoch 228/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 229/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 230/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 231/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 232/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 233/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 234/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 235/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 236/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0228\n",
      "Epoch 237/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 239/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 240/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 241/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 242/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 243/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 244/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 245/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0127 - val_loss: 0.0227\n",
      "Epoch 246/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 247/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 248/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 249/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 250/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 251/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 252/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 253/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 254/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0126 - val_loss: 0.0227\n",
      "Epoch 255/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 256/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 257/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 258/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 259/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 260/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 261/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 262/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0226\n",
      "Epoch 263/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 264/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0227\n",
      "Epoch 265/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 266/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 267/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0227\n",
      "Epoch 268/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 269/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 270/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 271/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 272/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 273/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 274/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 275/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 276/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 277/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 278/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 279/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 280/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 281/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 282/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 283/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0225\n",
      "Epoch 284/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 285/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 286/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0226\n",
      "Epoch 287/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0226\n",
      "Epoch 288/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 289/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 290/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 291/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 292/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 293/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 294/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 295/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 296/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 297/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 298/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 299/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 300/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 301/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 302/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 303/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 304/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 305/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 306/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 307/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 308/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 309/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 310/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 311/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 312/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 313/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 314/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 315/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 316/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 318/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 319/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 320/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 321/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 322/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 323/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 324/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 325/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 326/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 327/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 328/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0120 - val_loss: 0.0225\n",
      "Epoch 329/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 330/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0225\n",
      "Epoch 331/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 332/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 333/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 334/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 335/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 336/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 337/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 338/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 339/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 340/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 341/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 342/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 343/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 344/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 345/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 346/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 347/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 348/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 349/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 350/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 351/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 352/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 353/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 354/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 355/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 356/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 357/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 358/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 359/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 360/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 361/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0118 - val_loss: 0.0224\n",
      "Epoch 362/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 363/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 364/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 365/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 366/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 367/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 368/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 369/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 370/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0223\n",
      "Epoch 371/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 372/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 373/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0223\n",
      "Epoch 374/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 375/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 376/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 377/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0223\n",
      "Epoch 378/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 379/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 380/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 381/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 382/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 383/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 384/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 385/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 386/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 387/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 388/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 389/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 390/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 391/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 392/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 393/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0224\n",
      "Epoch 394/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 395/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 397/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 398/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 399/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 400/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 401/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 402/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 403/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0116 - val_loss: 0.0223\n",
      "Epoch 404/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 405/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 406/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 407/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 408/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 409/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 410/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 411/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 412/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 413/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 414/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 415/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 416/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 417/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 418/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 419/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 420/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 421/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 422/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 423/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 424/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0223\n",
      "Epoch 425/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 426/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 427/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 428/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 429/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 430/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 431/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 432/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 433/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 434/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 435/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 436/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 437/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 438/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 439/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 440/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 441/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 442/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 443/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 444/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 445/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 446/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 447/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 448/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0223\n",
      "Epoch 449/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 450/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 451/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 452/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 453/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 454/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 455/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 456/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 457/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 458/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 459/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 460/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 461/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 462/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 463/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 464/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 465/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 466/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 467/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 468/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 469/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 470/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n",
      "Epoch 471/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 472/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 473/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 474/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 476/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 477/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 478/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 479/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0222\n",
      "Epoch 480/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0223\n",
      "Epoch 481/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0223\n",
      "Epoch 482/500\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 483/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 484/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0223\n",
      "Epoch 485/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 486/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 487/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 488/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 489/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 490/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 491/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 492/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 493/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0223\n",
      "Epoch 494/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 495/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 496/500\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 497/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 498/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 499/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n",
      "Epoch 500/500\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.0112 - val_loss: 0.0222\n"
     ]
    }
   ],
   "source": [
    "hist = magru.fit([X_train['left'], X_train['right']], \n",
    "                np.array(df_train['sim']), \n",
    "                epochs=500, \n",
    "                batch_size=64,\n",
    "                validation_data=([X_test['left'], X_test['right']], df_test['sim'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc847578f28>]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-619-8625b85c5429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train: {pearsonr([x[0] for x in preds.tolist()], df_train['sim'])[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test: {pearsonr([x[0] for x in preds.tolist()], df_test['sim'])[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1203\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = magru.predict([X_train['left'], X_train['right']])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], df_train['sim'])[0]}\")\n",
    "preds = magru.predict([X_test['left'], X_test['right']])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], df_test['sim'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8131902365183467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = magru.predict([X_test['left'], X_test['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_idx = 0\n",
    "e_idx = 4927\n",
    "\n",
    "p_avg = list()\n",
    "while e_idx < len(preds):\n",
    "    first = preds[b_idx]\n",
    "    second = preds[e_idx]\n",
    "    p_avg.append(np.mean([first, second]))\n",
    "    b_idx += 1\n",
    "    e_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8313927605992647, 0.0)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(p_avg, df_test_norm['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>0.57500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>0.67500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>0.97500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is helping another animal in front...</td>\n",
       "      <td>0.66625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>The young girl is blowing a bubble that is huge</td>\n",
       "      <td>There is no girl in pink twirling a ribbon</td>\n",
       "      <td>0.27500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>A dog in a colored coat is running across the ...</td>\n",
       "      <td>The flute is being played by one man</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>A boy is happily playing the piano</td>\n",
       "      <td>A white bird is landing swiftly in the water</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>The girl, who is little, is combing her hair i...</td>\n",
       "      <td>Two people wearing helmets are driving over th...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>A man is in a parking lot and is playing tenni...</td>\n",
       "      <td>The snowboarder is leaping fearlessly over whi...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4927 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent_1  \\\n",
       "0     There is no boy playing outdoors and there is ...   \n",
       "1     A group of boys in a yard is playing and a man...   \n",
       "2     A group of children is playing in the house an...   \n",
       "3     A brown dog is attacking another animal in fro...   \n",
       "4     A brown dog is attacking another animal in fro...   \n",
       "...                                                 ...   \n",
       "4922    The young girl is blowing a bubble that is huge   \n",
       "4923  A dog in a colored coat is running across the ...   \n",
       "4924                 A boy is happily playing the piano   \n",
       "4925  The girl, who is little, is combing her hair i...   \n",
       "4926  A man is in a parking lot and is playing tenni...   \n",
       "\n",
       "                                                 sent_2      sim  \n",
       "0     A group of kids is playing in a yard and an ol...  0.57500  \n",
       "1     The young boys are playing outdoors and the ma...  0.67500  \n",
       "2     The young boys are playing outdoors and the ma...  0.50000  \n",
       "3     A brown dog is attacking another animal in fro...  0.97500  \n",
       "4     A brown dog is helping another animal in front...  0.66625  \n",
       "...                                                 ...      ...  \n",
       "4922         There is no girl in pink twirling a ribbon  0.27500  \n",
       "4923               The flute is being played by one man  0.00000  \n",
       "4924       A white bird is landing swiftly in the water  0.00000  \n",
       "4925  Two people wearing helmets are driving over th...  0.00000  \n",
       "4926  The snowboarder is leaping fearlessly over whi...  0.00000  \n",
       "\n",
       "[4927 rows x 3 columns]"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[135, 13, 16, 12, 3, 19, 16, 17, 396]</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 5, 7, 8, 9, 10, 4, 5, 11]</td>\n",
       "      <td>0.57500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 12, 13, 3, 4, 5, 6, 8, 9, 10, 4, 5, 11]</td>\n",
       "      <td>[18, 12, 13, 3, 19, 5, 8, 9, 20, 21]</td>\n",
       "      <td>0.67500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 13, 3, 14, 15, 16, 17, 9, 10, 4, 5, 11]</td>\n",
       "      <td>[18, 12, 13, 3, 19, 5, 8, 9, 20, 21]</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[31, 32, 9, 33, 34, 35, 4, 36, 338, 8, 4, 37]</td>\n",
       "      <td>[31, 32, 9, 33, 34, 35, 4, 36, 8, 4, 37]</td>\n",
       "      <td>0.97500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[31, 32, 9, 33, 34, 35, 4, 36, 8, 4, 37]</td>\n",
       "      <td>[31, 32, 9, 1586, 34, 35, 4, 36, 8, 4, 37]</td>\n",
       "      <td>0.66625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9849</th>\n",
       "      <td>4922</td>\n",
       "      <td>[5, 65, 102, 9, 675, 676, 349, 9, 677]</td>\n",
       "      <td>[135, 9, 16, 102, 4, 427, 2122, 2123]</td>\n",
       "      <td>0.27500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9850</th>\n",
       "      <td>4923</td>\n",
       "      <td>[32, 4, 760, 446, 9, 127, 98, 5, 6]</td>\n",
       "      <td>[5, 854, 9, 416, 982, 219, 17, 8]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>4924</td>\n",
       "      <td>[182, 9, 589, 3, 5, 789]</td>\n",
       "      <td>[206, 1784, 9, 1288, 1243, 4, 5, 161]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9852</th>\n",
       "      <td>4925</td>\n",
       "      <td>[5, 102, 55, 9, 18, 9, 811, 236, 272, 58, 631,...</td>\n",
       "      <td>[26, 23, 153, 2186, 13, 788, 166, 5, 1777, 206...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9853</th>\n",
       "      <td>4926</td>\n",
       "      <td>[8, 9, 4, 290, 291, 9, 3, 292, 293, 294, 295]</td>\n",
       "      <td>[5, 754, 9, 223, 1185, 166, 206, 82]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9854 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             sent_1  \\\n",
       "0         0              [135, 13, 16, 12, 3, 19, 16, 17, 396]   \n",
       "1         1        [1, 12, 13, 3, 4, 5, 6, 8, 9, 10, 4, 5, 11]   \n",
       "2         2     [1, 2, 13, 3, 14, 15, 16, 17, 9, 10, 4, 5, 11]   \n",
       "3         3      [31, 32, 9, 33, 34, 35, 4, 36, 338, 8, 4, 37]   \n",
       "4         4           [31, 32, 9, 33, 34, 35, 4, 36, 8, 4, 37]   \n",
       "...     ...                                                ...   \n",
       "9849   4922             [5, 65, 102, 9, 675, 676, 349, 9, 677]   \n",
       "9850   4923                [32, 4, 760, 446, 9, 127, 98, 5, 6]   \n",
       "9851   4924                           [182, 9, 589, 3, 5, 789]   \n",
       "9852   4925  [5, 102, 55, 9, 18, 9, 811, 236, 272, 58, 631,...   \n",
       "9853   4926      [8, 9, 4, 290, 291, 9, 3, 292, 293, 294, 295]   \n",
       "\n",
       "                                                 sent_2      sim  \n",
       "0          [1, 2, 3, 4, 5, 6, 5, 7, 8, 9, 10, 4, 5, 11]  0.57500  \n",
       "1                  [18, 12, 13, 3, 19, 5, 8, 9, 20, 21]  0.67500  \n",
       "2                  [18, 12, 13, 3, 19, 5, 8, 9, 20, 21]  0.50000  \n",
       "3              [31, 32, 9, 33, 34, 35, 4, 36, 8, 4, 37]  0.97500  \n",
       "4            [31, 32, 9, 1586, 34, 35, 4, 36, 8, 4, 37]  0.66625  \n",
       "...                                                 ...      ...  \n",
       "9849              [135, 9, 16, 102, 4, 427, 2122, 2123]  0.27500  \n",
       "9850                  [5, 854, 9, 416, 982, 219, 17, 8]  0.00000  \n",
       "9851              [206, 1784, 9, 1288, 1243, 4, 5, 161]  0.00000  \n",
       "9852  [26, 23, 153, 2186, 13, 788, 166, 5, 1777, 206...  0.00000  \n",
       "9853               [5, 754, 9, 223, 1185, 166, 206, 82]  0.00000  \n",
       "\n",
       "[9854 rows x 4 columns]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-626-333791d9745a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5][[1, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13499"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4499"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4928, 6)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13497"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9660"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9653"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sts_test) * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9653"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9653"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent_1, sent_2, sim]\n",
       "Index: []"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_aug.iloc[b_idx:e_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77238864],\n",
       "       [0.49613777],\n",
       "       [0.88367546],\n",
       "       ...,\n",
       "       [0.3223548 ],\n",
       "       [0.25252047],\n",
       "       [0.2946713 ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.50\n",
       "1       0.72\n",
       "2       1.00\n",
       "3       0.84\n",
       "4       0.30\n",
       "        ... \n",
       "1374    0.00\n",
       "1375    0.20\n",
       "1376    0.20\n",
       "1377    0.00\n",
       "1378    0.00\n",
       "Name: sim, Length: 1379, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_test['sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss for SiameseNet on STSBenchmark')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig('./results/sts_loss.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = magru.predict([X_test['left'], X_test['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90131104],\n",
       "       [0.9376601 ],\n",
       "       [0.9610757 ],\n",
       "       ...,\n",
       "       [0.5535589 ],\n",
       "       [0.21903707],\n",
       "       [0.4240261 ]], dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     preds\n",
       "1499     0.0  0.424026\n",
       "93       0.0  0.146512\n",
       "727      0.0  0.119679\n",
       "728      0.0  0.224491\n",
       "370      0.0  0.287769\n",
       "...      ...       ...\n",
       "134      1.0  0.950776\n",
       "135      1.0  0.950776\n",
       "1123     1.0  0.861014\n",
       "137      1.0  0.781642\n",
       "0        1.0  0.901311\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_plot = pd.DataFrame({\n",
    "    'actual': sts_test['sim'].tolist(),\n",
    "    'preds': [x[0] for x in preds]\n",
    "})\n",
    "sts_plot.sort_values('actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['actual'])\n",
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['preds'])\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.title('Predicted vs. Actual Similarity Scores for STSBenchmark')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Similarity')\n",
    "plt.text(x=1100, y=0.1, s='p = 0.86', size=16)\n",
    "plt.savefig('./results/stsbenchmark.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "magru.save_weights('./models/siam/sts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment\\n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.300</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.700</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>4.900</td>\n",
       "      <td>ENTAILMENT\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is helping another animal in front...</td>\n",
       "      <td>3.665</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pair_ID                                         sentence_A  \\\n",
       "0       6  There is no boy playing outdoors and there is ...   \n",
       "1       7  A group of boys in a yard is playing and a man...   \n",
       "2       8  A group of children is playing in the house an...   \n",
       "3      10  A brown dog is attacking another animal in fro...   \n",
       "4      11  A brown dog is attacking another animal in fro...   \n",
       "\n",
       "                                          sentence_B  relatedness_score  \\\n",
       "0  A group of kids is playing in a yard and an ol...              3.300   \n",
       "1  The young boys are playing outdoors and the ma...              3.700   \n",
       "2  The young boys are playing outdoors and the ma...              3.000   \n",
       "3  A brown dog is attacking another animal in fro...              4.900   \n",
       "4  A brown dog is helping another animal in front...              3.665   \n",
       "\n",
       "  entailment_judgment\\n  \n",
       "0             NEUTRAL\\n  \n",
       "1             NEUTRAL\\n  \n",
       "2             NEUTRAL\\n  \n",
       "3          ENTAILMENT\\n  \n",
       "4             NEUTRAL\\n  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('./data/semeval.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "\n",
    "file = open('./data/semeval.txt', 'r')\n",
    "data = list()\n",
    "for line in file:\n",
    "    data.append(line.split('\\t'))\n",
    "data = data[1:]\n",
    "\n",
    "data = pd.DataFrame(data, columns=headers)\n",
    "data['relatedness_score'] = pd.to_numeric(data['relatedness_score'])\n",
    "data = data.iloc[:len(data)-1,:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment\\n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>9993</td>\n",
       "      <td>A door is being opened by a man</td>\n",
       "      <td>A bald man in a band is playing guitar in the ...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>9997</td>\n",
       "      <td>Someone is boiling okra in a pot</td>\n",
       "      <td>The man is not playing the drums</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>9998</td>\n",
       "      <td>The man is singing heartily and playing the gu...</td>\n",
       "      <td>A bicyclist is holding a bike over his head in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>9999</td>\n",
       "      <td>A man in blue has a yellow ball in the mitt</td>\n",
       "      <td>A man is jumping rope outside</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>10000</td>\n",
       "      <td>Three dogs are resting on a sidewalk</td>\n",
       "      <td>The woman with a knife is slicing a pepper</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEUTRAL\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pair_ID                                         sentence_A  \\\n",
       "4495    9993                    A door is being opened by a man   \n",
       "4496    9997                   Someone is boiling okra in a pot   \n",
       "4497    9998  The man is singing heartily and playing the gu...   \n",
       "4498    9999        A man in blue has a yellow ball in the mitt   \n",
       "4499   10000               Three dogs are resting on a sidewalk   \n",
       "\n",
       "                                             sentence_B  relatedness_score  \\\n",
       "4495  A bald man in a band is playing guitar in the ...                1.1   \n",
       "4496                   The man is not playing the drums                1.0   \n",
       "4497  A bicyclist is holding a bike over his head in...                1.0   \n",
       "4498                      A man is jumping rope outside                1.2   \n",
       "4499         The woman with a knife is slicing a pepper                1.0   \n",
       "\n",
       "     entailment_judgment\\n  \n",
       "4495             NEUTRAL\\n  \n",
       "4496             NEUTRAL\\n  \n",
       "4497             NEUTRAL\\n  \n",
       "4498             NEUTRAL\\n  \n",
       "4499             NEUTRAL\\n  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('./data/semeval_train.txt', 'r')\n",
    "headers = file.readline().split('\\t')\n",
    "\n",
    "file = open('./data/semeval_train.txt', 'r')\n",
    "data_train = list()\n",
    "for line in file:\n",
    "    data_train.append(line.split('\\t'))\n",
    "data_train = data_train[1:]\n",
    "\n",
    "data_train = pd.DataFrame(data_train, columns=headers)\n",
    "data_train['relatedness_score'] = pd.to_numeric(data_train['relatedness_score'])\n",
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normed_score'] = norm(data['relatedness_score'])\n",
    "data_train['normed_score'] = norm(data_train['relatedness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, embedding_dim = prepare_embeddings(model=word2vec, datasets=[data_train, data], question_cols=['sentence_A', 'sentence_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(data_train.sentence_A.map(lambda x: len(x)).max(),\n",
    "                     data_train.sentence_B.map(lambda x: len(x)).max(),\n",
    "                     data.sentence_A.map(lambda x: len(x)).max(),\n",
    "                     data.sentence_B.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {'left': data_train.sentence_A, 'right': data_train.sentence_B}\n",
    "X_test = {'left': data.sentence_A, 'right': data.sentence_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "        dataset[side] = tf.keras.preprocessing.sequence.pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length,\n",
    "                                trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_gru = tf.keras.layers.GRU(100, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "                                bias_initializer=tf.keras.initializers.Constant(2.5), dropout=0.0)\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "magru_distance = tf.keras.layers.Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                        output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "magru = tf.keras.Model([left_input, right_input], [magru_distance])\n",
    "optimizer=tf.keras.optimizers.Adadelta(learning_rate=1, rho=0.985, clipvalue=2.0)\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "def pear(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return K.square(r)\n",
    "\n",
    "magru.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.0669 - val_loss: 0.0559\n",
      "Epoch 2/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0480\n",
      "Epoch 3/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0454 - val_loss: 0.0425\n",
      "Epoch 4/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0401 - val_loss: 0.0384\n",
      "Epoch 5/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0360 - val_loss: 0.0356\n",
      "Epoch 6/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0337 - val_loss: 0.0337\n",
      "Epoch 7/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0316 - val_loss: 0.0323\n",
      "Epoch 8/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0304 - val_loss: 0.0313\n",
      "Epoch 9/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0292 - val_loss: 0.0305\n",
      "Epoch 10/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0285 - val_loss: 0.0299\n",
      "Epoch 11/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0275 - val_loss: 0.0293\n",
      "Epoch 12/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0270 - val_loss: 0.0289\n",
      "Epoch 13/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0261 - val_loss: 0.0284\n",
      "Epoch 14/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0256 - val_loss: 0.0281\n",
      "Epoch 15/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0252 - val_loss: 0.0277\n",
      "Epoch 16/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0248 - val_loss: 0.0274\n",
      "Epoch 17/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0242 - val_loss: 0.0271\n",
      "Epoch 18/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0238 - val_loss: 0.0269\n",
      "Epoch 19/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0233 - val_loss: 0.0266\n",
      "Epoch 20/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0230 - val_loss: 0.0264\n",
      "Epoch 21/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0225 - val_loss: 0.0262\n",
      "Epoch 22/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0223 - val_loss: 0.0260\n",
      "Epoch 23/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0220 - val_loss: 0.0258\n",
      "Epoch 24/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0217 - val_loss: 0.0257\n",
      "Epoch 25/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0215 - val_loss: 0.0255\n",
      "Epoch 26/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0211 - val_loss: 0.0253\n",
      "Epoch 27/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0210 - val_loss: 0.0252\n",
      "Epoch 28/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0207 - val_loss: 0.0251\n",
      "Epoch 29/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0204 - val_loss: 0.0249\n",
      "Epoch 30/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0202 - val_loss: 0.0248\n",
      "Epoch 31/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0200 - val_loss: 0.0247\n",
      "Epoch 32/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0198 - val_loss: 0.0246\n",
      "Epoch 33/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0196 - val_loss: 0.0245\n",
      "Epoch 34/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0194 - val_loss: 0.0244\n",
      "Epoch 35/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0193 - val_loss: 0.0243\n",
      "Epoch 36/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0191 - val_loss: 0.0242\n",
      "Epoch 37/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0189 - val_loss: 0.0241\n",
      "Epoch 38/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0188 - val_loss: 0.0240\n",
      "Epoch 39/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0186 - val_loss: 0.0239\n",
      "Epoch 40/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0186 - val_loss: 0.0239\n",
      "Epoch 41/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0184 - val_loss: 0.0238\n",
      "Epoch 42/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0182 - val_loss: 0.0237\n",
      "Epoch 43/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0181 - val_loss: 0.0237\n",
      "Epoch 44/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0180 - val_loss: 0.0236\n",
      "Epoch 45/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0178 - val_loss: 0.0235\n",
      "Epoch 46/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0178 - val_loss: 0.0235\n",
      "Epoch 47/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0176 - val_loss: 0.0234\n",
      "Epoch 48/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0175 - val_loss: 0.0234\n",
      "Epoch 49/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0174 - val_loss: 0.0233\n",
      "Epoch 50/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0173 - val_loss: 0.0233\n",
      "Epoch 51/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0171 - val_loss: 0.0232\n",
      "Epoch 52/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0172 - val_loss: 0.0232\n",
      "Epoch 53/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0169 - val_loss: 0.0231\n",
      "Epoch 54/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0169 - val_loss: 0.0231\n",
      "Epoch 55/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0168 - val_loss: 0.0230\n",
      "Epoch 56/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0168 - val_loss: 0.0230\n",
      "Epoch 57/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0167 - val_loss: 0.0229\n",
      "Epoch 58/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0166 - val_loss: 0.0229\n",
      "Epoch 59/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0163 - val_loss: 0.0228\n",
      "Epoch 60/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0163 - val_loss: 0.0228\n",
      "Epoch 61/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0163 - val_loss: 0.0228\n",
      "Epoch 62/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0163 - val_loss: 0.0227\n",
      "Epoch 63/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0161 - val_loss: 0.0227\n",
      "Epoch 64/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0160 - val_loss: 0.0226\n",
      "Epoch 65/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0159 - val_loss: 0.0226\n",
      "Epoch 66/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0159 - val_loss: 0.0226\n",
      "Epoch 67/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0158 - val_loss: 0.0225\n",
      "Epoch 68/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0158 - val_loss: 0.0226\n",
      "Epoch 69/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0157 - val_loss: 0.0225\n",
      "Epoch 70/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0156 - val_loss: 0.0225\n",
      "Epoch 71/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0156 - val_loss: 0.0224\n",
      "Epoch 72/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0155 - val_loss: 0.0224\n",
      "Epoch 73/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0155 - val_loss: 0.0224\n",
      "Epoch 74/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0155 - val_loss: 0.0223\n",
      "Epoch 75/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0153 - val_loss: 0.0223\n",
      "Epoch 76/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0152 - val_loss: 0.0223\n",
      "Epoch 77/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0153 - val_loss: 0.0222\n",
      "Epoch 78/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0151 - val_loss: 0.0222\n",
      "Epoch 79/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0151 - val_loss: 0.0222\n",
      "Epoch 80/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0150 - val_loss: 0.0222\n",
      "Epoch 81/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0150 - val_loss: 0.0222\n",
      "Epoch 82/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0149 - val_loss: 0.0221\n",
      "Epoch 83/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0148 - val_loss: 0.0221\n",
      "Epoch 84/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0148 - val_loss: 0.0221\n",
      "Epoch 85/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0147 - val_loss: 0.0220\n",
      "Epoch 86/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0148 - val_loss: 0.0220\n",
      "Epoch 87/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0147 - val_loss: 0.0220\n",
      "Epoch 88/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0147 - val_loss: 0.0220\n",
      "Epoch 89/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0147 - val_loss: 0.0220\n",
      "Epoch 90/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0145 - val_loss: 0.0219\n",
      "Epoch 91/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0145 - val_loss: 0.0219\n",
      "Epoch 92/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0144 - val_loss: 0.0219\n",
      "Epoch 93/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0143 - val_loss: 0.0219\n",
      "Epoch 94/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0143 - val_loss: 0.0219\n",
      "Epoch 95/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0143 - val_loss: 0.0218\n",
      "Epoch 96/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 97/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 98/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 99/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 100/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0141 - val_loss: 0.0217\n",
      "Epoch 101/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0140 - val_loss: 0.0217\n",
      "Epoch 102/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0140 - val_loss: 0.0217\n",
      "Epoch 103/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0140 - val_loss: 0.0217\n",
      "Epoch 104/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0139 - val_loss: 0.0217\n",
      "Epoch 105/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0138 - val_loss: 0.0217\n",
      "Epoch 106/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0138 - val_loss: 0.0216\n",
      "Epoch 107/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0139 - val_loss: 0.0216\n",
      "Epoch 108/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0137 - val_loss: 0.0216\n",
      "Epoch 109/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0138 - val_loss: 0.0216\n",
      "Epoch 110/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0138 - val_loss: 0.0216\n",
      "Epoch 111/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0137 - val_loss: 0.0216\n",
      "Epoch 112/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 113/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 114/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 115/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 116/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0135 - val_loss: 0.0215\n",
      "Epoch 117/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0134 - val_loss: 0.0215\n",
      "Epoch 118/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0135 - val_loss: 0.0215\n",
      "Epoch 119/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0134 - val_loss: 0.0215\n",
      "Epoch 120/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 121/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 122/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 123/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 124/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 125/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 126/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 127/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0133 - val_loss: 0.0214\n",
      "Epoch 128/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 129/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0131 - val_loss: 0.0213\n",
      "Epoch 130/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0131 - val_loss: 0.0213\n",
      "Epoch 131/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0130 - val_loss: 0.0213\n",
      "Epoch 132/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0132 - val_loss: 0.0213\n",
      "Epoch 133/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0131 - val_loss: 0.0213\n",
      "Epoch 134/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0130 - val_loss: 0.0213\n",
      "Epoch 135/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0213\n",
      "Epoch 136/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0213\n",
      "Epoch 137/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0212\n",
      "Epoch 138/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0212\n",
      "Epoch 139/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0212\n",
      "Epoch 140/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0129 - val_loss: 0.0212\n",
      "Epoch 141/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0128 - val_loss: 0.0212\n",
      "Epoch 142/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0128 - val_loss: 0.0212\n",
      "Epoch 143/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0127 - val_loss: 0.0212\n",
      "Epoch 144/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0127 - val_loss: 0.0212\n",
      "Epoch 145/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0128 - val_loss: 0.0212\n",
      "Epoch 146/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0127 - val_loss: 0.0212\n",
      "Epoch 147/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0127 - val_loss: 0.0211\n",
      "Epoch 148/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0126 - val_loss: 0.0211\n",
      "Epoch 149/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0126 - val_loss: 0.0211\n",
      "Epoch 150/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0126 - val_loss: 0.0211\n",
      "Epoch 151/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0125 - val_loss: 0.0211\n",
      "Epoch 152/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0125 - val_loss: 0.0211\n",
      "Epoch 153/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0126 - val_loss: 0.0211\n",
      "Epoch 154/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0125 - val_loss: 0.0211\n",
      "Epoch 155/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0211\n",
      "Epoch 156/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0211\n",
      "Epoch 157/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0210\n",
      "Epoch 158/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0211\n",
      "Epoch 159/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0125 - val_loss: 0.0210\n",
      "Epoch 160/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0210\n",
      "Epoch 161/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0123 - val_loss: 0.0210\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0123 - val_loss: 0.0210\n",
      "Epoch 163/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0123 - val_loss: 0.0210\n",
      "Epoch 164/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0123 - val_loss: 0.0210\n",
      "Epoch 165/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0124 - val_loss: 0.0210\n",
      "Epoch 166/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0122 - val_loss: 0.0210\n",
      "Epoch 167/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0122 - val_loss: 0.0210\n",
      "Epoch 168/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0122 - val_loss: 0.0210\n",
      "Epoch 169/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0122 - val_loss: 0.0209\n",
      "Epoch 170/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0122 - val_loss: 0.0209\n",
      "Epoch 171/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0210\n",
      "Epoch 172/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0209\n",
      "Epoch 173/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0122 - val_loss: 0.0209\n",
      "Epoch 174/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0209\n",
      "Epoch 175/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0122 - val_loss: 0.0209\n",
      "Epoch 176/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0209\n",
      "Epoch 177/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0209\n",
      "Epoch 178/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0120 - val_loss: 0.0209\n",
      "Epoch 179/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0120 - val_loss: 0.0209\n",
      "Epoch 180/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0120 - val_loss: 0.0209\n",
      "Epoch 181/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0119 - val_loss: 0.0209\n",
      "Epoch 182/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0119 - val_loss: 0.0209\n",
      "Epoch 183/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0119 - val_loss: 0.0209\n",
      "Epoch 184/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0119 - val_loss: 0.0209\n",
      "Epoch 185/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0118 - val_loss: 0.0209\n",
      "Epoch 186/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0119 - val_loss: 0.0209\n",
      "Epoch 187/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0118 - val_loss: 0.0209\n",
      "Epoch 188/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0118 - val_loss: 0.0208\n",
      "Epoch 189/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0118 - val_loss: 0.0208\n",
      "Epoch 190/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 191/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0118 - val_loss: 0.0208\n",
      "Epoch 192/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0118 - val_loss: 0.0208\n",
      "Epoch 193/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 194/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0118 - val_loss: 0.0208\n",
      "Epoch 195/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 196/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 197/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 198/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 199/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 200/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 201/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 202/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 203/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 204/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 205/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 206/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0208\n",
      "Epoch 207/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 208/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 209/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0208\n",
      "Epoch 210/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0208\n",
      "Epoch 211/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 212/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 213/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0208\n",
      "Epoch 214/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 215/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 216/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 217/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 218/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 219/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 220/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 221/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 222/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 223/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 224/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 225/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 226/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0113 - val_loss: 0.0207\n",
      "Epoch 227/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0206\n",
      "Epoch 228/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0207\n",
      "Epoch 229/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0206\n",
      "Epoch 230/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0206\n",
      "Epoch 231/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0207\n",
      "Epoch 232/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0207\n",
      "Epoch 233/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0207\n",
      "Epoch 234/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0112 - val_loss: 0.0206\n",
      "Epoch 235/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 236/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 237/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 238/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 239/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 240/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 241/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 242/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 243/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 244/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 245/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 246/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 247/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 248/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 249/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 250/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 251/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 252/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 253/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 254/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0206\n",
      "Epoch 255/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 256/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 257/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0110 - val_loss: 0.0205\n",
      "Epoch 258/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 259/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 260/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 261/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 262/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0206\n",
      "Epoch 263/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0109 - val_loss: 0.0205\n",
      "Epoch 264/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0206\n",
      "Epoch 265/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0206\n",
      "Epoch 266/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 267/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 268/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 269/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 270/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 271/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 272/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 273/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0108 - val_loss: 0.0205\n",
      "Epoch 274/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 275/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 276/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 277/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 278/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 279/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 280/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 281/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 282/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 283/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0205\n",
      "Epoch 284/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 285/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 286/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 287/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 288/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 289/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0204\n",
      "Epoch 290/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 291/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 292/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 293/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 294/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 295/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 296/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 297/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0106 - val_loss: 0.0205\n",
      "Epoch 298/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0105 - val_loss: 0.0204\n",
      "Epoch 299/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 300/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 301/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 302/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 303/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0204\n",
      "Epoch 304/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 305/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 306/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 307/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0205\n",
      "Epoch 308/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 309/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 310/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 311/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 312/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 313/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0105 - val_loss: 0.0204\n",
      "Epoch 314/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0205\n",
      "Epoch 315/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0104 - val_loss: 0.0204\n",
      "Epoch 316/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 317/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 318/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 319/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 320/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 321/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 322/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 323/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 324/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 325/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 326/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 327/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 328/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0204\n",
      "Epoch 329/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 330/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 331/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 332/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0203\n",
      "Epoch 333/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 334/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0203\n",
      "Epoch 335/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0203\n",
      "Epoch 336/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0203\n",
      "Epoch 337/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 338/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0203\n",
      "Epoch 339/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 340/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 341/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 342/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 343/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 344/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 345/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 346/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0203\n",
      "Epoch 347/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0102 - val_loss: 0.0204\n",
      "Epoch 348/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 349/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0101 - val_loss: 0.0203\n",
      "Epoch 350/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0203\n",
      "Epoch 351/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 352/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 353/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 354/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0203\n",
      "Epoch 355/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 356/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0101 - val_loss: 0.0204\n",
      "Epoch 357/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 358/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 359/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 360/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 361/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 362/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 363/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 364/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 365/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 366/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 367/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 368/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0204\n",
      "Epoch 369/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 370/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0204\n",
      "Epoch 371/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 372/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 373/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 374/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 375/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 376/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 377/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0100 - val_loss: 0.0203\n",
      "Epoch 378/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 379/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 380/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 381/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 382/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 383/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 384/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 385/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 386/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0204\n",
      "Epoch 387/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 388/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 389/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 390/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 391/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 392/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 393/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 394/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 395/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 396/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 397/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0099 - val_loss: 0.0203\n",
      "Epoch 398/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 399/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 400/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 401/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 402/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 403/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 404/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 405/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 406/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 407/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0098 - val_loss: 0.0203\n",
      "Epoch 408/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 409/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 410/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 411/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 412/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 413/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 414/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 415/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 416/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 417/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0202\n",
      "Epoch 418/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0202\n",
      "Epoch 419/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 420/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 421/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 422/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 423/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 424/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 425/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 426/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 427/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 428/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0202\n",
      "Epoch 429/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 430/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 431/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0202\n",
      "Epoch 432/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0203\n",
      "Epoch 433/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 434/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 435/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 436/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 437/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 438/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 439/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 440/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 441/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 442/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 443/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 444/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 445/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 446/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0202\n",
      "Epoch 447/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0096 - val_loss: 0.0203\n",
      "Epoch 448/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 449/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 450/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 451/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 452/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 453/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 454/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 455/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 456/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 457/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 458/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 459/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 460/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 461/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 462/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 463/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0095 - val_loss: 0.0203\n",
      "Epoch 464/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 465/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 466/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 467/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 468/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 469/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0203\n",
      "Epoch 470/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 471/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 472/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 473/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 474/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 475/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 476/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 477/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 478/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 479/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 480/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 481/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 482/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 483/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 484/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 485/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0094 - val_loss: 0.0202\n",
      "Epoch 486/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 487/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 488/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 489/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 490/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 491/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 492/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 493/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 494/500\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.0092 - val_loss: 0.0202\n",
      "Epoch 495/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 496/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 497/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 498/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0092 - val_loss: 0.0202\n",
      "Epoch 499/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n",
      "Epoch 500/500\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.0093 - val_loss: 0.0202\n"
     ]
    }
   ],
   "source": [
    "hist = magru.fit([X_train['left'], X_train['right']], \n",
    "                np.array(data_train['normed_score']), \n",
    "                epochs=500, \n",
    "                batch_size=64,\n",
    "               validation_data=([X_test['left'], X_test['right']], data['normed_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9390291036484004\n",
      "Test: 0.8343675704582418\n"
     ]
    }
   ],
   "source": [
    "preds = magru.predict([X_train['left'], X_train['right']])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], data_train['normed_score'])[0]}\")\n",
    "preds = magru.predict([X_test['left'], X_test['right']])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], data['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f18d9538c18>"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss for SiameseNet on SICK')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['Train', 'Test'])\n",
    "# plt.savefig('./results/sick_loss.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = magru.predict([X_test['left'], X_test['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_plot = pd.DataFrame({\n",
    "    'actual': data['normed_score'].tolist(),\n",
    "    'preds': [x[0] for x in preds]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['preds'])\n",
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['actual'])\n",
    "plt.legend(['Predicted', 'Actual'])\n",
    "plt.title('Predicted vs. Actual Similarity Scores (SiameseNet)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Similarity')\n",
    "plt.text(x=2000, y=0.1, s='p = 0.834', size=16)\n",
    "plt.savefig('./results/sick.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "magru.save_weights('./models/siam/sick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9998750139273809\n"
     ]
    }
   ],
   "source": [
    "a = [2.7654870801855078, 0.35995355443076027, 0.016221679989074141, -0.012664358453398751, 0.0036888812311235068]\n",
    "b = [-6.2588482809118942, -0.88952297609194686, 0.017336984676103874, -0.0054928004763216964, 0.011122959185936367]\n",
    "\n",
    "print(-(cosine(a,b) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failed attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(256, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "manhattan = lambda x: tf.keras.backend.exp(tf.keras.backend.abs(x[0] - x[1]))\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=1.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(100, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "\n",
    "manhat_dist = tf.keras.layers.Lambda(\n",
    "    function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "     output_shape=lambda x: (x[0][0], 1))([encoded_l, encoded_r]\n",
    ")\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], manhat_dist)\n",
    "\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipnorm=1.25), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 0.0979 - val_loss: 0.1159\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0926 - val_loss: 0.1108\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0881 - val_loss: 0.1042\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0845 - val_loss: 0.1016\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0827 - val_loss: 0.0995\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0812 - val_loss: 0.0983\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0795 - val_loss: 0.0974\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0785 - val_loss: 0.0964\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0781 - val_loss: 0.0958\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0769 - val_loss: 0.0953\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0764 - val_loss: 0.0944\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0750 - val_loss: 0.0939\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0748 - val_loss: 0.0940\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0743 - val_loss: 0.0934\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0736 - val_loss: 0.0933\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0736 - val_loss: 0.0929\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0731 - val_loss: 0.0929\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0722 - val_loss: 0.0926\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0714 - val_loss: 0.0919\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0714 - val_loss: 0.0919\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0705 - val_loss: 0.0914\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0705 - val_loss: 0.0917\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0699 - val_loss: 0.0913\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0703 - val_loss: 0.0911\n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0690 - val_loss: 0.0910\n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0694 - val_loss: 0.0905\n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0686 - val_loss: 0.0892\n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0681 - val_loss: 0.0888\n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0674 - val_loss: 0.0895\n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0678 - val_loss: 0.0901\n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0672 - val_loss: 0.0902\n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0671 - val_loss: 0.0898\n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0670 - val_loss: 0.0895\n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0669 - val_loss: 0.0891\n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0667 - val_loss: 0.0890\n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0652 - val_loss: 0.0893\n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0653 - val_loss: 0.0890\n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0649 - val_loss: 0.0891\n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0653 - val_loss: 0.0891\n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0652 - val_loss: 0.0889\n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0654 - val_loss: 0.0887\n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0647 - val_loss: 0.0889\n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0647 - val_loss: 0.0887\n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0640 - val_loss: 0.0889\n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0643 - val_loss: 0.0882\n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0643 - val_loss: 0.0880\n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0640 - val_loss: 0.0882\n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0639 - val_loss: 0.0880\n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0638 - val_loss: 0.0881\n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0635 - val_loss: 0.0886\n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0629 - val_loss: 0.0886\n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0636 - val_loss: 0.0887\n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0628 - val_loss: 0.0883\n",
      "Epoch 54/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0629 - val_loss: 0.0880\n",
      "Epoch 55/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0626 - val_loss: 0.0882\n",
      "Epoch 56/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0627 - val_loss: 0.0881\n",
      "Epoch 57/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0631 - val_loss: 0.0878\n",
      "Epoch 58/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0629 - val_loss: 0.0882\n",
      "Epoch 59/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0628 - val_loss: 0.0882\n",
      "Epoch 60/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0623 - val_loss: 0.0879\n",
      "Epoch 61/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0623 - val_loss: 0.0878\n",
      "Epoch 62/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0623 - val_loss: 0.0882\n",
      "Epoch 63/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0619 - val_loss: 0.0876\n",
      "Epoch 64/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0615 - val_loss: 0.0878\n",
      "Epoch 65/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0617 - val_loss: 0.0878\n",
      "Epoch 66/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0620 - val_loss: 0.0875\n",
      "Epoch 67/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0620 - val_loss: 0.0876\n",
      "Epoch 68/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0619 - val_loss: 0.0879\n",
      "Epoch 69/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0615 - val_loss: 0.0876\n",
      "Epoch 70/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0615 - val_loss: 0.0872\n",
      "Epoch 71/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0614 - val_loss: 0.0874\n",
      "Epoch 72/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0607 - val_loss: 0.0872\n",
      "Epoch 73/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0610 - val_loss: 0.0869\n",
      "Epoch 74/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0604 - val_loss: 0.0866\n",
      "Epoch 75/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0871\n",
      "Epoch 76/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0610 - val_loss: 0.0873\n",
      "Epoch 77/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0610 - val_loss: 0.0870\n",
      "Epoch 78/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0873\n",
      "Epoch 79/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0871\n",
      "Epoch 80/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0599 - val_loss: 0.0873\n",
      "Epoch 81/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0870\n",
      "Epoch 82/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0871\n",
      "Epoch 83/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0598 - val_loss: 0.0869\n",
      "Epoch 84/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0603 - val_loss: 0.0866\n",
      "Epoch 85/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0605 - val_loss: 0.0876\n",
      "Epoch 86/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0871\n",
      "Epoch 87/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0596 - val_loss: 0.0868\n",
      "Epoch 88/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0595 - val_loss: 0.0869\n",
      "Epoch 89/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0600 - val_loss: 0.0861\n",
      "Epoch 90/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0601 - val_loss: 0.0868\n",
      "Epoch 91/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0593 - val_loss: 0.0867\n",
      "Epoch 92/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0590 - val_loss: 0.0866\n",
      "Epoch 93/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0596 - val_loss: 0.0862\n",
      "Epoch 94/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0597 - val_loss: 0.0863\n",
      "Epoch 95/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0594 - val_loss: 0.0867\n",
      "Epoch 96/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0594 - val_loss: 0.0862\n",
      "Epoch 97/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0592 - val_loss: 0.0866\n",
      "Epoch 98/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0590 - val_loss: 0.0868\n",
      "Epoch 99/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0590 - val_loss: 0.0863\n",
      "Epoch 100/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0590 - val_loss: 0.0868\n",
      "Epoch 101/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0593 - val_loss: 0.0867\n",
      "Epoch 102/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0591 - val_loss: 0.0863\n",
      "Epoch 103/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0590 - val_loss: 0.0865\n",
      "Epoch 104/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0593 - val_loss: 0.0860\n",
      "Epoch 105/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0582 - val_loss: 0.0863\n",
      "Epoch 106/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0587 - val_loss: 0.0867\n",
      "Epoch 107/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0583 - val_loss: 0.0865\n",
      "Epoch 108/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0586 - val_loss: 0.0863\n",
      "Epoch 109/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0585 - val_loss: 0.0863\n",
      "Epoch 110/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0574 - val_loss: 0.0864\n",
      "Epoch 111/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0587 - val_loss: 0.0865\n",
      "Epoch 112/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0578 - val_loss: 0.0865\n",
      "Epoch 113/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0585 - val_loss: 0.0864\n",
      "Epoch 114/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0581 - val_loss: 0.0862\n",
      "Epoch 115/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0576 - val_loss: 0.0862\n",
      "Epoch 116/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0577 - val_loss: 0.0865\n",
      "Epoch 117/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0573 - val_loss: 0.0859\n",
      "Epoch 118/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0581 - val_loss: 0.0863\n",
      "Epoch 119/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0577 - val_loss: 0.0863\n",
      "Epoch 120/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0579 - val_loss: 0.0862\n",
      "Epoch 121/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0576 - val_loss: 0.0862\n",
      "Epoch 122/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0576 - val_loss: 0.0862\n",
      "Epoch 123/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0579 - val_loss: 0.0863\n",
      "Epoch 124/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0575 - val_loss: 0.0859\n",
      "Epoch 125/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0571 - val_loss: 0.0859\n",
      "Epoch 126/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0574 - val_loss: 0.0861\n",
      "Epoch 127/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0570 - val_loss: 0.0862\n",
      "Epoch 128/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0571 - val_loss: 0.0855\n",
      "Epoch 129/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0572 - val_loss: 0.0860\n",
      "Epoch 130/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0575 - val_loss: 0.0859\n",
      "Epoch 131/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0572 - val_loss: 0.0858\n",
      "Epoch 132/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0566 - val_loss: 0.0854\n",
      "Epoch 133/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0568 - val_loss: 0.0864\n",
      "Epoch 134/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0567 - val_loss: 0.0856\n",
      "Epoch 135/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0571 - val_loss: 0.0859\n",
      "Epoch 136/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0570 - val_loss: 0.0855\n",
      "Epoch 137/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0569 - val_loss: 0.0856\n",
      "Epoch 138/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0566 - val_loss: 0.0858\n",
      "Epoch 139/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0566 - val_loss: 0.0858\n",
      "Epoch 140/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0565 - val_loss: 0.0861\n",
      "Epoch 141/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0570 - val_loss: 0.0862\n",
      "Epoch 142/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0567 - val_loss: 0.0861\n",
      "Epoch 143/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0563 - val_loss: 0.0857\n",
      "Epoch 144/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0562 - val_loss: 0.0858\n",
      "Epoch 145/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0567 - val_loss: 0.0864\n",
      "Epoch 146/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0565 - val_loss: 0.0861\n",
      "Epoch 147/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0558 - val_loss: 0.0864\n",
      "Epoch 148/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0564 - val_loss: 0.0861\n",
      "Epoch 149/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0568 - val_loss: 0.0856\n",
      "Epoch 150/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0561 - val_loss: 0.0863\n",
      "Epoch 151/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0560 - val_loss: 0.0860\n",
      "Epoch 152/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0557 - val_loss: 0.0862\n",
      "Epoch 153/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0559 - val_loss: 0.0865\n",
      "Epoch 154/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0560 - val_loss: 0.0861\n",
      "Epoch 155/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0560 - val_loss: 0.0861\n",
      "Epoch 156/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0554 - val_loss: 0.0863\n",
      "Epoch 157/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0554 - val_loss: 0.0855\n",
      "Epoch 158/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0566 - val_loss: 0.0856\n",
      "Epoch 159/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0555 - val_loss: 0.0852\n",
      "Epoch 160/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0558 - val_loss: 0.0862\n",
      "Epoch 161/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0557 - val_loss: 0.0863\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0552 - val_loss: 0.0864\n",
      "Epoch 163/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0559 - val_loss: 0.0863\n",
      "Epoch 164/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0555 - val_loss: 0.0860\n",
      "Epoch 165/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0560 - val_loss: 0.0857\n",
      "Epoch 166/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0554 - val_loss: 0.0866\n",
      "Epoch 167/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0555 - val_loss: 0.0868\n",
      "Epoch 168/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0551 - val_loss: 0.0870\n",
      "Epoch 169/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0553 - val_loss: 0.0863\n",
      "Epoch 170/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0551 - val_loss: 0.0863\n",
      "Epoch 171/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0553 - val_loss: 0.0863\n",
      "Epoch 172/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0553 - val_loss: 0.0861\n",
      "Epoch 173/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0553 - val_loss: 0.0860\n",
      "Epoch 174/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0550 - val_loss: 0.0868\n",
      "Epoch 175/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0550 - val_loss: 0.0864\n",
      "Epoch 176/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0553 - val_loss: 0.0860\n",
      "Epoch 177/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0859\n",
      "Epoch 178/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0551 - val_loss: 0.0865\n",
      "Epoch 179/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0550 - val_loss: 0.0866\n",
      "Epoch 180/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0552 - val_loss: 0.0862\n",
      "Epoch 181/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0552 - val_loss: 0.0864\n",
      "Epoch 182/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0551 - val_loss: 0.0863\n",
      "Epoch 183/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0551 - val_loss: 0.0868\n",
      "Epoch 184/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0550 - val_loss: 0.0866\n",
      "Epoch 185/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0546 - val_loss: 0.0869\n",
      "Epoch 186/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0552 - val_loss: 0.0861\n",
      "Epoch 187/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0857\n",
      "Epoch 188/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0862\n",
      "Epoch 189/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0864\n",
      "Epoch 190/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0861\n",
      "Epoch 191/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0864\n",
      "Epoch 192/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0546 - val_loss: 0.0862\n",
      "Epoch 193/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0543 - val_loss: 0.0862\n",
      "Epoch 194/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0544 - val_loss: 0.0863\n",
      "Epoch 195/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0864\n",
      "Epoch 196/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0546 - val_loss: 0.0873\n",
      "Epoch 197/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0549 - val_loss: 0.0863\n",
      "Epoch 198/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0862\n",
      "Epoch 199/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0540 - val_loss: 0.0863\n",
      "Epoch 200/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0545 - val_loss: 0.0860\n",
      "Epoch 201/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0862\n",
      "Epoch 202/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0543 - val_loss: 0.0859\n",
      "Epoch 203/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0548 - val_loss: 0.0865\n",
      "Epoch 204/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0542 - val_loss: 0.0865\n",
      "Epoch 205/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0541 - val_loss: 0.0869\n",
      "Epoch 206/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0544 - val_loss: 0.0865\n",
      "Epoch 207/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0548 - val_loss: 0.0871\n",
      "Epoch 208/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0542 - val_loss: 0.0872\n",
      "Epoch 209/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0865\n",
      "Epoch 210/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0546 - val_loss: 0.0865\n",
      "Epoch 211/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0545 - val_loss: 0.0862\n",
      "Epoch 212/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0547 - val_loss: 0.0868\n",
      "Epoch 213/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0536 - val_loss: 0.0866\n",
      "Epoch 214/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0543 - val_loss: 0.0863\n",
      "Epoch 215/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0542 - val_loss: 0.0859\n",
      "Epoch 216/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0858\n",
      "Epoch 217/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0864\n",
      "Epoch 218/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0543 - val_loss: 0.0864\n",
      "Epoch 219/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0543 - val_loss: 0.0867\n",
      "Epoch 220/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0868\n",
      "Epoch 221/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0541 - val_loss: 0.0870\n",
      "Epoch 222/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0535 - val_loss: 0.0868\n",
      "Epoch 223/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0541 - val_loss: 0.0865\n",
      "Epoch 224/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0868\n",
      "Epoch 225/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0535 - val_loss: 0.0871\n",
      "Epoch 226/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0874\n",
      "Epoch 227/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0544 - val_loss: 0.0869\n",
      "Epoch 228/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0539 - val_loss: 0.0866\n",
      "Epoch 229/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0535 - val_loss: 0.0868\n",
      "Epoch 230/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0867\n",
      "Epoch 231/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0535 - val_loss: 0.0861\n",
      "Epoch 232/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0541 - val_loss: 0.0869\n",
      "Epoch 233/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0541 - val_loss: 0.0863\n",
      "Epoch 234/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0540 - val_loss: 0.0860\n",
      "Epoch 235/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0865\n",
      "Epoch 236/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0536 - val_loss: 0.0864\n",
      "Epoch 237/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0538 - val_loss: 0.0864\n",
      "Epoch 238/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0869\n",
      "Epoch 239/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0539 - val_loss: 0.0867\n",
      "Epoch 240/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0856\n",
      "Epoch 241/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0862\n",
      "Epoch 242/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0539 - val_loss: 0.0865\n",
      "Epoch 243/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0535 - val_loss: 0.0862\n",
      "Epoch 244/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0539 - val_loss: 0.0868\n",
      "Epoch 245/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0868\n",
      "Epoch 246/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0535 - val_loss: 0.0868\n",
      "Epoch 247/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0868\n",
      "Epoch 248/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0540 - val_loss: 0.0868\n",
      "Epoch 249/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0867\n",
      "Epoch 250/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0532 - val_loss: 0.0860\n",
      "Epoch 251/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0867\n",
      "Epoch 252/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0861\n",
      "Epoch 253/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0534 - val_loss: 0.0871\n",
      "Epoch 254/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0532 - val_loss: 0.0867\n",
      "Epoch 255/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0534 - val_loss: 0.0866\n",
      "Epoch 256/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0533 - val_loss: 0.0862\n",
      "Epoch 257/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0863\n",
      "Epoch 258/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0866\n",
      "Epoch 259/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0867\n",
      "Epoch 260/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0533 - val_loss: 0.0867\n",
      "Epoch 261/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0538 - val_loss: 0.0864\n",
      "Epoch 262/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0534 - val_loss: 0.0866\n",
      "Epoch 263/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0536 - val_loss: 0.0865\n",
      "Epoch 264/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0537 - val_loss: 0.0866\n",
      "Epoch 265/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0528 - val_loss: 0.0862\n",
      "Epoch 266/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0862\n",
      "Epoch 267/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0868\n",
      "Epoch 268/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0867\n",
      "Epoch 269/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0865\n",
      "Epoch 270/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0533 - val_loss: 0.0867\n",
      "Epoch 271/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0860\n",
      "Epoch 272/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0864\n",
      "Epoch 273/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0859\n",
      "Epoch 274/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0532 - val_loss: 0.0865\n",
      "Epoch 275/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0861\n",
      "Epoch 276/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0859\n",
      "Epoch 277/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0866\n",
      "Epoch 278/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0861\n",
      "Epoch 279/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0861\n",
      "Epoch 280/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0533 - val_loss: 0.0863\n",
      "Epoch 281/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0528 - val_loss: 0.0866\n",
      "Epoch 282/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0865\n",
      "Epoch 283/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0861\n",
      "Epoch 284/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0865\n",
      "Epoch 285/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0864\n",
      "Epoch 286/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0864\n",
      "Epoch 287/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0528 - val_loss: 0.0862\n",
      "Epoch 288/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0870\n",
      "Epoch 289/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0530 - val_loss: 0.0862\n",
      "Epoch 290/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0530 - val_loss: 0.0864\n",
      "Epoch 291/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0864\n",
      "Epoch 292/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0862\n",
      "Epoch 293/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0860\n",
      "Epoch 294/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0861\n",
      "Epoch 295/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0858\n",
      "Epoch 296/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0529 - val_loss: 0.0867\n",
      "Epoch 297/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0858\n",
      "Epoch 298/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0527 - val_loss: 0.0865\n",
      "Epoch 299/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0868\n",
      "Epoch 300/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0867\n",
      "Epoch 301/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0527 - val_loss: 0.0863\n",
      "Epoch 302/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0861\n",
      "Epoch 303/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0864\n",
      "Epoch 304/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0865\n",
      "Epoch 305/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0863\n",
      "Epoch 306/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0531 - val_loss: 0.0861\n",
      "Epoch 307/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0863\n",
      "Epoch 308/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0861\n",
      "Epoch 309/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0864\n",
      "Epoch 310/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0866\n",
      "Epoch 311/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0864\n",
      "Epoch 312/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0862\n",
      "Epoch 313/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0855\n",
      "Epoch 314/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0865\n",
      "Epoch 315/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0866\n",
      "Epoch 316/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0524 - val_loss: 0.0870\n",
      "Epoch 317/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0864\n",
      "Epoch 318/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0871\n",
      "Epoch 319/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0868\n",
      "Epoch 320/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0522 - val_loss: 0.0869\n",
      "Epoch 321/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0861\n",
      "Epoch 322/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0522 - val_loss: 0.0858\n",
      "Epoch 323/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0526 - val_loss: 0.0867\n",
      "Epoch 324/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0866\n",
      "Epoch 325/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0521 - val_loss: 0.0861\n",
      "Epoch 326/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0863\n",
      "Epoch 327/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0870\n",
      "Epoch 328/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0864\n",
      "Epoch 329/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0864\n",
      "Epoch 330/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0522 - val_loss: 0.0873\n",
      "Epoch 331/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0522 - val_loss: 0.0868\n",
      "Epoch 332/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0866\n",
      "Epoch 333/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0521 - val_loss: 0.0872\n",
      "Epoch 334/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0869\n",
      "Epoch 335/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0523 - val_loss: 0.0871\n",
      "Epoch 336/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0525 - val_loss: 0.0869\n",
      "Epoch 337/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0877\n",
      "Epoch 338/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0868\n",
      "Epoch 339/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0870\n",
      "Epoch 340/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0867\n",
      "Epoch 341/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0867\n",
      "Epoch 342/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0521 - val_loss: 0.0879\n",
      "Epoch 343/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0870\n",
      "Epoch 344/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0872\n",
      "Epoch 345/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0868\n",
      "Epoch 346/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0871\n",
      "Epoch 347/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0522 - val_loss: 0.0876\n",
      "Epoch 348/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0868\n",
      "Epoch 349/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0871\n",
      "Epoch 350/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0865\n",
      "Epoch 351/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0869\n",
      "Epoch 352/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0866\n",
      "Epoch 353/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0866\n",
      "Epoch 354/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0870\n",
      "Epoch 355/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0867\n",
      "Epoch 356/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0868\n",
      "Epoch 357/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0860\n",
      "Epoch 358/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0864\n",
      "Epoch 359/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0870\n",
      "Epoch 360/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0511 - val_loss: 0.0867\n",
      "Epoch 361/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0865\n",
      "Epoch 362/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0863\n",
      "Epoch 363/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0867\n",
      "Epoch 364/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0869\n",
      "Epoch 365/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0861\n",
      "Epoch 366/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0868\n",
      "Epoch 367/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0508 - val_loss: 0.0870\n",
      "Epoch 368/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0511 - val_loss: 0.0866\n",
      "Epoch 369/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0867\n",
      "Epoch 370/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0512 - val_loss: 0.0862\n",
      "Epoch 371/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0868\n",
      "Epoch 372/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0870\n",
      "Epoch 373/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0512 - val_loss: 0.0870\n",
      "Epoch 374/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0872\n",
      "Epoch 375/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0876\n",
      "Epoch 376/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0519 - val_loss: 0.0867\n",
      "Epoch 377/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0508 - val_loss: 0.0863\n",
      "Epoch 378/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0871\n",
      "Epoch 379/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0865\n",
      "Epoch 380/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0521 - val_loss: 0.0871\n",
      "Epoch 381/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0873\n",
      "Epoch 382/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0865\n",
      "Epoch 383/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0872\n",
      "Epoch 384/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0873\n",
      "Epoch 385/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0508 - val_loss: 0.0868\n",
      "Epoch 386/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0512 - val_loss: 0.0872\n",
      "Epoch 387/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0512 - val_loss: 0.0871\n",
      "Epoch 388/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0873\n",
      "Epoch 389/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0870\n",
      "Epoch 390/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0878\n",
      "Epoch 391/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0869\n",
      "Epoch 392/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0866\n",
      "Epoch 393/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0518 - val_loss: 0.0866\n",
      "Epoch 394/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0873\n",
      "Epoch 395/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0515 - val_loss: 0.0866\n",
      "Epoch 396/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0873\n",
      "Epoch 397/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0869\n",
      "Epoch 398/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0866\n",
      "Epoch 399/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0867\n",
      "Epoch 400/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0869\n",
      "Epoch 401/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0874\n",
      "Epoch 402/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0866\n",
      "Epoch 403/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0509 - val_loss: 0.0864\n",
      "Epoch 404/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0868\n",
      "Epoch 405/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0867\n",
      "Epoch 406/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0871\n",
      "Epoch 407/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0508 - val_loss: 0.0866\n",
      "Epoch 408/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0866\n",
      "Epoch 409/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0864\n",
      "Epoch 410/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0866\n",
      "Epoch 411/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0874\n",
      "Epoch 412/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0507 - val_loss: 0.0865\n",
      "Epoch 413/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0861\n",
      "Epoch 414/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0871\n",
      "Epoch 415/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0867\n",
      "Epoch 416/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0867\n",
      "Epoch 417/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0870\n",
      "Epoch 418/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0867\n",
      "Epoch 419/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0508 - val_loss: 0.0865\n",
      "Epoch 420/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0861\n",
      "Epoch 421/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0508 - val_loss: 0.0857\n",
      "Epoch 422/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0862\n",
      "Epoch 423/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0508 - val_loss: 0.0860\n",
      "Epoch 424/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0863\n",
      "Epoch 425/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0871\n",
      "Epoch 426/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0863\n",
      "Epoch 427/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0864\n",
      "Epoch 428/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0866\n",
      "Epoch 429/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0872\n",
      "Epoch 430/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0871\n",
      "Epoch 431/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0867\n",
      "Epoch 432/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0873\n",
      "Epoch 433/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0868\n",
      "Epoch 434/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0864\n",
      "Epoch 435/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0865\n",
      "Epoch 436/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0867\n",
      "Epoch 437/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0867\n",
      "Epoch 438/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0503 - val_loss: 0.0866\n",
      "Epoch 439/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0512 - val_loss: 0.0870\n",
      "Epoch 440/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0868\n",
      "Epoch 441/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0863\n",
      "Epoch 442/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0865\n",
      "Epoch 443/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0864\n",
      "Epoch 444/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0504 - val_loss: 0.0869\n",
      "Epoch 445/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0864\n",
      "Epoch 446/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0869\n",
      "Epoch 447/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0509 - val_loss: 0.0861\n",
      "Epoch 448/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0864\n",
      "Epoch 449/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0858\n",
      "Epoch 450/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0862\n",
      "Epoch 451/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0865\n",
      "Epoch 452/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0863\n",
      "Epoch 453/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0869\n",
      "Epoch 454/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0498 - val_loss: 0.0863\n",
      "Epoch 455/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0862\n",
      "Epoch 456/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0505 - val_loss: 0.0865\n",
      "Epoch 457/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0865\n",
      "Epoch 458/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0864\n",
      "Epoch 459/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0857\n",
      "Epoch 460/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0861\n",
      "Epoch 461/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0504 - val_loss: 0.0864\n",
      "Epoch 462/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0504 - val_loss: 0.0863\n",
      "Epoch 463/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0497 - val_loss: 0.0857\n",
      "Epoch 464/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0862\n",
      "Epoch 465/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0501 - val_loss: 0.0855\n",
      "Epoch 466/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0863\n",
      "Epoch 467/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0866\n",
      "Epoch 468/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0501 - val_loss: 0.0862\n",
      "Epoch 469/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0857\n",
      "Epoch 470/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0855\n",
      "Epoch 471/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0852\n",
      "Epoch 472/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0857\n",
      "Epoch 473/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0498 - val_loss: 0.0859\n",
      "Epoch 474/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0869\n",
      "Epoch 475/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0863\n",
      "Epoch 476/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0497 - val_loss: 0.0859\n",
      "Epoch 477/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0507 - val_loss: 0.0855\n",
      "Epoch 478/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0856\n",
      "Epoch 479/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0864\n",
      "Epoch 480/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0495 - val_loss: 0.0861\n",
      "Epoch 481/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0497 - val_loss: 0.0855\n",
      "Epoch 482/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0512 - val_loss: 0.0857\n",
      "Epoch 483/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0500 - val_loss: 0.0860\n",
      "Epoch 484/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0502 - val_loss: 0.0861\n",
      "Epoch 485/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0861\n",
      "Epoch 486/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0504 - val_loss: 0.0859\n",
      "Epoch 487/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0861\n",
      "Epoch 488/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0496 - val_loss: 0.0851\n",
      "Epoch 489/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0503 - val_loss: 0.0857\n",
      "Epoch 490/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0500 - val_loss: 0.0857\n",
      "Epoch 491/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0498 - val_loss: 0.0857\n",
      "Epoch 492/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0491 - val_loss: 0.0861\n",
      "Epoch 493/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0504 - val_loss: 0.0859\n",
      "Epoch 494/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0859\n",
      "Epoch 495/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0497 - val_loss: 0.0861\n",
      "Epoch 496/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0498 - val_loss: 0.0857\n",
      "Epoch 497/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0499 - val_loss: 0.0858\n",
      "Epoch 498/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0854\n",
      "Epoch 499/500\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0510 - val_loss: 0.0855\n",
      "Epoch 500/500\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0499 - val_loss: 0.0857\n"
     ]
    }
   ],
   "source": [
    "hist = siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(sts_train['normed_score']), \n",
    "                epochs=500, \n",
    "                batch_size=64,\n",
    "               validation_data=([np.array(test_a), np.array(test_b)], sts_test['normed_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6983347681067998\n",
      "Test: 0.48304227536519245\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], sts_train['normed_score'])[0]}\")\n",
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], sts_test['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1cb439aac8>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 4.9 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.11.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (46.1.3)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading up the embedding layer...\n",
    "use_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "use_model = hub.KerasLayer(use_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06608703,  0.0574947 , -0.01160311, ..., -0.02843104,\n",
       "         0.01001185,  0.02347358],\n",
       "       [-0.02112009,  0.05138672, -0.0064065 , ..., -0.00271566,\n",
       "         0.04889863,  0.00453754],\n",
       "       [-0.03194014,  0.07775315, -0.05130988, ...,  0.05198707,\n",
       "        -0.0646082 ,  0.03915566],\n",
       "       ...,\n",
       "       [-0.02353627,  0.01033876,  0.10228971, ..., -0.01280252,\n",
       "        -0.07449164,  0.02618971],\n",
       "       [-0.02190577,  0.02311402, -0.0801158 , ...,  0.02143592,\n",
       "         0.00990087,  0.00959624],\n",
       "       [-0.02485645,  0.0554212 ,  0.04666391, ...,  0.03585761,\n",
       "        -0.03794718,  0.00068345]], dtype=float32)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_model(data_train['sentence_A']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21873575448989868"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 = perfect similarityt\n",
    "cosine(use_model([data_train['sentence_A'][0]]).numpy(), \n",
    "       use_model([data_train['sentence_B'][0]]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.875\n",
       "1       0.550\n",
       "2       0.925\n",
       "3       0.600\n",
       "4       0.675\n",
       "        ...  \n",
       "4495    0.025\n",
       "4496    0.000\n",
       "4497    0.000\n",
       "4498    0.050\n",
       "4499    0.000\n",
       "Name: normed_score, Length: 4500, dtype: float64"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['normed_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x,y):\n",
    "    return -(cosine(x,y) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_simil = [cossim(use_model([data['sentence_A'][x]]), use_model([data['sentence_B'][x]])) for x in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7760975814137541, 0.0)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(norm(use_simil), data_train['normed_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7711464242647449, 0.0)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr([1 - use_simil[x] for x in range(len(use_simil))], data['normed_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(2000, 0.1, 'p = 0.776')"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sts_plot = pd.DataFrame({\n",
    "    'actual': data['normed_score'].tolist(),\n",
    "    'preds': [1 - use_simil[x] for x in range(len(use_simil))]\n",
    "})\n",
    "\n",
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['preds'])\n",
    "plt.scatter(x=[x for x in range(len(sts_plot))],\n",
    "            y=sts_plot.sort_values('actual')['actual'])\n",
    "plt.legend(['Predicted', 'Actual'])\n",
    "plt.title('Predicted vs. Actual Similarity Scores (USE)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Similarity')\n",
    "plt.text(x=2000, y=0.1, s='p = 0.776', size=16)\n",
    "# plt.savefig('./results/sick_use.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5904361009597778,\n",
       " 0.4980056881904602,\n",
       " 0.6018387675285339,\n",
       " 0.06102311611175537,\n",
       " 0.10316848754882812,\n",
       " 0.16162806749343872,\n",
       " 0.7003941535949707,\n",
       " 0.6231006383895874,\n",
       " 0.6375586688518524,\n",
       " 0.07068800926208496,\n",
       " 0.16717803478240967,\n",
       " 0.09676086902618408,\n",
       " 0.10559290647506714,\n",
       " 0.21175146102905273,\n",
       " 0.7165170013904572,\n",
       " 0.6023523509502411,\n",
       " 0.2604421377182007,\n",
       " 0.153228759765625,\n",
       " 0.17473310232162476,\n",
       " 0.267844557762146,\n",
       " 0.6002654433250427,\n",
       " 0.3791876435279846,\n",
       " 0.34755241870880127,\n",
       " 0.3716665506362915,\n",
       " 0.188795804977417,\n",
       " 0.03794938325881958,\n",
       " 0.47953569889068604,\n",
       " 0.6440955102443695,\n",
       " 0.436232328414917,\n",
       " 0.43879061937332153,\n",
       " 0.026356995105743408,\n",
       " 0.1205453872680664,\n",
       " 0.16650211811065674,\n",
       " 0.47273164987564087,\n",
       " 0.4649462103843689,\n",
       " 0.48310917615890503,\n",
       " 0.48823755979537964,\n",
       " 0.12997525930404663,\n",
       " 0.15398037433624268,\n",
       " 0.17464131116867065,\n",
       " 0.13946259021759033,\n",
       " 0.24216389656066895,\n",
       " 0.23953568935394287,\n",
       " 0.1741187572479248,\n",
       " 0.03836625814437866,\n",
       " 0.16312211751937866,\n",
       " 0.1389351487159729,\n",
       " 0.6777772605419159,\n",
       " 0.5696329176425934,\n",
       " 0.040192246437072754,\n",
       " 0.4382491111755371,\n",
       " 0.422851026058197,\n",
       " 0.5515772104263306,\n",
       " 0.2518811821937561,\n",
       " 0.4739518165588379,\n",
       " 0.569369912147522,\n",
       " 0.439059853553772,\n",
       " 0.47123175859451294,\n",
       " 0.15041518211364746,\n",
       " 0.3043478727340698,\n",
       " 0.18825912475585938,\n",
       " 0.24261415004730225,\n",
       " 0.6320607662200928,\n",
       " 0.68470698595047,\n",
       " 0.0710289478302002,\n",
       " 0.06509864330291748,\n",
       " 0.4115998148918152,\n",
       " 0.45753300189971924,\n",
       " 0.41182565689086914,\n",
       " 0.16756266355514526,\n",
       " 0.2914777398109436,\n",
       " 0.5238812565803528,\n",
       " 0.4609220623970032,\n",
       " 0.6696151196956635,\n",
       " 0.2736881971359253,\n",
       " 0.2502787709236145,\n",
       " 0.638113260269165,\n",
       " 0.8042919635772705,\n",
       " 0.6514692604541779,\n",
       " 0.15366315841674805,\n",
       " 0.04944634437561035,\n",
       " 0.20064949989318848,\n",
       " 0.06124091148376465,\n",
       " 0.2276497483253479,\n",
       " 0.16598308086395264,\n",
       " 0.11143434047698975,\n",
       " 0.021390140056610107,\n",
       " 0.07378768920898438,\n",
       " 0.19795191287994385,\n",
       " 0.47019392251968384,\n",
       " 0.45347946882247925,\n",
       " 0.47816991806030273,\n",
       " 0.13147282600402832,\n",
       " 0.23885875940322876,\n",
       " 0.03198814392089844,\n",
       " 0.032756686210632324,\n",
       " 0.28846102952957153,\n",
       " 0.2857999801635742,\n",
       " 0.24747121334075928,\n",
       " 0.28181588649749756,\n",
       " 0.12826693058013916,\n",
       " 0.3134872317314148,\n",
       " 0.08656799793243408,\n",
       " 0.1564081907272339,\n",
       " 0.15392279624938965,\n",
       " 0.4920467734336853,\n",
       " 0.45585519075393677,\n",
       " 0.43418174982070923,\n",
       " 0.19732844829559326,\n",
       " 0.12986981868743896,\n",
       " 0.258289635181427,\n",
       " 0.2848464250564575,\n",
       " 0.4209049940109253,\n",
       " 0.271151065826416,\n",
       " 0.14082711935043335,\n",
       " 0.3614909052848816,\n",
       " 0.24291956424713135,\n",
       " 0.27334368228912354,\n",
       " 0.235152006149292,\n",
       " 0.20839357376098633,\n",
       " 0.33585792779922485,\n",
       " 0.20621740818023682,\n",
       " 0.49263423681259155,\n",
       " 0.5760602653026581,\n",
       " 0.2000044584274292,\n",
       " 0.044587790966033936,\n",
       " 0.07727056741714478,\n",
       " 0.10654306411743164,\n",
       " 0.37858396768569946,\n",
       " 0.052004098892211914,\n",
       " 0.7735992074012756,\n",
       " 0.7766881883144379,\n",
       " 0.04978072643280029,\n",
       " 0.24494218826293945,\n",
       " 0.30547845363616943,\n",
       " 0.17011600732803345,\n",
       " 0.5157302618026733,\n",
       " 0.5521853566169739,\n",
       " 0.4014703035354614,\n",
       " 0.44941842555999756,\n",
       " 0.4177471399307251,\n",
       " 0.08800697326660156,\n",
       " 0.5956918895244598,\n",
       " 0.10876965522766113,\n",
       " 0.11827325820922852,\n",
       " 0.0910225510597229,\n",
       " 0.43462032079696655,\n",
       " 0.4232359528541565,\n",
       " 0.3649200201034546,\n",
       " 0.3666372299194336,\n",
       " 0.0636742115020752,\n",
       " 0.25939542055130005,\n",
       " 0.601085364818573,\n",
       " 0.5122983157634735,\n",
       " 0.08386313915252686,\n",
       " 0.12322425842285156,\n",
       " 0.473707377910614,\n",
       " 0.4814837574958801,\n",
       " 0.36818474531173706,\n",
       " 0.11017686128616333,\n",
       " 0.45335477590560913,\n",
       " 0.5824507474899292,\n",
       " 0.697732537984848,\n",
       " 0.4898417592048645,\n",
       " 0.05699014663696289,\n",
       " 0.39473533630371094,\n",
       " 0.36951297521591187,\n",
       " 0.5401698648929596,\n",
       " 0.08171141147613525,\n",
       " 0.8754919990897179,\n",
       " 0.8727080225944519,\n",
       " 0.890121579170227,\n",
       " 0.8300357162952423,\n",
       " 0.8751656264066696,\n",
       " 0.06109356880187988,\n",
       " 0.27224200963974,\n",
       " 0.5024130046367645,\n",
       " 0.6726907193660736,\n",
       " 0.6809140145778656,\n",
       " 0.6470352411270142,\n",
       " 0.138433039188385,\n",
       " 0.5298483073711395,\n",
       " 0.7407163381576538,\n",
       " 0.08539950847625732,\n",
       " 0.36563098430633545,\n",
       " 0.433508038520813,\n",
       " 0.4975839853286743,\n",
       " 0.35599327087402344,\n",
       " 0.1651250123977661,\n",
       " 0.09321117401123047,\n",
       " 0.22113001346588135,\n",
       " 0.348197340965271,\n",
       " 0.35040706396102905,\n",
       " 0.4715597629547119,\n",
       " 0.0796363353729248,\n",
       " 0.017189323902130127,\n",
       " 0.26362746953964233,\n",
       " 0.28143560886383057,\n",
       " 0.09495782852172852,\n",
       " 0.5212202668190002,\n",
       " 0.2760049104690552,\n",
       " 0.11224579811096191,\n",
       " 0.1776140332221985,\n",
       " 0.27034443616867065,\n",
       " 0.30994904041290283,\n",
       " 0.5611334443092346,\n",
       " 0.5825028121471405,\n",
       " 0.47823548316955566,\n",
       " 0.27426207065582275,\n",
       " 0.7456415593624115,\n",
       " 0.8779222294688225,\n",
       " 0.5182752907276154,\n",
       " 0.10512101650238037,\n",
       " 0.2826085686683655,\n",
       " 0.1714535355567932,\n",
       " 0.3798239231109619,\n",
       " 0.4037777781486511,\n",
       " 0.25973325967788696,\n",
       " 0.33628547191619873,\n",
       " 0.33799076080322266,\n",
       " 0.1669687032699585,\n",
       " 0.18434393405914307,\n",
       " 0.0760546326637268,\n",
       " 0.12969166040420532,\n",
       " 0.20489037036895752,\n",
       " 0.19911563396453857,\n",
       " 0.1330554485321045,\n",
       " 0.19699794054031372,\n",
       " 0.3842819333076477,\n",
       " 0.09757280349731445,\n",
       " 0.12489575147628784,\n",
       " 0.18799090385437012,\n",
       " 0.4370459318161011,\n",
       " 0.3737946152687073,\n",
       " 0.21222561597824097,\n",
       " 0.40265125036239624,\n",
       " 0.30898576974868774,\n",
       " 0.5580483675003052,\n",
       " 0.6038802266120911,\n",
       " 0.4505311846733093,\n",
       " 0.23621124029159546,\n",
       " 0.2247214913368225,\n",
       " 0.14760863780975342,\n",
       " 0.4505913257598877,\n",
       " 0.4911176562309265,\n",
       " 0.15868127346038818,\n",
       " 0.3149258494377136,\n",
       " 0.43680673837661743,\n",
       " 0.17428851127624512,\n",
       " 0.4064987897872925,\n",
       " 0.48951470851898193,\n",
       " 0.5740967690944672,\n",
       " 0.4289538264274597,\n",
       " 0.05141758918762207,\n",
       " 0.32632458209991455,\n",
       " 0.40309756994247437,\n",
       " 0.35626327991485596,\n",
       " 0.30590271949768066,\n",
       " 0.17378127574920654,\n",
       " 0.30676132440567017,\n",
       " 0.306399405002594,\n",
       " 0.5027525424957275,\n",
       " 0.47478318214416504,\n",
       " 0.10664474964141846,\n",
       " 0.1564008593559265,\n",
       " 0.19638478755950928,\n",
       " 0.60506272315979,\n",
       " 0.6578624546527863,\n",
       " 0.5969433784484863,\n",
       " 0.5529730021953583,\n",
       " 0.21810728311538696,\n",
       " 0.03628838062286377,\n",
       " 0.029454171657562256,\n",
       " 0.4376276731491089,\n",
       " 0.5135598182678223,\n",
       " 0.48080193996429443,\n",
       " 0.07616239786148071,\n",
       " 0.17381072044372559,\n",
       " 0.34035253524780273,\n",
       " 0.4615969657897949,\n",
       " 0.38689327239990234,\n",
       " 0.43827933073043823,\n",
       " 0.4181128740310669,\n",
       " 0.21471643447875977,\n",
       " 0.08895647525787354,\n",
       " 0.1414594054222107,\n",
       " 0.4977850317955017,\n",
       " 0.5208220481872559,\n",
       " 0.4778746962547302,\n",
       " 0.06801575422286987,\n",
       " 0.09015679359436035,\n",
       " 0.25077760219573975,\n",
       " 0.47843337059020996,\n",
       " 0.3054795265197754,\n",
       " 0.3279191851615906,\n",
       " 0.0638662576675415,\n",
       " 0.1221015453338623,\n",
       " 0.22750234603881836,\n",
       " 0.16936194896697998,\n",
       " 0.31808048486709595,\n",
       " 0.41476970911026,\n",
       " 0.46663832664489746,\n",
       " 0.3456382751464844,\n",
       " 0.3692578673362732,\n",
       " 0.3817705512046814,\n",
       " 0.3405733108520508,\n",
       " 0.017964482307434082,\n",
       " 0.10065650939941406,\n",
       " 0.08904212713241577,\n",
       " 0.07680702209472656,\n",
       " 0.4169151186943054,\n",
       " 0.5092979073524475,\n",
       " 0.4451192021369934,\n",
       " 0.07451194524765015,\n",
       " 0.0514562726020813,\n",
       " 0.11105167865753174,\n",
       " 0.5229364037513733,\n",
       " 0.5782236158847809,\n",
       " 0.4998013377189636,\n",
       " 0.07143521308898926,\n",
       " 0.28109210729599,\n",
       " 0.5197155475616455,\n",
       " 0.5845988094806671,\n",
       " 0.6395560801029205,\n",
       " 0.5151863694190979,\n",
       " 0.03351116180419922,\n",
       " 0.0646357536315918,\n",
       " 0.10972380638122559,\n",
       " 0.12811875343322754,\n",
       " 0.31439220905303955,\n",
       " 0.7471291124820709,\n",
       " 0.38993972539901733,\n",
       " 0.3391726016998291,\n",
       " 0.17279744148254395,\n",
       " 0.460446298122406,\n",
       " 0.5397526025772095,\n",
       " 0.5183210074901581,\n",
       " 0.04416155815124512,\n",
       " 0.10739701986312866,\n",
       " 0.09986865520477295,\n",
       " 0.24829286336898804,\n",
       " 0.10274356603622437,\n",
       " 0.32378464937210083,\n",
       " 0.052136003971099854,\n",
       " 0.3654409646987915,\n",
       " 0.439731240272522,\n",
       " 0.3309604525566101,\n",
       " 0.3894560933113098,\n",
       " 0.06096071004867554,\n",
       " 0.10969281196594238,\n",
       " 0.1393304467201233,\n",
       " 0.21163839101791382,\n",
       " 0.288873553276062,\n",
       " 0.3189629912376404,\n",
       " 0.3081837296485901,\n",
       " 0.29316186904907227,\n",
       " 0.10213816165924072,\n",
       " 0.19306200742721558,\n",
       " 0.38556212186813354,\n",
       " 0.37189817428588867,\n",
       " 0.512103259563446,\n",
       " 0.43175292015075684,\n",
       " 0.36653363704681396,\n",
       " 0.15221238136291504,\n",
       " 0.591951996088028,\n",
       " 0.5497758090496063,\n",
       " 0.5618136823177338,\n",
       " 0.19698160886764526,\n",
       " 0.3507976531982422,\n",
       " 0.49346548318862915,\n",
       " 0.7003150582313538,\n",
       " 0.683958888053894,\n",
       " 0.557087242603302,\n",
       " 0.5653992295265198,\n",
       " 0.07432663440704346,\n",
       " 0.29898834228515625,\n",
       " 0.029078781604766846,\n",
       " 0.5544916391372681,\n",
       " 0.5758920311927795,\n",
       " 0.5547307431697845,\n",
       " 0.6406473219394684,\n",
       " 0.14257335662841797,\n",
       " 0.0830240249633789,\n",
       " 0.20436447858810425,\n",
       " 0.2989996075630188,\n",
       " 0.432483971118927,\n",
       " 0.46321654319763184,\n",
       " 0.2488355040550232,\n",
       " 0.3169249892234802,\n",
       " 0.07945108413696289,\n",
       " 0.34741514921188354,\n",
       " 0.3838159441947937,\n",
       " 0.3358895182609558,\n",
       " 0.1833530068397522,\n",
       " 0.5523623526096344,\n",
       " 0.5277367532253265,\n",
       " 0.5019115209579468,\n",
       " 0.16693741083145142,\n",
       " 0.058846116065979004,\n",
       " 0.5679178237915039,\n",
       " 0.6023633480072021,\n",
       " 0.5629308819770813,\n",
       " 0.0949474573135376,\n",
       " 0.12845218181610107,\n",
       " 0.22809457778930664,\n",
       " 0.4151642322540283,\n",
       " 0.42288637161254883,\n",
       " 0.43485361337661743,\n",
       " 0.06001561880111694,\n",
       " 0.06484055519104004,\n",
       " 0.09420037269592285,\n",
       " 0.5920453071594238,\n",
       " 0.5276763141155243,\n",
       " 0.5028894245624542,\n",
       " 0.08227300643920898,\n",
       " 0.060377418994903564,\n",
       " 0.23319077491760254,\n",
       " 0.07097494602203369,\n",
       " 0.16259020566940308,\n",
       " 0.7577128410339355,\n",
       " 0.740972101688385,\n",
       " 0.7350135445594788,\n",
       " 0.12528419494628906,\n",
       " 0.11718130111694336,\n",
       " 0.20697152614593506,\n",
       " 0.571369469165802,\n",
       " 0.6087921261787415,\n",
       " 0.5504875183105469,\n",
       " 0.05500310659408569,\n",
       " 0.20821678638458252,\n",
       " 0.2767174243927002,\n",
       " 0.36018049716949463,\n",
       " 0.3704322576522827,\n",
       " 0.433077335357666,\n",
       " 0.32234227657318115,\n",
       " 0.37281519174575806,\n",
       " 0.10371124744415283,\n",
       " 0.11597239971160889,\n",
       " 0.11838710308074951,\n",
       " 0.4796064496040344,\n",
       " 0.4970526099205017,\n",
       " 0.5166813135147095,\n",
       " 0.11918866634368896,\n",
       " 0.2835884690284729,\n",
       " 0.03478264808654785,\n",
       " 0.43029677867889404,\n",
       " 0.2283816933631897,\n",
       " 0.11513292789459229,\n",
       " 0.36091798543930054,\n",
       " 0.2727678418159485,\n",
       " 0.049514055252075195,\n",
       " 0.3357367515563965,\n",
       " 0.3526110053062439,\n",
       " 0.3769197463989258,\n",
       " 0.17865848541259766,\n",
       " 0.301361620426178,\n",
       " 0.5503974854946136,\n",
       " 0.6258465051651001,\n",
       " 0.6535370349884033,\n",
       " 0.05709570646286011,\n",
       " 0.07585245370864868,\n",
       " 0.1643531322479248,\n",
       " 0.3307778239250183,\n",
       " 0.35463178157806396,\n",
       " 0.2865309715270996,\n",
       " 0.03525114059448242,\n",
       " 0.059789299964904785,\n",
       " 0.22838300466537476,\n",
       " 0.739291787147522,\n",
       " 0.49745655059814453,\n",
       " 0.5620685517787933,\n",
       " 0.23841536045074463,\n",
       " 0.12848889827728271,\n",
       " 0.2783203125,\n",
       " 0.10273253917694092,\n",
       " 0.34498846530914307,\n",
       " 0.3747949004173279,\n",
       " 0.26919180154800415,\n",
       " 0.09841084480285645,\n",
       " 0.009331583976745605,\n",
       " 0.10283660888671875,\n",
       " 0.19092923402786255,\n",
       " 0.47082287073135376,\n",
       " 0.46284449100494385,\n",
       " 0.46399515867233276,\n",
       " 0.533848226070404,\n",
       " 0.1680808663368225,\n",
       " 0.7090769410133362,\n",
       " 0.36949819326400757,\n",
       " 0.2915384769439697,\n",
       " 0.14946478605270386,\n",
       " 0.14162993431091309,\n",
       " 0.5485381186008453,\n",
       " 0.6316492259502411,\n",
       " 0.571253776550293,\n",
       " 0.6370239555835724,\n",
       " 0.5824604332447052,\n",
       " 0.1519445776939392,\n",
       " 0.06245702505111694,\n",
       " 0.11861193180084229,\n",
       " 0.5563324987888336,\n",
       " 0.45539915561676025,\n",
       " 0.5295796990394592,\n",
       " 0.042501628398895264,\n",
       " 0.022590935230255127,\n",
       " 0.6298960447311401,\n",
       " 0.680101603269577,\n",
       " 0.6999496817588806,\n",
       " 0.6106423735618591,\n",
       " 0.04942440986633301,\n",
       " 0.06483322381973267,\n",
       " 0.6251756548881531,\n",
       " 0.6844986975193024,\n",
       " 0.6251870691776276,\n",
       " 0.04967153072357178,\n",
       " 0.3522363305091858,\n",
       " 0.12821859121322632,\n",
       " 0.31293827295303345,\n",
       " 0.38724517822265625,\n",
       " 0.36745965480804443,\n",
       " 0.32659125328063965,\n",
       " 0.1695774793624878,\n",
       " 0.038944363594055176,\n",
       " 0.32355475425720215,\n",
       " 0.48725593090057373,\n",
       " 0.48658663034439087,\n",
       " 0.039650917053222656,\n",
       " 0.22935199737548828,\n",
       " 0.040967464447021484,\n",
       " 0.08431875705718994,\n",
       " 0.5018332898616791,\n",
       " 0.39921820163726807,\n",
       " 0.44285112619400024,\n",
       " 0.5227040648460388,\n",
       " 0.5670542418956757,\n",
       " 0.09005296230316162,\n",
       " 0.07502013444900513,\n",
       " 0.16276168823242188,\n",
       " 0.1127772331237793,\n",
       " 0.1111571192741394,\n",
       " 0.31123054027557373,\n",
       " 0.3797492980957031,\n",
       " 0.34458982944488525,\n",
       " 0.055313944816589355,\n",
       " 0.4301674962043762,\n",
       " 0.09375882148742676,\n",
       " 0.17499321699142456,\n",
       " 0.2730967402458191,\n",
       " 0.29543888568878174,\n",
       " 0.27089935541152954,\n",
       " 0.23260676860809326,\n",
       " 0.01832360029220581,\n",
       " 0.09470182657241821,\n",
       " 0.2871361970901489,\n",
       " 0.15770655870437622,\n",
       " 0.837644025683403,\n",
       " 0.9153265431523323,\n",
       " 0.9234084859490395,\n",
       " 0.04875636100769043,\n",
       " 0.09088301658630371,\n",
       " 0.20660793781280518,\n",
       " 0.7221684455871582,\n",
       " 0.7634489834308624,\n",
       " 0.709671676158905,\n",
       " 0.18381041288375854,\n",
       " 0.15175193548202515,\n",
       " 0.14947831630706787,\n",
       " 0.10332417488098145,\n",
       " 0.07673817873001099,\n",
       " 0.08436262607574463,\n",
       " 0.04322928190231323,\n",
       " 0.2507660984992981,\n",
       " 0.2532161474227905,\n",
       " 0.24264872074127197,\n",
       " 0.22005778551101685,\n",
       " 0.2796299457550049,\n",
       " 0.36291998624801636,\n",
       " 0.10370928049087524,\n",
       " 0.13582146167755127,\n",
       " 0.8204989433288574,\n",
       " 0.9012181386351585,\n",
       " 0.928697757422924,\n",
       " 0.19175481796264648,\n",
       " 0.18365591764450073,\n",
       " 0.22737228870391846,\n",
       " 0.38865530490875244,\n",
       " 0.3125506639480591,\n",
       " 0.18334412574768066,\n",
       " 0.1427229642868042,\n",
       " 0.0571935772895813,\n",
       " 0.46888309717178345,\n",
       " 0.4079267978668213,\n",
       " 0.11739087104797363,\n",
       " 0.13704973459243774,\n",
       " 0.4447838068008423,\n",
       " 0.42813509702682495,\n",
       " 0.3856252431869507,\n",
       " 0.31926101446151733,\n",
       " 0.09223490953445435,\n",
       " 0.6668103039264679,\n",
       " 0.670985072851181,\n",
       " 0.1943274736404419,\n",
       " 0.11887657642364502,\n",
       " 0.15717220306396484,\n",
       " 0.246723473072052,\n",
       " 0.2616473436355591,\n",
       " 0.10568785667419434,\n",
       " 0.5537745952606201,\n",
       " 0.2911091446876526,\n",
       " 0.4534149765968323,\n",
       " 0.2916628122329712,\n",
       " 0.18250715732574463,\n",
       " 0.12726497650146484,\n",
       " 0.47324860095977783,\n",
       " 0.46793580055236816,\n",
       " 0.1241406798362732,\n",
       " 0.14898711442947388,\n",
       " 0.11648547649383545,\n",
       " 0.3307012915611267,\n",
       " 0.31201064586639404,\n",
       " 0.09771811962127686,\n",
       " 0.16398799419403076,\n",
       " 0.12165045738220215,\n",
       " 0.5019442737102509,\n",
       " 0.5485266745090485,\n",
       " 0.2966746687889099,\n",
       " 0.127325177192688,\n",
       " 0.21294593811035156,\n",
       " 0.22129416465759277,\n",
       " 0.1336292028427124,\n",
       " 0.13783496618270874,\n",
       " 0.496135950088501,\n",
       " 0.3978903293609619,\n",
       " 0.20962750911712646,\n",
       " 0.15371173620224,\n",
       " 0.3701625466346741,\n",
       " 0.6201368570327759,\n",
       " 0.5670418739318848,\n",
       " 0.6743564307689667,\n",
       " 0.05799531936645508,\n",
       " 0.1279911994934082,\n",
       " 0.06499296426773071,\n",
       " 0.4091404676437378,\n",
       " 0.09849107265472412,\n",
       " 0.49547338485717773,\n",
       " 0.23889422416687012,\n",
       " 0.5801998972892761,\n",
       " 0.0942421555519104,\n",
       " 0.7490501999855042,\n",
       " 0.4533432722091675,\n",
       " 0.5224539637565613,\n",
       " 0.28645265102386475,\n",
       " 0.5379074811935425,\n",
       " 0.5266386568546295,\n",
       " 0.13766294717788696,\n",
       " 0.034343063831329346,\n",
       " 0.09251582622528076,\n",
       " 0.299522340297699,\n",
       " 0.18711912631988525,\n",
       " 0.1746375560760498,\n",
       " 0.30946463346481323,\n",
       " 0.23785114288330078,\n",
       " 0.30111366510391235,\n",
       " 0.43997371196746826,\n",
       " 0.6931010186672211,\n",
       " 0.46437394618988037,\n",
       " 0.2905800938606262,\n",
       " 0.162558913230896,\n",
       " 0.2622441053390503,\n",
       " 0.801847830414772,\n",
       " 0.8623609691858292,\n",
       " 0.7550978809595108,\n",
       " 0.9169450774788857,\n",
       " 0.009519875049591064,\n",
       " 0.6629297435283661,\n",
       " 0.01460111141204834,\n",
       " 0.43290579319000244,\n",
       " 0.68210169672966,\n",
       " 0.3666062355041504,\n",
       " 0.2837905287742615,\n",
       " 0.0660015344619751,\n",
       " 0.0877375602722168,\n",
       " 0.24514710903167725,\n",
       " 0.3293578028678894,\n",
       " 0.3133137822151184,\n",
       " 0.25238800048828125,\n",
       " 0.20019447803497314,\n",
       " 0.2834293842315674,\n",
       " 0.411213755607605,\n",
       " 0.19812744855880737,\n",
       " 0.2923579216003418,\n",
       " 0.053488969802856445,\n",
       " 0.2765233516693115,\n",
       " 0.3548542261123657,\n",
       " 0.21892356872558594,\n",
       " 0.1905977725982666,\n",
       " 0.3466002345085144,\n",
       " 0.3522624373435974,\n",
       " 0.5282058417797089,\n",
       " 0.3459888696670532,\n",
       " 0.23229986429214478,\n",
       " 0.24483025074005127,\n",
       " 0.14295601844787598,\n",
       " 0.3604578971862793,\n",
       " 0.16000592708587646,\n",
       " 0.06962841749191284,\n",
       " 0.35853707790374756,\n",
       " 0.2528069019317627,\n",
       " 0.8516018688678741,\n",
       " 0.7945024073123932,\n",
       " 0.7572540640830994,\n",
       " 0.21144390106201172,\n",
       " 0.5130239725112915,\n",
       " 0.3075984716415405,\n",
       " 0.46836817264556885,\n",
       " 0.437630295753479,\n",
       " 0.278988778591156,\n",
       " 0.10336989164352417,\n",
       " 0.5831995010375977,\n",
       " 0.12202799320220947,\n",
       " 0.20848649740219116,\n",
       " 0.3730963468551636,\n",
       " 0.29720938205718994,\n",
       " 0.22964274883270264,\n",
       " 0.17642152309417725,\n",
       " 0.06009566783905029,\n",
       " 0.6966618299484253,\n",
       " 0.638495922088623,\n",
       " 0.6064024269580841,\n",
       " 0.1264573335647583,\n",
       " 0.27527016401290894,\n",
       " 0.49289095401763916,\n",
       " 0.5078065693378448,\n",
       " 0.13840258121490479,\n",
       " 0.2538982033729553,\n",
       " 0.3051599860191345,\n",
       " 0.44280844926834106,\n",
       " 0.6247210502624512,\n",
       " 0.4792131185531616,\n",
       " 0.2142350673675537,\n",
       " 0.04585850238800049,\n",
       " 0.12321436405181885,\n",
       " 0.8020699173212051,\n",
       " 0.7795755863189697,\n",
       " 0.748375654220581,\n",
       " 0.10457867383956909,\n",
       " 0.223557710647583,\n",
       " 0.46235764026641846,\n",
       " 0.7663358002901077,\n",
       " 0.7077178955078125,\n",
       " 0.0308954119682312,\n",
       " 0.5982080698013306,\n",
       " 0.48497146368026733,\n",
       " 0.20700830221176147,\n",
       " 0.0538175106048584,\n",
       " 0.07464057207107544,\n",
       " 0.13054633140563965,\n",
       " 0.6711803674697876,\n",
       " 0.640620082616806,\n",
       " 0.5573351085186005,\n",
       " 0.24497902393341064,\n",
       " 0.16245514154434204,\n",
       " 0.30191874504089355,\n",
       " 0.20614397525787354,\n",
       " 0.3297958970069885,\n",
       " 0.3386576175689697,\n",
       " 0.34761112928390503,\n",
       " 0.4157590866088867,\n",
       " 0.6747829616069794,\n",
       " 0.6630547940731049,\n",
       " 0.7489417791366577,\n",
       " 0.2775833010673523,\n",
       " 0.1879199743270874,\n",
       " 0.40854835510253906,\n",
       " 0.35006803274154663,\n",
       " 0.1886013150215149,\n",
       " 0.19038891792297363,\n",
       " 0.9832872655242682,\n",
       " 0.8833334445953369,\n",
       " 0.1468726396560669,\n",
       " 0.16541695594787598,\n",
       " 0.4889688491821289,\n",
       " 0.5274894535541534,\n",
       " 0.5801703035831451,\n",
       " 0.4604477882385254,\n",
       " 0.24291539192199707,\n",
       " 0.5767189264297485,\n",
       " 0.3469734191894531,\n",
       " 0.21734577417373657,\n",
       " 0.15420228242874146,\n",
       " 0.18685835599899292,\n",
       " 0.348538339138031,\n",
       " 0.10837352275848389,\n",
       " 0.13619834184646606,\n",
       " 0.10682034492492676,\n",
       " 0.10960555076599121,\n",
       " 0.5592029094696045,\n",
       " 0.5947498083114624,\n",
       " 0.5486800074577332,\n",
       " 0.18295443058013916,\n",
       " 0.07725632190704346,\n",
       " 0.17996370792388916,\n",
       " 0.4323883652687073,\n",
       " 0.09207373857498169,\n",
       " 0.1911950707435608,\n",
       " 0.44891542196273804,\n",
       " 0.5829961597919464,\n",
       " 0.038737475872039795,\n",
       " 0.3637009263038635,\n",
       " 0.23382329940795898,\n",
       " 0.350505530834198,\n",
       " 0.6015138328075409,\n",
       " 0.6567972600460052,\n",
       " 0.4349514842033386,\n",
       " 0.042470574378967285,\n",
       " 0.15080153942108154,\n",
       " 0.24203258752822876,\n",
       " 0.1326383352279663,\n",
       " 0.12178778648376465,\n",
       " 0.0830303430557251,\n",
       " 0.6535768806934357,\n",
       " 0.7704702615737915,\n",
       " 0.7059670686721802,\n",
       " 0.05369102954864502,\n",
       " 0.7008916735649109,\n",
       " 0.7556252628564835,\n",
       " 0.5731583535671234,\n",
       " 0.6276505589485168,\n",
       " 0.6339151561260223,\n",
       " 0.2265610694885254,\n",
       " 0.3204275965690613,\n",
       " 0.16112369298934937,\n",
       " 0.1513913869857788,\n",
       " 0.18057239055633545,\n",
       " 0.10710841417312622,\n",
       " 0.21849697828292847,\n",
       " 0.07495671510696411,\n",
       " 0.2564581632614136,\n",
       " 0.172493577003479,\n",
       " 0.0337674617767334,\n",
       " 0.8998871743679047,\n",
       " 0.8018004447221756,\n",
       " 0.1415729522705078,\n",
       " 0.29883134365081787,\n",
       " 0.15331828594207764,\n",
       " 0.1841057538986206,\n",
       " 0.2028120756149292,\n",
       " 0.17679345607757568,\n",
       " 0.24880832433700562,\n",
       " 0.32953470945358276,\n",
       " 0.05889707803726196,\n",
       " 0.23553681373596191,\n",
       " 0.11482244729995728,\n",
       " 0.2001400589942932,\n",
       " 0.33444058895111084,\n",
       " 0.31175363063812256,\n",
       " 0.2330162525177002,\n",
       " 0.4053528308868408,\n",
       " 0.08496212959289551,\n",
       " 0.6497528851032257,\n",
       " 0.6852996647357941,\n",
       " 0.12087428569793701,\n",
       " 0.258980393409729,\n",
       " 0.2533160448074341,\n",
       " 0.21774423122406006,\n",
       " 0.12593823671340942,\n",
       " 0.14037203788757324,\n",
       " 0.1015632152557373,\n",
       " 0.05895733833312988,\n",
       " 0.6318747997283936,\n",
       " 0.5679377317428589,\n",
       " 0.5612461566925049,\n",
       " 0.6083539128303528,\n",
       " 0.18851131200790405,\n",
       " 0.4612250328063965,\n",
       " 0.131000816822052,\n",
       " 0.6634387969970703,\n",
       " 0.7895416766405106,\n",
       " 0.6892665922641754,\n",
       " 0.7281429767608643,\n",
       " 0.12009286880493164,\n",
       " 0.15217596292495728,\n",
       " 0.06789839267730713,\n",
       " 0.09433096647262573,\n",
       " 0.2943342328071594,\n",
       " 0.3038225769996643,\n",
       " 0.3735339045524597,\n",
       " 0.35015350580215454,\n",
       " 0.21023964881896973,\n",
       " 0.3154655694961548,\n",
       " 0.2698326110839844,\n",
       " 0.23690372705459595,\n",
       " 0.30619943141937256,\n",
       " 0.22096168994903564,\n",
       " 0.23945236206054688,\n",
       " 0.06514173746109009,\n",
       " 0.08822768926620483,\n",
       " 0.3325786590576172,\n",
       " 0.3721652030944824,\n",
       " 0.42806917428970337,\n",
       " 0.019702792167663574,\n",
       " 0.11886072158813477,\n",
       " 0.19188416004180908,\n",
       " 0.16982567310333252,\n",
       " 0.16709071397781372,\n",
       " 0.11759710311889648,\n",
       " 0.23492783308029175,\n",
       " 0.30652469396591187,\n",
       " 0.3684345483779907,\n",
       " 0.4480243921279907,\n",
       " 0.5162099599838257,\n",
       " 0.43430447578430176,\n",
       " 0.32540106773376465,\n",
       " 0.3145672678947449,\n",
       " 0.08589673042297363,\n",
       " 0.08334732055664062,\n",
       " 0.2571521997451782,\n",
       " 0.21809709072113037,\n",
       " 0.33626461029052734,\n",
       " 0.6169567108154297,\n",
       " 0.20263004302978516,\n",
       " 0.08334839344024658,\n",
       " 0.13501489162445068,\n",
       " 0.6324907839298248,\n",
       " 0.6671403646469116,\n",
       " 0.5415643453598022,\n",
       " 0.05661821365356445,\n",
       " 0.22344058752059937,\n",
       " 0.24793708324432373,\n",
       " 0.11770927906036377,\n",
       " 0.6590303182601929,\n",
       " 0.6123522222042084,\n",
       " 0.6756666004657745,\n",
       " 0.7032871544361115,\n",
       " 0.7437914609909058,\n",
       " 0.6364223659038544,\n",
       " 0.1552777886390686,\n",
       " 0.8401549756526947,\n",
       " 0.882082536816597,\n",
       " 0.10233277082443237,\n",
       " 0.3992784023284912,\n",
       " 0.3648238182067871,\n",
       " 0.34004688262939453,\n",
       " 0.3787217140197754,\n",
       " 0.05107772350311279,\n",
       " 0.1261671781539917,\n",
       " 0.07350081205368042,\n",
       " 0.17829173803329468,\n",
       " 0.3205569386482239,\n",
       " 0.17747259140014648,\n",
       " 0.37302249670028687,\n",
       " 0.3080378770828247,\n",
       " 0.38450056314468384,\n",
       " 0.26228630542755127,\n",
       " 0.04166930913925171,\n",
       " 0.0897212028503418,\n",
       " 0.33962488174438477,\n",
       " 0.2754669189453125,\n",
       " 0.03330659866333008,\n",
       " 0.19369161128997803,\n",
       " 0.30156582593917847,\n",
       " 0.3241357207298279,\n",
       " 0.46989279985427856,\n",
       " 0.4791870713233948,\n",
       " 0.3453548550605774,\n",
       " 0.09748625755310059,\n",
       " 0.2721577286720276,\n",
       " 0.19898825883865356,\n",
       " 0.6809887886047363,\n",
       " 0.7370783984661102,\n",
       " 0.6948018968105316,\n",
       " 0.38208115100860596,\n",
       " 0.16369295120239258,\n",
       " 0.38113492727279663,\n",
       " 0.6081636250019073,\n",
       " 0.569329023361206,\n",
       " 0.5315230190753937,\n",
       " 0.7075121402740479,\n",
       " 0.06574535369873047,\n",
       " 0.2501254081726074,\n",
       " 0.14494818449020386,\n",
       " 0.13204443454742432,\n",
       " 0.07648265361785889,\n",
       " 0.9238663166761398,\n",
       " 0.8683901429176331,\n",
       " 0.8782439902424812,\n",
       " 0.8714391440153122,\n",
       " 0.877842091023922,\n",
       " 0.8704371750354767,\n",
       " 0.05635136365890503,\n",
       " 0.1237185001373291,\n",
       " 0.7159784734249115,\n",
       " 0.02787858247756958,\n",
       " 0.16910886764526367,\n",
       " 0.5548650920391083,\n",
       " 0.568466067314148,\n",
       " 0.14086008071899414,\n",
       " 0.05801808834075928,\n",
       " 0.24134427309036255,\n",
       " 0.8844766542315483,\n",
       " ...]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 - use_simil[x] for x in range(len(use_simil))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4927"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['normed_score'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(use_simil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_simil = [cossim(use_model([data['sentence_A'][x]]), use_model([data['sentence_B'][x]])) for x in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7711464249136771, 0.0)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(use_simil, data['normed_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.random.randint(10) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.3707055437449"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a) / ((np.dot(a, a)) ** (1/2) * (np.dot(a, a)) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (np.dot(a, a) / (np.linalg.norm(a) * np.linalg.norm(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>yr</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>normed_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>24</td>\n",
       "      <td>2.5</td>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>33</td>\n",
       "      <td>3.6</td>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>63</td>\n",
       "      <td>4.2</td>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>66</td>\n",
       "      <td>1.5</td>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     test        yr  id  score                                     sentence_A  \\\n",
       "0  MSRvid  2012test  24    2.5                    A girl is styling her hair.   \n",
       "1  MSRvid  2012test  33    3.6       A group of men play soccer on the beach.   \n",
       "2  MSRvid  2012test  45    5.0  One woman is measuring another woman's ankle.   \n",
       "3  MSRvid  2012test  63    4.2                A man is cutting up a cucumber.   \n",
       "4  MSRvid  2012test  66    1.5                       A man is playing a harp.   \n",
       "\n",
       "                                         sentence_B  normed_score  \n",
       "0                      A girl is brushing her hair.          0.50  \n",
       "1  A group of boys are playing soccer on the beach.          0.72  \n",
       "2           A woman measures another woman's ankle.          1.00  \n",
       "3                      A man is slicing a cucumber.          0.84  \n",
       "4                      A man is playing a keyboard.          0.30  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./data/Stsbenchmark/test.csv')\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) \n",
    "\n",
    "df_test['normed_score'] = norm(df_test['score'])\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSRpar', 'MSRvid', 'deft-forum', 'deft-news', 'headlines',\n",
       "       'images'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_train['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSRpar', 'MSRvid', 'answer-answer', 'headlines', 'images',\n",
       "       'track5.en-en'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_test['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentence_A'] = [x.lower() for x in df_train['sentence_A']]\n",
    "df_train['sentence_B'] = [x.lower() for x in df_train['sentence_B']]\n",
    "df_test['sentence_A'] = [x.lower() for x in df_test['sentence_A']]\n",
    "df_test['sentence_B'] = [x.lower() for x in df_test['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/enwiki_20180420_300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data</th>\n",
       "      <th>desc</th>\n",
       "      <th>yr</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>normed_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>two green and white trains sitting on the tracks.</td>\n",
       "      <td>two green and white trains on tracks.</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>a small white cat with glowing eyes standing u...</td>\n",
       "      <td>a white cat stands on the floor.</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>a large boat in the water at the marina.</td>\n",
       "      <td>a large boat on the sea.</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>a bus driving in a street.</td>\n",
       "      <td>red double decker bus driving down street.</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>a passenger train waiting in a station.</td>\n",
       "      <td>a passenger train sits in the station.</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1995</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2015</td>\n",
       "      <td>1488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dog running towards camera with a ball in its ...</td>\n",
       "      <td>the black and white dog swims with a brown obj...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1996</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2015</td>\n",
       "      <td>1489</td>\n",
       "      <td>4.2</td>\n",
       "      <td>a young girl running on the beach.</td>\n",
       "      <td>a little girl running at on the shore of a beach.</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1997</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2015</td>\n",
       "      <td>1490</td>\n",
       "      <td>1.6</td>\n",
       "      <td>a baseball player throws the ball.</td>\n",
       "      <td>the basketball player holds the ball.</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1998</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2015</td>\n",
       "      <td>1493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a man is swinging on a rope over water.</td>\n",
       "      <td>a man in a maroon bathing suit swings on a rop...</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1999</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>images</td>\n",
       "      <td>2015</td>\n",
       "      <td>1498</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a young girl dressed in a minnie mouse outfit ...</td>\n",
       "      <td>a man wearing a white suit holding a newspaper...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index           data    desc    yr    id  score  \\\n",
       "0     1000  main-captions  images  2014     1    4.4   \n",
       "1     1001  main-captions  images  2014     2    2.6   \n",
       "2     1002  main-captions  images  2014     3    3.2   \n",
       "3     1003  main-captions  images  2014     4    4.0   \n",
       "4     1004  main-captions  images  2014     6    4.8   \n",
       "..     ...            ...     ...   ...   ...    ...   \n",
       "995   1995  main-captions  images  2015  1488    2.0   \n",
       "996   1996  main-captions  images  2015  1489    4.2   \n",
       "997   1997  main-captions  images  2015  1490    1.6   \n",
       "998   1998  main-captions  images  2015  1493    3.0   \n",
       "999   1999  main-captions  images  2015  1498    1.0   \n",
       "\n",
       "                                            sentence_A  \\\n",
       "0    two green and white trains sitting on the tracks.   \n",
       "1    a small white cat with glowing eyes standing u...   \n",
       "2             a large boat in the water at the marina.   \n",
       "3                           a bus driving in a street.   \n",
       "4              a passenger train waiting in a station.   \n",
       "..                                                 ...   \n",
       "995  dog running towards camera with a ball in its ...   \n",
       "996                 a young girl running on the beach.   \n",
       "997                 a baseball player throws the ball.   \n",
       "998            a man is swinging on a rope over water.   \n",
       "999  a young girl dressed in a minnie mouse outfit ...   \n",
       "\n",
       "                                            sentence_B  normed_score  \n",
       "0                two green and white trains on tracks.          0.88  \n",
       "1                     a white cat stands on the floor.          0.52  \n",
       "2                             a large boat on the sea.          0.64  \n",
       "3           red double decker bus driving down street.          0.80  \n",
       "4               a passenger train sits in the station.          0.96  \n",
       "..                                                 ...           ...  \n",
       "995  the black and white dog swims with a brown obj...          0.40  \n",
       "996  a little girl running at on the shore of a beach.          0.84  \n",
       "997              the basketball player holds the ball.          0.32  \n",
       "998  a man in a maroon bathing suit swings on a rop...          0.60  \n",
       "999  a man wearing a white suit holding a newspaper...          0.20  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['desc'] == 'images'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images = df_train[df_train['desc'] == 'images'].reset_index()\n",
    "df_test_images = df_test[df_test['test'] == 'images'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_test_images)):\n",
    "    if len(df_test_images['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_images['sentence_A'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_test_images)):\n",
    "    if len(df_test_images['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_images['sentence_B'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_train_images)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_train_images['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_train_images['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 27:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 27:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TESTING DATA\"\"\"\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_images)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_images['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_images['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 27:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 27:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "    \n",
    "with open('./data/test_a_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATA DEFINITION\"\"\"\n",
    "train_a = pickle.load(open('./data/train_a_images_w2v300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_images_w2v300.data', 'rb'))\n",
    "test_a = pickle.load(open('./data/test_a_images_w2v300.data', 'rb'))\n",
    "test_b = pickle.load(open('./data/test_b_images_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(0.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.5,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=1.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3532\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3054\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2239\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1771\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1654\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1616\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1465\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1461\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1382\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1287\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1254\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1218\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1174\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1144\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1089\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1111\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1070\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1059\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1075\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1036\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0993\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0993\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1023\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0984\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0964\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0993\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0969\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0985\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0965\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0972\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0951\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0939\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0949\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0935\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0967\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0955\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0924\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0912\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0951\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0949\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0928\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0914\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0909\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0929\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0883\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0918\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0910\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0916\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0874\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0879\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0895\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0888\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0894\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0891\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0886\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0919\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0858\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0879\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0872\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0864\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0868\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0873\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0860\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0866\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0860\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0865\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0869\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0865\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0852\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0832\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0828\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0821\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0843\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0827\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0821\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0816\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0827\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0823\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0796\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0790\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0798\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0807\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0775\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0793\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0772\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0720\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0768\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0729\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0747\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0730\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0739\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0744\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0728\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0695\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0735\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0677\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0707\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0686\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0665\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0643\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0669\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0682\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0664\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0621\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0645\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0639\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0658\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0655\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0644\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0633\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0648\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0642\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0626\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0634\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0625\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0617\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0600\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0616\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0592\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0614\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0560\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0580\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0568\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0574\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0583\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0566\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0550\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0540\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0561\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0541\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0556\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0537\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0570\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0522\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0518\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0539\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0534\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0500\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0515\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0455\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0473\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0454\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0470\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0432\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0469\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0450\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0470\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0473\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0455\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0442\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0446\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0446\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0455\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0438\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0417\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0422\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0426\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0449\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0409\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0417\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0412\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0414\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0405\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0409\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0429\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0419\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0401\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0399\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0399\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0407\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0367\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0402\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0378\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0388\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0373\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0399\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0373\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0375\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0367\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0357\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0372\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0334\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0356\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0357\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0369\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0347\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0384\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0341\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0361\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0342\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0349\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0365\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0332\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0355\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0351\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0328\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0322\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0334\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0322\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0362\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0332\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0318\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0334\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0341\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0332\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0323\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0323\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0317\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0312\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0316\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0298\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0294\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0303\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0317\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0308\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0327\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0302\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0318\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0291\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0307\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0299\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0284\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0290\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0304\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0282\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0288\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0302\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0292\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0276\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0268\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0282\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0277\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0270\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0271\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0272\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0258\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0283\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0258\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0262\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0260\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0252\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0254\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0267\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0245\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0237\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0244\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0253\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0233\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0235\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0211\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0249\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0222\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0249\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0224\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1e1561bdd8>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(df_train_images['normed_score']), \n",
    "                epochs=300, \n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.9561392711587636\n",
      "Spearmans: 0.9465485981672122\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_train_images['normed_score'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_train_images['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.4570436504390102\n",
      "Spearmans: 0.4217631754731278\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)]) #53.7\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_test_images['normed_score'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_test_images['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "50, glorot_uniform, 2.5, 0.35, Adadelta(0.5, 0.9, 1.5)\n",
    "\"\"\"\n",
    "siamese_net.save_weights('./models/siam/images/') # 0.7010318394360036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headlines (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/sts2015_headlines/train_input.txt', 'r')\n",
    "data = list()\n",
    "for line in file:\n",
    "    data.append(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/sts2015_headlines/train_scores.txt', 'r')\n",
    "labels = list()\n",
    "for line in file:\n",
    "    labels.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    if labels[i] == '\\n':\n",
    "        labels[i] = None\n",
    "    if type(labels[i]) == str:\n",
    "        labels[i] = float(labels[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_headlines = pd.DataFrame(data, columns=['sentence_A', 'sentence_B'])\n",
    "df_train_headlines['score'] = labels\n",
    "df_train_headlines['normed_scre'] = norm(df_train_headlines['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/sts2015_headlines/test_all.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_headlines = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "df_test_headlines = df_test_headlines.iloc[:750,:]\n",
    "df_test_headlines['scores'] = [float(x) for x in df_test_headlines['scores']]\n",
    "df_test_headlines['normed_scre'] = norm(df_test_headlines['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_train_headlines)):\n",
    "    if len(df_train_headlines['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_train_headlines['sentence_A'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_test_headlines)):\n",
    "    if len(df_test_headlines['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_headlines['sentence_B'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_train_headlines)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_train_headlines['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_train_headlines['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 35:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 35:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TESTING DATA\"\"\"\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_headlines)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_headlines['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_headlines['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 27:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 27:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "    \n",
    "with open('./data/test_a_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATA DEFINITION\"\"\"\n",
    "train_a = pickle.load(open('./data/train_a_headlines_w2v300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_headlines_w2v300.data', 'rb'))\n",
    "test_a = pickle.load(open('./data/test_a_headlines_w2v300.data', 'rb'))\n",
    "test_b = pickle.load(open('./data/test_b_headlines_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(0.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.5,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=1.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.3516\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.3212\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.2823\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1814\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1462\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1354\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1235\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1159\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1087\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1027\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0999\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0963\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0934\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0929\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0886\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0879\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0855\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0854\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0867\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0811\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0821\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0814\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0806\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0809\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0799\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0794\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0795\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0793\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0769\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0779\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0779\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0771\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0780\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0775\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0776\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0759\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0774\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0768\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0776\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0764\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0778\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0770\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0771\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0758\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0764\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0760\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0755\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0763\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0761\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1df706fcf8>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(df_train_headlines['normed_scre']), \n",
    "                epochs=50, \n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.959683653388316\n",
      "Spearmans: 0.9567902315961634\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_train_headlines['normed_scre'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_train_headlines['normed_scre'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.8711128255491617\n",
      "Spearmans: 0.8600551135784084\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a), np.array(test_b)]) #53.7\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_test_headlines['normed_scre'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_test_headlines['normed_scre'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "50, glorot_uniform, 0.5, 0.35, Adadelta(0.5, 0.9, 1.5), 600\n",
    "\"\"\"\n",
    "siamese_net.save_weights('./models/siam/headlines/') # 0.8746802156351821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/semeval_2015_all_train.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    test.append(a)\n",
    "df_train_all = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "df_train_all = df_train_all.iloc[:3000,:]\n",
    "df_train_all['scores'] = [float(x) for x in df_train_all['scores']]\n",
    "df_train_all['normed_scre'] = norm(df_train_all['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>normed_scre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>the activity of learning or being trained</td>\n",
       "      <td>the gradual process of acquiring knowledge.</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.8</td>\n",
       "      <td>thin metal strips used in typesetting</td>\n",
       "      <td>thin strip of metal used to separate lines of ...</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>falcon: abandon intended game and pursue lesse...</td>\n",
       "      <td>abandon the intended prey, turn, and pursue an...</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.8</td>\n",
       "      <td>the act of designating a role to someone</td>\n",
       "      <td>the act of designating or identifying something.</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>make pure or free from sin</td>\n",
       "      <td>make pure or free from sin or guilt.</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores                                         sentence_A  \\\n",
       "0     4.0          the activity of learning or being trained   \n",
       "1     3.8              thin metal strips used in typesetting   \n",
       "2     4.2  falcon: abandon intended game and pursue lesse...   \n",
       "3     1.8           the act of designating a role to someone   \n",
       "4     4.0                         make pure or free from sin   \n",
       "\n",
       "                                          sentence_B  normed_scre  \n",
       "0        the gradual process of acquiring knowledge.         0.80  \n",
       "1  thin strip of metal used to separate lines of ...         0.76  \n",
       "2  abandon the intended prey, turn, and pursue an...         0.84  \n",
       "3   the act of designating or identifying something.         0.36  \n",
       "4               make pure or free from sin or guilt.         0.80  "
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('./data/onwn_test.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_own = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "\n",
    "df_test_own = df_test_own.iloc[:len(df_test_own)-1,:]\n",
    "df_test_own['scores'] = [float(x) for x in df_test_own['scores']]\n",
    "df_test_own['normed_scre'] = norm(df_test_own['scores'])\n",
    "df_test_own.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_test_own)):\n",
    "    if len(df_test_own['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_own['sentence_A'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 0\n",
    "for i in range(len(df_test_own)):\n",
    "    if len(df_test_own['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_own['sentence_B'][i].split(' '))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_own)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_own['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_own['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 21:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 21:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_own_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_own_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'453'"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.correction('453')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scores                                         sentence_A  \\\n",
      "0     3.0  How about some testimonies from real health ex...   \n",
      "1     0.8                         Then the captain was gone.   \n",
      "2     3.8                  The problem is simpler than that.   \n",
      "3     1.0              NEGATIVE RECONNAISSANCE REQUIREMENTS.   \n",
      "4     0.4  So, I am dropping, for now, asking you the que...   \n",
      "\n",
      "                                          sentence_B  normed_scre  \n",
      "0  Also, who's to say there aren't testimonies fr...         0.60  \n",
      "1                        Then the captain came back.         0.16  \n",
      "2                             The problem is simple.         0.76  \n",
      "3                             PACIFIC REQUIREMENTS .         0.20  \n",
      "4  I am moving on to ask you the question:  Where...         0.08  \n",
      "18\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/forum_test.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '').replace('\"', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_forum = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "\n",
    "df_test_forum = df_test_forum.iloc[:len(df_test_forum)-1,:]\n",
    "df_test_forum['scores'] = [float(x) for x in df_test_forum['scores']]\n",
    "df_test_forum['normed_scre'] = norm(df_test_forum['scores'])\n",
    "print(df_test_forum.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_forum)):\n",
    "    if len(df_test_forum['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_forum['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_forum)):\n",
    "    if len(df_test_forum['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_forum['sentence_B'][i].split(' '))\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [03:14<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How about some testimonies from real health experts?'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctedA = list()\n",
    "correctedB = list()\n",
    "for i in tqdm(range(len(df_test_forum))):\n",
    "    corA = ' '.join([sc.correction(x) for x in df_test_forum['sentence_A'][i].split(' ')])\n",
    "    corB = ' '.join([sc.correction(x) for x in df_test_forum['sentence_B'][i].split(' ')])\n",
    "    correctedA.append(corA)\n",
    "    correctedB.append(corB)\n",
    "df_test_forum['sentence_A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_forum['sentence_A'] = correctedA\n",
    "df_test_forum['sentence_B'] = correctedB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      also whos to say there arendt testimonies from...\n",
       "1                             Then the captain came back\n",
       "2                                  The problem is simple\n",
       "3                                 PACIFIC REQUIREMENTS .\n",
       "4      I am moving on to ask you the question \" Where...\n",
       "                             ...                        \n",
       "445                   What is your definition of \"soul\"?\n",
       "446    Aye but they ARE responsible to their constitu...\n",
       "447    The ideology was built around violent expansio...\n",
       "448             @ossobuco, Could this have been an enema\n",
       "449       taking advantage has the effect of reinforcing\n",
       "Name: sentence_B, Length: 450, dtype: object"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_forum['sentence_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. 1so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_forum)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_forum['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_forum['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 18:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 18:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_forum_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_forum_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scores                                         sentence_A  \\\n",
      "0     4.0      mexico wishes to guarantee citizens' safety.    \n",
      "1     4.2  spain currently holds the rotating presidency ...   \n",
      "2     4.2              the treaty was first signed in 1990.    \n",
      "3     3.2     gorgich and pashtoon were executed in prison.    \n",
      "4     0.6  safe bourada was sentenced to 15 years in pris...   \n",
      "\n",
      "                                          sentence_B  normed_scre  \n",
      "0              mexico wishes to avoid more violence.         0.80  \n",
      "1       spain currently holds the osce's presidency.         0.84  \n",
      "2                 the cfe treaty was signed in 1990.         0.84  \n",
      "3  gorgich and pashtoon were executed for traffic...         0.64  \n",
      "4        djamel badaoui was sentenced to five years.         0.12  \n",
      "57\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/news_test.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_news = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "\n",
    "df_test_news = df_test_news.iloc[:len(df_test_news)-1,:]\n",
    "df_test_news['scores'] = [float(x) for x in df_test_news['scores']]\n",
    "df_test_news['normed_scre'] = norm(df_test_news['scores'])\n",
    "print(df_test_news.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_news)):\n",
    "    if len(df_test_news['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_news['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_news)):\n",
    "    if len(df_test_news['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_news['sentence_B'][i].split(' '))\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_news)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_news['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_news['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 57:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 57:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_news_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_news_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scores                                         sentence_A  \\\n",
      "0     3.6                   A cat standing on tree branches.   \n",
      "1     4.4  Two green and white trains sitting on the tracks.   \n",
      "2     2.6  A small white cat with glowing eyes standing u...   \n",
      "3     3.2           A large boat in the water at the marina.   \n",
      "4     4.0                         a bus driving in a street.   \n",
      "\n",
      "                                          sentence_B  normed_scre  \n",
      "0  A black and white cat is high up on tree branc...         0.72  \n",
      "1              Two green and white trains on tracks.         0.88  \n",
      "2                   A white cat stands on the floor.         0.52  \n",
      "3                           A large boat on the sea.         0.64  \n",
      "4         Red double decker bus driving down street.         0.80  \n",
      "22\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/images_test.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_images = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "\n",
    "df_test_images = df_test_images.iloc[:len(df_test_images)-1,:]\n",
    "df_test_images['scores'] = [float(x) for x in df_test_images['scores']]\n",
    "df_test_images['normed_scre'] = norm(df_test_images['scores'])\n",
    "print(df_test_images.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_images)):\n",
    "    if len(df_test_images['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_images['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_images)):\n",
    "    if len(df_test_images['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_images['sentence_B'][i].split(' '))\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_images)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_images['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_images['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 27:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 27:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_images_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scores                                         sentence_A  \\\n",
      "0     0.6  there exist a number of different possible eve...   \n",
      "1     0.8  this frame contians lus which are like those o...   \n",
      "2     0.8  a recipient starts off without the theme in th...   \n",
      "3     1.2  this frame concerns a specific subset of insta...   \n",
      "4     0.4  this frame contains words that describe an ite...   \n",
      "\n",
      "                                          sentence_B  normed_scre  \n",
      "0                   doing as one pleases or chooses;         0.15  \n",
      "1  being or characteristic of or appropriate to e...         0.20  \n",
      "2               win something through one's efforts;         0.20  \n",
      "3                     a standard or typical example;         0.30  \n",
      "4  lacking in specific resources, qualities or su...         0.10  \n",
      "22\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/headlines_2013_test.txt')\n",
    "test = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '').replace('#', '') for x in a]\n",
    "    test.append(a)\n",
    "df_test_headlines = pd.DataFrame(test, columns=['scores', 'sentence_A', 'sentence_B'])\n",
    "\n",
    "df_test_headlines = df_test_headlines.iloc[:len(df_test_headlines)-1,:]\n",
    "df_test_headlines['scores'] = [float(x) for x in df_test_headlines['scores']]\n",
    "df_test_headlines['normed_scre'] = norm(df_test_headlines['scores'])\n",
    "print(df_test_fnwn.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_headlines)):\n",
    "    if len(df_test_headlines['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_test_headlines['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_test_headlines)):\n",
    "    if len(df_test_headlines['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_test_headlines['sentence_B'][i].split(' '))\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_test_headlines)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_test_headlines['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_test_headlines['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 22:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 22:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_headlines_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATA DEFINITION\"\"\"\n",
    "train_a = pickle.load(open('./data/train_a_all_w2v300.data', 'rb'))\n",
    "train_b = pickle.load(open('./data/train_b_all_w2v300.data', 'rb'))\n",
    "\n",
    "test_a_head = pickle.load(open('./data/test_a_headlines_w2v300.data', 'rb'))\n",
    "test_b_head = pickle.load(open('./data/test_b_headlines_w2v300.data', 'rb'))\n",
    "test_a_own = pickle.load(open('./data/test_a_own_w2v300.data', 'rb'))\n",
    "test_b_own = pickle.load(open('./data/test_b_own_w2v300.data', 'rb'))\n",
    "test_a_forum = pickle.load(open('./data/test_a_forum_w2v300.data', 'rb'))\n",
    "test_b_forum = pickle.load(open('./data/test_b_forum_w2v300.data', 'rb'))\n",
    "test_a_news = pickle.load(open('./data/test_a_news_w2v300.data', 'rb'))\n",
    "test_b_news = pickle.load(open('./data/test_b_news_w2v300.data', 'rb'))\n",
    "test_a_images = pickle.load(open('./data/test_a_images_w2v300.data', 'rb'))\n",
    "test_b_images = pickle.load(open('./data/test_b_images_w2v300.data', 'rb'))\n",
    "test_a_tweets = pickle.load(open('./data/test_a_tweets_w2v300.data', 'rb'))\n",
    "test_b_tweets = pickle.load(open('./data/test_b_tweets_w2v300.data', 'rb'))\n",
    "\n",
    "\n",
    "test_a_fnwn = pickle.load(open('./data/test_a_fnwn_w2v300.data', 'rb'))\n",
    "test_b_fnwn = pickle.load(open('./data/test_b_fnwn_w2v300.data', 'rb'))\n",
    "test_a_own = pickle.load(open('./data/test_a_own_w2v300.data', 'rb'))\n",
    "test_b_own = pickle.load(open('./data/test_b_own_w2v300.data', 'rb'))\n",
    "test_a_head = pickle.load(open('./data/test_a_headlines_w2v300.data', 'rb'))\n",
    "test_b_head = pickle.load(open('./data/test_b_headlines_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_a_own' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-551-98542f8a7edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtest_a_fnwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtest_b_fnwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtest_a_own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtest_b_own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_a_own' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"DATA DEFINITION\"\"\"\n",
    "del train_a\n",
    "del train_b\n",
    "\n",
    "del test_a_head\n",
    "del test_b_head\n",
    "del test_a_own\n",
    "del test_b_own\n",
    "del test_a_forum\n",
    "del test_b_forum\n",
    "del test_a_news\n",
    "del test_b_news\n",
    "del test_a_images\n",
    "del test_b_images\n",
    "del test_a_tweets\n",
    "del test_b_tweets\n",
    "\n",
    "del test_a_fnwn\n",
    "del test_b_fnwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(0.5),\n",
    "                        dropout=0.4)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.95,\n",
    "                                                           clipvalue=3), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0188\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0183\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0191\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0189\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0185\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0184\n",
      "Epoch 9/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0187\n",
      "Epoch 10/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 11/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0182\n",
      "Epoch 12/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0176\n",
      "Epoch 13/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0184\n",
      "Epoch 14/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 15/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0183\n",
      "Epoch 16/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0181\n",
      "Epoch 17/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0187\n",
      "Epoch 18/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0186\n",
      "Epoch 19/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0188\n",
      "Epoch 20/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0179\n",
      "Epoch 21/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0193\n",
      "Epoch 22/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0184\n",
      "Epoch 23/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0184\n",
      "Epoch 24/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0174\n",
      "Epoch 25/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0187\n",
      "Epoch 26/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0176\n",
      "Epoch 27/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0182\n",
      "Epoch 28/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0182\n",
      "Epoch 29/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 30/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0185\n",
      "Epoch 31/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0189\n",
      "Epoch 32/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0183\n",
      "Epoch 33/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0176\n",
      "Epoch 34/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 35/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0187\n",
      "Epoch 36/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0177\n",
      "Epoch 37/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 38/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0182\n",
      "Epoch 39/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 40/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0187\n",
      "Epoch 41/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0177\n",
      "Epoch 42/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0181\n",
      "Epoch 43/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 44/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0173\n",
      "Epoch 45/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 46/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0173\n",
      "Epoch 47/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0186\n",
      "Epoch 48/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0174\n",
      "Epoch 49/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 50/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 51/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 52/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0183\n",
      "Epoch 53/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 54/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0181\n",
      "Epoch 55/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0181\n",
      "Epoch 56/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0190\n",
      "Epoch 57/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 58/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0177\n",
      "Epoch 59/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 60/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0177\n",
      "Epoch 61/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 62/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0172\n",
      "Epoch 63/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0179\n",
      "Epoch 64/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0173\n",
      "Epoch 65/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0168\n",
      "Epoch 66/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0172\n",
      "Epoch 67/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0172\n",
      "Epoch 68/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0181\n",
      "Epoch 69/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0171\n",
      "Epoch 70/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 71/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0171\n",
      "Epoch 72/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 73/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0180\n",
      "Epoch 74/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0176\n",
      "Epoch 75/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0179\n",
      "Epoch 76/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0168\n",
      "Epoch 77/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0172\n",
      "Epoch 78/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0171\n",
      "Epoch 79/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0177\n",
      "Epoch 80/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 81/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0182\n",
      "Epoch 82/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0171\n",
      "Epoch 83/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0178\n",
      "Epoch 84/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0167\n",
      "Epoch 85/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0173\n",
      "Epoch 86/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 87/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0167\n",
      "Epoch 88/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0169\n",
      "Epoch 89/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0169\n",
      "Epoch 90/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0174\n",
      "Epoch 91/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 92/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0175\n",
      "Epoch 93/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0164\n",
      "Epoch 94/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0169\n",
      "Epoch 95/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0161\n",
      "Epoch 96/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Epoch 97/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0173\n",
      "Epoch 98/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0160\n",
      "Epoch 99/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0176\n",
      "Epoch 100/100\n",
      "94/94 [==============================] - 1s 10ms/step - loss: 0.0165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d272c4518>"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a), np.array(train_b)], \n",
    "                np.array(df_train_all['normed_scre']), \n",
    "                epochs=100, \n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    results = dict()\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "    results['train'] = pearsonr([x[0] for x in preds.tolist()], df_train_all['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_head), np.array(test_b_head)])\n",
    "    results['headlines'] = pearsonr([x[0] for x in preds.tolist()], df_test_headlines['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_own), np.array(test_b_own)]) \n",
    "    results['own'] = pearsonr([x[0] for x in preds.tolist()], df_test_own['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_forum), np.array(test_b_forum)]) \n",
    "    results['forum'] = pearsonr([x[0] for x in preds.tolist()], df_test_forum['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_news), np.array(test_b_news)]) \n",
    "    results['news'] = pearsonr([x[0] for x in preds.tolist()], df_test_news['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_images), np.array(test_b_images)]) \n",
    "    results['images'] = pearsonr([x[0] for x in preds.tolist()], df_test_images['normed_scre'])[0]\n",
    "    \n",
    "    preds = siamese_net.predict([np.array(test_a_tweets), np.array(test_b_tweets)]) \n",
    "    results['tweets'] = pearsonr([x[0] for x in preds.tolist()], df_test_tweets['normed_scre'])[0]\n",
    "    \n",
    "    avg = np.mean([results['headlines'], \n",
    "                   results['own'], \n",
    "                   results['forum'], \n",
    "                   results['news'], \n",
    "                   results['images'], \n",
    "                   #results['tweets']\n",
    "                  ])\n",
    "    avg2 = np.mean([results['headlines'], \n",
    "                   results['own'], \n",
    "                   results['forum'], \n",
    "                   results['news'], \n",
    "                   results['images'], \n",
    "                   results['tweets']\n",
    "                  ])\n",
    "    results['mean_all'] = avg2\n",
    "    results['mean_notweet'] = avg\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2014 CLASS RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.9654789681246029,\n",
       " 'headlines': 0.9049427711299217,\n",
       " 'own': 0.9098780349979988,\n",
       " 'forum': 0.7169983776916885,\n",
       " 'news': 0.9671735219910839,\n",
       " 'images': 0.9134596792636631,\n",
       " 'tweets': 0.10263018755546863,\n",
       " 'mean_all': 0.7525137621049708,\n",
       " 'mean_notweet': 0.8824904770148713}"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = evaluate()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "50, glorot_uniform, 0.5, 0.35, Adadelta(1, 0.9, 2), 800\n",
    "\"\"\"\n",
    "siamese_net.save_weights('./models/siam/all/') # 0.753/0.882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f18dd579978>"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.load_weights('./models/siam/all/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-877-775d37ee0364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_train_all['normed_scre'])[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_train_all['normed_scre'])[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_a' is not defined"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a), np.array(train_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_train_all['normed_scre'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_train_all['normed_scre'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.7203484664972691\n",
      "Spearmans: 0.7139978071154707\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a_head), np.array(test_b_head)]) #53.7\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_test_headlines['normed_scre'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_test_headlines['normed_scre'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds = siamese_net.predict([np.array(test_a_own), np.array(test_b_own)]) #53.7\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_test_own['normed_scre'])[0]}\")\n",
    "print(f\"Spearmans: {spearmanr([x[0] for x in preds.tolist()], df_test_own['normed_scre'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/mrpc/train.txt')\n",
    "header = file.readline().split('\\t')\n",
    "header[3] = 'sentence_A'\n",
    "header[4] = 'sentence_B'\n",
    "file = open('./data/mrpc/train.txt')\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "    \n",
    "df_mrpc_train = pd.DataFrame(data, columns=header)\n",
    "\n",
    "df_mrpc_train = df_mrpc_train.iloc[1:len(df_mrpc_train)-1,:]\n",
    "df_mrpc_train = df_mrpc_train.reset_index()\n",
    "df_mrpc_train['Quality'] = [int(x) for x in df_mrpc_train['Quality']]\n",
    "# df_mrpc_train = df_mrpc_train.drop(df_mrpc_train.index[bad_idxs])\n",
    "# df_mrpc_train = df_mrpc_train.reset_index()\n",
    "# print(df_mrpc_train.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_mrpc_train)):\n",
    "    if len(df_mrpc_train['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_mrpc_train['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_mrpc_train)):\n",
    "    if len(df_mrpc_train['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_mrpc_train['sentence_B'][i].split(' '))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>4072</td>\n",
       "      <td>1</td>\n",
       "      <td>1620264</td>\n",
       "      <td>1620507</td>\n",
       "      <td>\"At this point, Mr. Brando announced: 'Somebod...</td>\n",
       "      <td>Brando said that \"somebody ought to put a bull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>4073</td>\n",
       "      <td>0</td>\n",
       "      <td>1848001</td>\n",
       "      <td>1848224</td>\n",
       "      <td>Martin, 58, will be freed today after serving ...</td>\n",
       "      <td>Martin served two thirds of a five-year senten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>4074</td>\n",
       "      <td>1</td>\n",
       "      <td>747160</td>\n",
       "      <td>747144</td>\n",
       "      <td>\"We have concluded that the outlook for price ...</td>\n",
       "      <td>In a statement, the ECB said the outlook for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>4075</td>\n",
       "      <td>1</td>\n",
       "      <td>2539933</td>\n",
       "      <td>2539850</td>\n",
       "      <td>The notification was first reported Friday by ...</td>\n",
       "      <td>MSNBC.com first reported the CIA request on Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>4076</td>\n",
       "      <td>0</td>\n",
       "      <td>453575</td>\n",
       "      <td>453448</td>\n",
       "      <td>The 30-year bond US30YT=RR rose 22/32 for a yi...</td>\n",
       "      <td>The 30-year bond US30YT=RR grew 1-3/32 for a y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Quality    #1 ID    #2 ID  \\\n",
       "4071   4072        1  1620264  1620507   \n",
       "4072   4073        0  1848001  1848224   \n",
       "4073   4074        1   747160   747144   \n",
       "4074   4075        1  2539933  2539850   \n",
       "4075   4076        0   453575   453448   \n",
       "\n",
       "                                             sentence_A  \\\n",
       "4071  \"At this point, Mr. Brando announced: 'Somebod...   \n",
       "4072  Martin, 58, will be freed today after serving ...   \n",
       "4073  \"We have concluded that the outlook for price ...   \n",
       "4074  The notification was first reported Friday by ...   \n",
       "4075  The 30-year bond US30YT=RR rose 22/32 for a yi...   \n",
       "\n",
       "                                             sentence_B  \n",
       "4071  Brando said that \"somebody ought to put a bull...  \n",
       "4072  Martin served two thirds of a five-year senten...  \n",
       "4073  In a statement, the ECB said the outlook for p...  \n",
       "4074  MSNBC.com first reported the CIA request on Fr...  \n",
       "4075  The 30-year bond US30YT=RR grew 1-3/32 for a y...  "
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrpc_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrpc_train['sentence_A'] = [x.lower() for x in df_mrpc_train['sentence_A']]\n",
    "df_mrpc_train['sentence_B'] = [x.lower() for x in df_mrpc_train['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_mrpc_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_mrpc_train['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_mrpc_train['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 31:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 31:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_mrpc_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_mrpc_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/mrpc/test.txt')\n",
    "header = file.readline().split('\\t')\n",
    "header[3] = 'sentence_A'\n",
    "header[4] = 'sentence_B'\n",
    "file = open('./data/mrpc/test.txt')\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "    \n",
    "df_mrpc_test= pd.DataFrame(data, columns=header)\n",
    "df_mrpc_test = df_mrpc_test.iloc[1:len(df_mrpc_test)-1,:]\n",
    "df_mrpc_test = df_mrpc_test.reset_index()\n",
    "df_mrpc_test['Quality'] = [int(x) for x in df_mrpc_test['Quality']]\n",
    "# df_mrpc_test = df_mrpc_test.drop(df_mrpc_test.index[bad_idxs])\n",
    "# df_mrpc_test = df_mrpc_test.reset_index()\n",
    "# print(df_mrpc_test.head())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_mrpc_test)):\n",
    "    if len(df_mrpc_test['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_mrpc_test['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_mrpc_test)):\n",
    "    if len(df_mrpc_test['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_mrpc_test['sentence_B'][i].split(' '))\n",
    "print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrpc_test['sentence_A'] = [x.lower() for x in df_mrpc_test['sentence_A']]\n",
    "df_mrpc_test['sentence_B'] = [x.lower() for x in df_mrpc_test['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_mrpc_test)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_mrpc_test['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_mrpc_test['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 30:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 30:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_mrpc_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_mrpc_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_vec_a\n",
    "del all_vec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mrpc_a = pickle.load(open('./data/train_a_mrpc_w2v300.data', 'rb'))\n",
    "train_mrpc_b = pickle.load(open('./data/train_b_mrpc_w2v300.data', 'rb'))\n",
    "\n",
    "test_mrpc_a = pickle.load(open('./data/test_a_mrpc_w2v300.data', 'rb'))\n",
    "test_mrpc_b = pickle.load(open('./data/test_b_mrpc_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.3)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='sigmoid')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=1.5), \n",
    "                    loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2282\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2320\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2345\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2233\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2129\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2281\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2157\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2217\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2218\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2243\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2157\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2199\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2269\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2199\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2253\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2229\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2234\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2199\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2273\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2268\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2168\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2174\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2160\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2181\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2137\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2194\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2089\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2174\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2164\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2237\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2120\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2180\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2154\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2182\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2049\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2073\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2059\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2142\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2192\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2084\n",
      "Epoch 41/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2066\n",
      "Epoch 42/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2137\n",
      "Epoch 43/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2119\n",
      "Epoch 44/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2163\n",
      "Epoch 45/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2114\n",
      "Epoch 46/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2095\n",
      "Epoch 47/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2047\n",
      "Epoch 48/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2152\n",
      "Epoch 49/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2076\n",
      "Epoch 50/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2179\n",
      "Epoch 51/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2108\n",
      "Epoch 52/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1991\n",
      "Epoch 53/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2043\n",
      "Epoch 54/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2110\n",
      "Epoch 55/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1998\n",
      "Epoch 56/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2031\n",
      "Epoch 57/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2056\n",
      "Epoch 58/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2008\n",
      "Epoch 59/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1987\n",
      "Epoch 60/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1953\n",
      "Epoch 61/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1914\n",
      "Epoch 62/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1934\n",
      "Epoch 63/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1993\n",
      "Epoch 64/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2049\n",
      "Epoch 65/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1958\n",
      "Epoch 66/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1992\n",
      "Epoch 67/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1986\n",
      "Epoch 68/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1990\n",
      "Epoch 69/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1950\n",
      "Epoch 70/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1991\n",
      "Epoch 71/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2024\n",
      "Epoch 72/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1911\n",
      "Epoch 73/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1862\n",
      "Epoch 74/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1958\n",
      "Epoch 75/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1919\n",
      "Epoch 76/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2016\n",
      "Epoch 77/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1992\n",
      "Epoch 78/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2052\n",
      "Epoch 79/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1901\n",
      "Epoch 80/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2044\n",
      "Epoch 81/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1959\n",
      "Epoch 82/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1948\n",
      "Epoch 83/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1914\n",
      "Epoch 84/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1880\n",
      "Epoch 85/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1909\n",
      "Epoch 86/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1967\n",
      "Epoch 87/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2004\n",
      "Epoch 88/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1847\n",
      "Epoch 89/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1868\n",
      "Epoch 90/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1811\n",
      "Epoch 91/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1896\n",
      "Epoch 92/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2038\n",
      "Epoch 93/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2051\n",
      "Epoch 94/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1977\n",
      "Epoch 95/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1928\n",
      "Epoch 96/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1781\n",
      "Epoch 97/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1888\n",
      "Epoch 98/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1992\n",
      "Epoch 99/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1938\n",
      "Epoch 100/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1972\n",
      "Epoch 101/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1911\n",
      "Epoch 102/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1824\n",
      "Epoch 103/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1898\n",
      "Epoch 104/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1772\n",
      "Epoch 105/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1833\n",
      "Epoch 106/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1772\n",
      "Epoch 107/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1809\n",
      "Epoch 108/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1770\n",
      "Epoch 109/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1871\n",
      "Epoch 110/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1748\n",
      "Epoch 111/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1786\n",
      "Epoch 112/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1874\n",
      "Epoch 113/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1802\n",
      "Epoch 114/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1868\n",
      "Epoch 115/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1821\n",
      "Epoch 116/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1741\n",
      "Epoch 117/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1782\n",
      "Epoch 118/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1793\n",
      "Epoch 119/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1756\n",
      "Epoch 120/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1896\n",
      "Epoch 121/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1835\n",
      "Epoch 122/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1805\n",
      "Epoch 123/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1887\n",
      "Epoch 124/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1739\n",
      "Epoch 125/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1870\n",
      "Epoch 126/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1881\n",
      "Epoch 127/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1837\n",
      "Epoch 128/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1748\n",
      "Epoch 129/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1675\n",
      "Epoch 130/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1868\n",
      "Epoch 131/200\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1763\n",
      "Epoch 132/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1678\n",
      "Epoch 133/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1800\n",
      "Epoch 134/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1884\n",
      "Epoch 135/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1838\n",
      "Epoch 136/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1885\n",
      "Epoch 137/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1887\n",
      "Epoch 138/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1753\n",
      "Epoch 139/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1853\n",
      "Epoch 140/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1677\n",
      "Epoch 141/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1674\n",
      "Epoch 142/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1790\n",
      "Epoch 143/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1651\n",
      "Epoch 144/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1841\n",
      "Epoch 145/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1801\n",
      "Epoch 146/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1802\n",
      "Epoch 147/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1766\n",
      "Epoch 148/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1791\n",
      "Epoch 149/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1722\n",
      "Epoch 150/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1733\n",
      "Epoch 151/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1711\n",
      "Epoch 152/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1664\n",
      "Epoch 153/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1810\n",
      "Epoch 154/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1737\n",
      "Epoch 155/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1642\n",
      "Epoch 156/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1745\n",
      "Epoch 157/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1668\n",
      "Epoch 158/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1674\n",
      "Epoch 159/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1735\n",
      "Epoch 160/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1860\n",
      "Epoch 161/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1769\n",
      "Epoch 162/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1769\n",
      "Epoch 163/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1692\n",
      "Epoch 164/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1670\n",
      "Epoch 165/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1699\n",
      "Epoch 166/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1697\n",
      "Epoch 167/200\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1734\n",
      "Epoch 168/200\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1732\n",
      "Epoch 169/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1640\n",
      "Epoch 170/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1720\n",
      "Epoch 171/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1713\n",
      "Epoch 172/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1704\n",
      "Epoch 173/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1688\n",
      "Epoch 174/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1732\n",
      "Epoch 175/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1750\n",
      "Epoch 176/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1613\n",
      "Epoch 177/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1737\n",
      "Epoch 178/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1737\n",
      "Epoch 179/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1740\n",
      "Epoch 180/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1746\n",
      "Epoch 181/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1498\n",
      "Epoch 182/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1588\n",
      "Epoch 183/200\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1678\n",
      "Epoch 184/200\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1578\n",
      "Epoch 185/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1604\n",
      "Epoch 186/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1752\n",
      "Epoch 187/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1675\n",
      "Epoch 188/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1536\n",
      "Epoch 189/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1670\n",
      "Epoch 190/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1659\n",
      "Epoch 191/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1602\n",
      "Epoch 192/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1655\n",
      "Epoch 193/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1731\n",
      "Epoch 194/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1548\n",
      "Epoch 195/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1634\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1721\n",
      "Epoch 197/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1486\n",
      "Epoch 198/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1526\n",
      "Epoch 199/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1711\n",
      "Epoch 200/200\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.1582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1cb8c9d828>"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_mrpc_a), np.array(train_mrpc_b)], \n",
    "                np.array(df_mrpc_train['Quality']), \n",
    "                epochs=200, \n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9774288518155054\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_mrpc_a), np.array(train_mrpc_b)])\n",
    "preds = [round(x[0]) for x in preds]\n",
    "print(f'Train acc: {np.sum([preds[x] == df_mrpc_train[\"Quality\"].tolist()[x] for x in range(len(preds))]) / len(preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.6301449275362319\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_mrpc_a), np.array(test_mrpc_b)])\n",
    "preds = [abs(round(x[0])) for x in preds]\n",
    "print(f'Test acc: {np.sum([preds[x] == df_mrpc_test[\"Quality\"].tolist()[x] for x in range(len(preds))]) / len(preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  sentence_A  \\\n",
      "0   A woman and man are dancing in the rain.   \n",
      "1                        Someone is drawing.   \n",
      "2  A man and a woman are kissing each other.   \n",
      "3               A woman is slicing an onion.   \n",
      "4                A person is peeling shrimp.   \n",
      "\n",
      "                                     sentence_B  score  normed_score  \n",
      "0          A man and woman are dancing in rain.    5.0          1.00  \n",
      "1                           Someone is dancing.    0.3          0.06  \n",
      "2  A man and a woman are talking to each other.    0.6          0.12  \n",
      "3                  A woman is cutting an onion.    4.2          0.84  \n",
      "4                 A person is preparing shrimp.    3.6          0.72  \n",
      "                           sentence_A                    sentence_B  score  \\\n",
      "744     Two men are dancing together.       A woman opens a closet.    0.0   \n",
      "745  A woman is running on the beach.  A dog is swimming in a pool.    0.0   \n",
      "746        A man is reading an email.    A person opening a banana.    0.0   \n",
      "747         A man is straining pasta.   A man plays a wooden flute.    0.0   \n",
      "748          Panda's play on a swing.      A man is playing guitar.    0.8   \n",
      "\n",
      "     normed_score  \n",
      "744          0.00  \n",
      "745          0.00  \n",
      "746          0.00  \n",
      "747          0.00  \n",
      "748          0.16  \n",
      "24\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/2013/train/STS.input.MSRvid.txt')\n",
    "file.readline()\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "df_msrvid_train = pd.DataFrame(data, columns=['sentence_A', 'sentence_B'])\n",
    "\n",
    "file = open('./data/2013/train/STS.gs.MSRvid.txt')\n",
    "file.readline()\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.replace('\\n', '')\n",
    "    data.append(a)\n",
    "    \n",
    "df_msrvid_train['score'] = [float(x) for x in data]\n",
    "df_msrvid_train['normed_score'] = norm(df_msrvid_train['score'])\n",
    "\n",
    "print(df_msrvid_train.head())\n",
    "print(df_msrvid_train.tail())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_train)):\n",
    "    if len(df_msrvid_train['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_train['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_train)):\n",
    "    if len(df_msrvid_train['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_train['sentence_B'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "df_msrvid_train['sentence_A'] = [x.lower() for x in df_msrvid_train['sentence_A']]\n",
    "df_msrvid_train['sentence_B'] = [x.lower() for x in df_msrvid_train['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_msrvid_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_msrvid_train['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_msrvid_train['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 24:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 24:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_msrvid_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_msrvid_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      sentence_A  \\\n",
      "0                         A plane is taking off.   \n",
      "1               A young child is riding a horse.   \n",
      "2           A man is feeding a mouse to a snake.   \n",
      "3                A man is playing a large flute.   \n",
      "4  A man is spreading shreded cheese on a pizza.   \n",
      "\n",
      "                                          sentence_B  score  normed_score  \n",
      "0                        An air plane is taking off.   5.00          1.00  \n",
      "1                         A child is riding a horse.   4.75          0.95  \n",
      "2           The man is feeding a mouse to the snake.   5.00          1.00  \n",
      "3                          A man is playing a flute.   3.80          0.76  \n",
      "4  A man is spreading shredded cheese on an uncoo...   3.80          0.76  \n",
      "                              sentence_A                       sentence_B  \\\n",
      "744  A submarine is going through water.        A baby is falling asleep.   \n",
      "745        A man is running on a street.          A man plays the guitar.   \n",
      "746        A person is slicing an onion.  A cat is pooping into a toilet.   \n",
      "747     A badger is digging in the dirt.     A man is moving large rocks.   \n",
      "748       A woman is slicing big pepper.       A dog is moving its mouth.   \n",
      "\n",
      "     score  normed_score  \n",
      "744    0.0          0.00  \n",
      "745    0.8          0.16  \n",
      "746    0.0          0.00  \n",
      "747    0.0          0.00  \n",
      "748    0.0          0.00  \n",
      "18\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/2013/test-gold/STS.input.MSRvid.txt')\n",
    "file.readline()\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "df_msrvid_test = pd.DataFrame(data, columns=['sentence_A', 'sentence_B'])\n",
    "\n",
    "file = open('./data/2013/test-gold/STS.gs.MSRvid.txt')\n",
    "file.readline()\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.replace('\\n', '')\n",
    "    data.append(a)\n",
    "    \n",
    "df_msrvid_test['score'] = [float(x) for x in data]\n",
    "df_msrvid_test['normed_score'] = norm(df_msrvid_test['score'])\n",
    "\n",
    "print(df_msrvid_test.head())\n",
    "print(df_msrvid_test.tail())\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    if len(df_msrvid_test['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_test['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    if len(df_msrvid_test['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_test['sentence_B'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "df_msrvid_test['sentence_A'] = [x.lower() for x in df_msrvid_test['sentence_A']]\n",
    "df_msrvid_test['sentence_B'] = [x.lower() for x in df_msrvid_test['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_msrvid_test['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_msrvid_test['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 18:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 18:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_msrvid_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_msrvid_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mrpc_a = pickle.load(open('./data/train_a_msrvid_w2v300.data', 'rb'))\n",
    "train_mrpc_b = pickle.load(open('./data/train_b_msrvid_w2v300.data', 'rb'))\n",
    "\n",
    "test_mrpc_a = pickle.load(open('./data/test_a_msrvid_w2v300.data', 'rb'))\n",
    "test_mrpc_b = pickle.load(open('./data/test_b_msrvid_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.4)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=2.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0296\n",
      "Epoch 2/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0294\n",
      "Epoch 3/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0303\n",
      "Epoch 4/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0300\n",
      "Epoch 5/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0286\n",
      "Epoch 6/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0291\n",
      "Epoch 7/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0298\n",
      "Epoch 8/300\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0294\n",
      "Epoch 9/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0295\n",
      "Epoch 10/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0289\n",
      "Epoch 11/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0296\n",
      "Epoch 12/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0291\n",
      "Epoch 13/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0295\n",
      "Epoch 14/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0298\n",
      "Epoch 15/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0292\n",
      "Epoch 16/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0296\n",
      "Epoch 17/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0297\n",
      "Epoch 18/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0316\n",
      "Epoch 19/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0284\n",
      "Epoch 20/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 21/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0278\n",
      "Epoch 22/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0306\n",
      "Epoch 23/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0282\n",
      "Epoch 24/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0290\n",
      "Epoch 25/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 26/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0283\n",
      "Epoch 27/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 28/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0282\n",
      "Epoch 29/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0300\n",
      "Epoch 30/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0273\n",
      "Epoch 31/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0299\n",
      "Epoch 32/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 33/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 34/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0285\n",
      "Epoch 35/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0272\n",
      "Epoch 36/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0282\n",
      "Epoch 37/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0278\n",
      "Epoch 38/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0287\n",
      "Epoch 39/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0270\n",
      "Epoch 40/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 41/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0292\n",
      "Epoch 42/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0267\n",
      "Epoch 43/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0290\n",
      "Epoch 44/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0256\n",
      "Epoch 45/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0288\n",
      "Epoch 46/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0277\n",
      "Epoch 47/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0266\n",
      "Epoch 48/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0256\n",
      "Epoch 49/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0274\n",
      "Epoch 50/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 51/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0254\n",
      "Epoch 52/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 53/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0282\n",
      "Epoch 54/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0272\n",
      "Epoch 55/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 56/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0268\n",
      "Epoch 57/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0290\n",
      "Epoch 58/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 59/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0272\n",
      "Epoch 60/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0274\n",
      "Epoch 61/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0278\n",
      "Epoch 62/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0259\n",
      "Epoch 63/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0277\n",
      "Epoch 64/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0267\n",
      "Epoch 65/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0265\n",
      "Epoch 66/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 67/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 68/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0267\n",
      "Epoch 69/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0252\n",
      "Epoch 70/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0273\n",
      "Epoch 71/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0261\n",
      "Epoch 72/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0253\n",
      "Epoch 73/300\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0266\n",
      "Epoch 74/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0263\n",
      "Epoch 75/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 76/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0281\n",
      "Epoch 77/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0267\n",
      "Epoch 78/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0272\n",
      "Epoch 79/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0252\n",
      "Epoch 80/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 81/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 82/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0254\n",
      "Epoch 83/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0260\n",
      "Epoch 84/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 85/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0253\n",
      "Epoch 86/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0233\n",
      "Epoch 87/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0265\n",
      "Epoch 88/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0253\n",
      "Epoch 89/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0262\n",
      "Epoch 90/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0251\n",
      "Epoch 91/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0269\n",
      "Epoch 92/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0272\n",
      "Epoch 93/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 94/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0254\n",
      "Epoch 95/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 96/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 97/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0248\n",
      "Epoch 98/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0253\n",
      "Epoch 99/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0263\n",
      "Epoch 100/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0258\n",
      "Epoch 101/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0247\n",
      "Epoch 102/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 103/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0239\n",
      "Epoch 104/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 105/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 106/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 107/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0256\n",
      "Epoch 108/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 109/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 110/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0241\n",
      "Epoch 111/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 112/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0245\n",
      "Epoch 113/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 114/300\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0257\n",
      "Epoch 115/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 116/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0259\n",
      "Epoch 117/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 118/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0259\n",
      "Epoch 119/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0251\n",
      "Epoch 120/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0260\n",
      "Epoch 121/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 122/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0257\n",
      "Epoch 123/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 124/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0239\n",
      "Epoch 125/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0244\n",
      "Epoch 126/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0239\n",
      "Epoch 127/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 128/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 129/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 130/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0248\n",
      "Epoch 131/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 132/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 133/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0246\n",
      "Epoch 134/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 135/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0214\n",
      "Epoch 136/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0233\n",
      "Epoch 137/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0241\n",
      "Epoch 138/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0241\n",
      "Epoch 139/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0228\n",
      "Epoch 140/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 141/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0235\n",
      "Epoch 142/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 143/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 144/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0240\n",
      "Epoch 145/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0249\n",
      "Epoch 146/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 147/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0239\n",
      "Epoch 148/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0236\n",
      "Epoch 149/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0217\n",
      "Epoch 150/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0220\n",
      "Epoch 151/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 152/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 153/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 154/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0240\n",
      "Epoch 155/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0237\n",
      "Epoch 156/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0223\n",
      "Epoch 157/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0234\n",
      "Epoch 158/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 159/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 160/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 161/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 162/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0238\n",
      "Epoch 163/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0232\n",
      "Epoch 164/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0214\n",
      "Epoch 165/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0242\n",
      "Epoch 166/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 167/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 168/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 169/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0231\n",
      "Epoch 170/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 171/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0247\n",
      "Epoch 172/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0228\n",
      "Epoch 173/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 174/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 175/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0218\n",
      "Epoch 176/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 177/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0218\n",
      "Epoch 178/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0224\n",
      "Epoch 179/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 180/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0245\n",
      "Epoch 181/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 182/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 183/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0205\n",
      "Epoch 184/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 185/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0215\n",
      "Epoch 186/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 187/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0222\n",
      "Epoch 188/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0232\n",
      "Epoch 189/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 190/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0209\n",
      "Epoch 191/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 192/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0223\n",
      "Epoch 193/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0228\n",
      "Epoch 194/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0224\n",
      "Epoch 195/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0230\n",
      "Epoch 196/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0228\n",
      "Epoch 197/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 198/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0223\n",
      "Epoch 199/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0211\n",
      "Epoch 200/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0222\n",
      "Epoch 201/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0219\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0212\n",
      "Epoch 203/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 204/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0219\n",
      "Epoch 205/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0206\n",
      "Epoch 206/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 207/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 208/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 209/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0218\n",
      "Epoch 210/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0209\n",
      "Epoch 211/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0207\n",
      "Epoch 212/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 213/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0221\n",
      "Epoch 214/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0210\n",
      "Epoch 215/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0212\n",
      "Epoch 216/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0224\n",
      "Epoch 217/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0207\n",
      "Epoch 218/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0214\n",
      "Epoch 219/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0204\n",
      "Epoch 220/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0229\n",
      "Epoch 221/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0227\n",
      "Epoch 222/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200\n",
      "Epoch 223/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200\n",
      "Epoch 224/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0226\n",
      "Epoch 225/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0207\n",
      "Epoch 226/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0213\n",
      "Epoch 227/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0205\n",
      "Epoch 228/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0206\n",
      "Epoch 229/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0212\n",
      "Epoch 230/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0206\n",
      "Epoch 231/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0216\n",
      "Epoch 232/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 233/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0215\n",
      "Epoch 234/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0188\n",
      "Epoch 235/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 236/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0192\n",
      "Epoch 237/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0211\n",
      "Epoch 238/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 239/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0215\n",
      "Epoch 240/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0213\n",
      "Epoch 241/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0217\n",
      "Epoch 242/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0209\n",
      "Epoch 243/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0219\n",
      "Epoch 244/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0210\n",
      "Epoch 245/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0214\n",
      "Epoch 246/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0205\n",
      "Epoch 247/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0217\n",
      "Epoch 248/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 249/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0215\n",
      "Epoch 250/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0210\n",
      "Epoch 251/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0199\n",
      "Epoch 252/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 253/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0202\n",
      "Epoch 254/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0198\n",
      "Epoch 255/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0207\n",
      "Epoch 256/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 257/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0190\n",
      "Epoch 258/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0199\n",
      "Epoch 259/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0192\n",
      "Epoch 260/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0195\n",
      "Epoch 261/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0209\n",
      "Epoch 262/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0203\n",
      "Epoch 263/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0183\n",
      "Epoch 264/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200\n",
      "Epoch 265/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 266/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 267/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0199\n",
      "Epoch 268/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0219\n",
      "Epoch 269/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 270/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0202\n",
      "Epoch 271/300\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0214\n",
      "Epoch 272/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0195\n",
      "Epoch 273/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0208\n",
      "Epoch 274/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0200\n",
      "Epoch 275/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0217\n",
      "Epoch 276/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0198\n",
      "Epoch 277/300\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0191\n",
      "Epoch 278/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0193\n",
      "Epoch 279/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0212\n",
      "Epoch 280/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0197\n",
      "Epoch 281/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200\n",
      "Epoch 282/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0197\n",
      "Epoch 283/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0195\n",
      "Epoch 284/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0186\n",
      "Epoch 285/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0201\n",
      "Epoch 286/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0203\n",
      "Epoch 287/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0201\n",
      "Epoch 288/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0202\n",
      "Epoch 289/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0194\n",
      "Epoch 290/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0181\n",
      "Epoch 291/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0194\n",
      "Epoch 292/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0189\n",
      "Epoch 293/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0189\n",
      "Epoch 294/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0189\n",
      "Epoch 295/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0199\n",
      "Epoch 296/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0206\n",
      "Epoch 297/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200\n",
      "Epoch 298/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0194\n",
      "Epoch 299/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0203\n",
      "Epoch 300/300\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18dd44d0b8>"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_mrpc_a), np.array(train_mrpc_b)], \n",
    "                np.array(df_msrvid_train['normed_score']), \n",
    "                epochs=300, \n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.945222432927972\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_mrpc_a), np.array(train_mrpc_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_msrvid_train['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.6199107678512107\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_mrpc_a), np.array(test_mrpc_b)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_msrvid_test['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      sentence_A  \\\n",
      "0           measure the depth of a body of water   \n",
      "1             someone who plays a principal role   \n",
      "2           (cause to) come to rest on a surface   \n",
      "3   the act of advocating or promoting something   \n",
      "4  alphanumeric sequence used for identification   \n",
      "\n",
      "                                          sentence_B  score  normed_score  \n",
      "0                      any large deep body of water.    0.8          0.16  \n",
      "1               an actor who plays a principal role.    3.0          0.60  \n",
      "2                             reach or come to rest.    3.8          0.76  \n",
      "3                  the act of choosing or selecting.    2.2          0.44  \n",
      "4  a numeral or string of numerals that is used f...    3.8          0.76  \n",
      "                                       sentence_A  \\\n",
      "556         decide upon or fix definitely/specify   \n",
      "557                    provide a seat or base for   \n",
      "558  the act of explaining or defending something   \n",
      "559          an observer of a process or activity   \n",
      "560       make keen; give (new) life or energy to   \n",
      "\n",
      "                                            sentence_B  score  normed_score  \n",
      "556                     decide upon or fix definitely.    4.0          0.80  \n",
      "557                    provide a feast or banquet for.    1.4          0.28  \n",
      "558  the act of rending or ripping or splitting som...    0.0          0.00  \n",
      "559        a meeting devoted to a particular activity.    0.4          0.08  \n",
      "560                        give new life or energy to.    3.8          0.76  \n",
      "21\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/2013/onwn_test.txt')\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "df_msrvid_test = pd.DataFrame(data, columns=['sentence_A', 'sentence_B'])\n",
    "\n",
    "file = open('./data/2013/onwn_gs.txt')\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.replace('\\n', '')\n",
    "    data.append(a)\n",
    "    \n",
    "df_msrvid_test['score'] = [float(x) for x in data]\n",
    "df_msrvid_test['normed_score'] = norm(df_msrvid_test['score'])\n",
    "\n",
    "print(df_msrvid_test.head())\n",
    "print(df_msrvid_test.tail())\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    if len(df_msrvid_test['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_test['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    if len(df_msrvid_test['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_msrvid_test['sentence_B'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "df_msrvid_test['sentence_A'] = [x.lower() for x in df_msrvid_test['sentence_A']]\n",
    "df_msrvid_test['sentence_B'] = [x.lower() for x in df_msrvid_test['sentence_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_msrvid_test)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_msrvid_test['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_msrvid_test['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 22:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 22:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/test_a_on_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/test_b_on_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score                                         sentence_A  \\\n",
      "0   5.00            render one language in another language   \n",
      "1   3.25  nations unified by shared interests, history o...   \n",
      "2   3.25  convert into absorbable substances, (as if) wi...   \n",
      "3   4.00  devote or adapt exclusively to an skill, study...   \n",
      "4   3.25                   elevated wooden porch of a house   \n",
      "\n",
      "                                          sentence_B  normed_score  \n",
      "0  restate (words) from one language into another...          1.00  \n",
      "1        a group of nations having common interests.          0.65  \n",
      "2  soften or disintegrate by means of chemical ac...          0.65  \n",
      "3          devote oneself to a special area of work.          0.80  \n",
      "4         a porch that resembles the deck on a ship.          0.65  \n",
      "      score                                         sentence_A  \\\n",
      "3836   4.75  The bottom line is that when interest rates ri...   \n",
      "3837   4.75  A defeat on Alstom would have profound consequ...   \n",
      "3838   4.50  Tocqueville believed that there are no effecti...   \n",
      "3839   1.00  Will it give us the right to divorce the husba...   \n",
      "3840   3.25  But US stock prices fell only 5.2% between May...   \n",
      "\n",
      "                                             sentence_B  normed_score  \n",
      "3836  The key is that when interest rates go up, the...          0.95  \n",
      "3837  Losing on the issue of Alstom would have serio...          0.95  \n",
      "3838  Tocqueville thought that on the long run, noth...          0.90  \n",
      "3839                          A couple who have left?            0.20  \n",
      "3840  However, the Americans have accused that lower...          0.65  \n",
      "70\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "file = open('./data/2013/onwn_train.txt')\n",
    "data = list()\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1)\n",
    "\n",
    "file = open('./data/2013/msrpar_train.txt')\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1)\n",
    "\n",
    "file = open('./data/2013/msrpar_train2.txt')\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1)   \n",
    "file = open('./data/2013/europarl_train.txt')\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1) \n",
    "file = open('./data/2013/europarl_train2.txt')\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1) \n",
    "file = open('./data/2013/news_train.txt')\n",
    "for line in file:\n",
    "    a = line.split('\\t')\n",
    "    a = [x.replace('\\n', '') for x in a]\n",
    "    data.append(a)\n",
    "data.pop(len(data)-1)  \n",
    "df_onwn_train = pd.DataFrame(data, columns=['score', 'sentence_A', 'sentence_B'])\n",
    "df_onwn_train = df_onwn_train.iloc[:len(df_onwn_train)-1, :]\n",
    "df_onwn_train['score'] = [float(x) for x in df_onwn_train['score']]\n",
    "df_onwn_train['normed_score'] = norm(df_onwn_train['score'])\n",
    "\n",
    "print(df_onwn_train.head())\n",
    "print(df_onwn_train.tail())\n",
    "m = 0\n",
    "for i in range(len(df_onwn_train)):\n",
    "    if len(df_onwn_train['sentence_A'][i].split(' ')) > m:\n",
    "        m = len(df_onwn_train['sentence_A'][i].split(' '))\n",
    "print(m)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(df_onwn_train)):\n",
    "    if len(df_onwn_train['sentence_B'][i].split(' ')) > m:\n",
    "        m = len(df_onwn_train['sentence_B'][i].split(' '))\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAINING DATA\"\"\"\n",
    "# 57 is the longest sentence. so let's pad all with [0,..., 0] until len()==57\n",
    "all_vec_a = list()\n",
    "all_vec_b = list()\n",
    "for i in range(len(df_onwn_train)):\n",
    "    full_vec_a = list()\n",
    "    full_vec_b = list()\n",
    "    \n",
    "    for token in df_onwn_train['sentence_A'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_a.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for token in df_onwn_train['sentence_B'][i].split(' '):\n",
    "        try:\n",
    "            full_vec_b.append(word2vec[token].tolist())\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "    # Padding... we're using 100\n",
    "    while len(full_vec_a) < 72:\n",
    "        full_vec_a.append(np.zeros(300))\n",
    "    while len(full_vec_b) < 72:\n",
    "        full_vec_b.append(np.zeros(300))\n",
    "    all_vec_a.append(np.array(full_vec_a))\n",
    "    all_vec_b.append(np.array(full_vec_b))\n",
    "\n",
    "# Now we need to ensure that each \n",
    "with open('./data/train_a_on_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_a, f)\n",
    "    \n",
    "with open('./data/train_b_on_w2v300.data', 'wb') as f:\n",
    "    pickle.dump(all_vec_b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_vec_a\n",
    "del all_vec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_on = pickle.load(open('./data/train_a_on_w2v300.data', 'rb'))\n",
    "train_b_on = pickle.load(open('./data/train_b_on_w2v300.data', 'rb'))\n",
    "test_a_on = pickle.load(open('./data/test_a_on_w2v300.data', 'rb'))\n",
    "test_b_on = pickle.load(open('./data/test_b_on_w2v300.data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NETWORK DEFINITION\"\"\"\n",
    "input_shape = (None, 300,)\n",
    "left_input = tf.keras.layers.Input(input_shape)\n",
    "right_input = tf.keras.layers.Input(input_shape)\n",
    "siam = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(50, kernel_initializer='glorot_normal',\n",
    "#                          recurrent_initializer='glorot_normal',\n",
    "#                         #bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "#                         dropout=0.1)\n",
    "    tf.keras.layers.GRU(50, kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer=tf.keras.initializers.Constant(2.5),\n",
    "                        dropout=0.35)\n",
    "])\n",
    "\n",
    "encoded_l = siam(left_input)\n",
    "encoded_r = siam(right_input)\n",
    "manhattan = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
    "# manhattan = lambda x: tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(x[0] - x[1])))\n",
    "merged_mangattan = tf.keras.layers.Lambda(function=manhattan, output_shape=lambda x: x[0])([encoded_l, encoded_r])\n",
    "prediction = tf.keras.layers.Dense(1, activation='linear')(merged_mangattan)\n",
    "\n",
    "siamese_net = tf.keras.Model([left_input, right_input], prediction)\n",
    "\n",
    "\"\"\"OPTIMIZER AND LOSS DEFINITION\"\"\"\n",
    "siamese_net.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1,\n",
    "                                                           rho=0.9,\n",
    "                                                           clipvalue=2.5), \n",
    "                    loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "61/61 [==============================] - 1s 14ms/step - loss: 0.0095\n",
      "Epoch 2/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 3/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 4/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0090\n",
      "Epoch 5/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 6/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0101\n",
      "Epoch 7/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 8/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 9/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 10/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0100\n",
      "Epoch 11/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 12/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 13/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 14/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 15/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 16/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 17/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 18/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 19/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 20/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 21/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 22/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 23/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0096\n",
      "Epoch 24/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 25/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 26/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 27/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 28/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 29/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 30/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 31/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0089\n",
      "Epoch 32/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 33/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 34/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 35/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 36/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 37/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 38/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 39/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 40/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 41/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 42/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0098\n",
      "Epoch 43/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 44/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0091\n",
      "Epoch 45/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 46/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 47/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 48/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 49/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 50/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 51/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 52/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 53/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 54/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 55/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 56/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 57/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 58/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 59/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0100\n",
      "Epoch 60/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 61/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 62/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 63/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 64/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 65/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 66/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 67/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 68/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 69/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 70/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 71/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 72/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 73/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 74/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 75/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 76/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 77/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 78/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 79/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 80/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 81/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 82/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 83/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 84/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 85/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0098\n",
      "Epoch 86/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 87/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 88/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 89/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 90/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 91/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 92/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 93/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 94/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 95/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 96/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 97/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 98/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 99/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 100/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 101/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 102/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 103/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 104/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 105/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 106/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 107/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 108/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 109/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 110/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 111/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 112/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 113/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 114/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 115/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 116/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 117/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 118/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0089\n",
      "Epoch 119/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 120/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 121/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 122/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 123/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0097\n",
      "Epoch 124/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 125/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 126/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 127/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 128/1000\n",
      "61/61 [==============================] - 1s 14ms/step - loss: 0.0088\n",
      "Epoch 129/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0091\n",
      "Epoch 130/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 131/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 132/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 133/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 134/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 135/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 136/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 137/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 138/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 139/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 140/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 141/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 142/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 143/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 144/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 145/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 146/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 147/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 148/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 149/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 150/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0103\n",
      "Epoch 151/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 152/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 153/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 154/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 155/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 156/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 157/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 158/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 159/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 160/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 161/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 162/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 163/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 164/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 165/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 166/1000\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.008 - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 167/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 168/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 169/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 170/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 171/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 172/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 173/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 174/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 175/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 176/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 177/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 178/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 179/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0092\n",
      "Epoch 180/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 181/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 182/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 183/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 184/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 185/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 186/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 187/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 188/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 189/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 190/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 191/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 192/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 193/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 194/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 195/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 196/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 197/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 198/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 199/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 200/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 201/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 202/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 203/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 204/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 205/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 206/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 207/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 208/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 209/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 210/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 211/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 212/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 213/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 214/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 215/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 216/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 217/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 218/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 219/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 220/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 221/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 222/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 223/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 224/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 225/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 226/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 227/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 228/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 229/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 230/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 231/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 232/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 233/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 234/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 235/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 236/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 237/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 238/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 239/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0090\n",
      "Epoch 240/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 241/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 242/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 243/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 244/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 245/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 246/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 247/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 248/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 249/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 250/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 251/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 252/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 253/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 254/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 255/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 256/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 257/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 258/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 259/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 260/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 261/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 262/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 263/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 264/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 265/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 266/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0091\n",
      "Epoch 267/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 268/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 269/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 270/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 271/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 272/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 273/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 274/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 275/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 276/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 277/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 278/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 279/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 280/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 281/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 282/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 283/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 284/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 285/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 286/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 287/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 288/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 289/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 290/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 291/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 292/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 293/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 294/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 295/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 296/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 297/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 298/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 299/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 300/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 301/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 302/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 303/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 304/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 305/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 306/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 307/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 308/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 309/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 310/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 311/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 312/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 313/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 314/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 315/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 316/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0089\n",
      "Epoch 317/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 318/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 319/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 320/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 321/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 322/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 323/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 324/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 325/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 326/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 327/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 328/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 329/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 330/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 331/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 332/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 333/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 334/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 335/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 336/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 337/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0090\n",
      "Epoch 338/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 339/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 340/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 341/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 342/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 343/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 344/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 345/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 346/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 347/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 348/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 349/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 350/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 351/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 352/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 353/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 354/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0088\n",
      "Epoch 355/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 356/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 357/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 358/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 359/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 360/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 361/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 362/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0090\n",
      "Epoch 363/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 364/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 365/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 366/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 367/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 368/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 369/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 370/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 371/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 372/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 373/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 374/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 375/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 376/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 377/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 378/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 379/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 380/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 381/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 382/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 383/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 384/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 385/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 386/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 387/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 388/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 389/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 390/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 391/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 392/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 393/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 394/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 395/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 396/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 397/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 398/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 399/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 400/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 401/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 402/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 403/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 404/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 405/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 406/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 407/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 408/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 409/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 410/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 411/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 412/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 413/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 414/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 415/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 416/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 417/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 418/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 419/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 420/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 421/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 422/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 423/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 424/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 425/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 426/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 427/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 428/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 429/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 430/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 431/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 432/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 433/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 434/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 435/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 436/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 437/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 438/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 439/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 440/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 441/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 442/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 443/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 444/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 445/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0101\n",
      "Epoch 446/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 447/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 448/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 449/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 450/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 451/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 452/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 453/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0093\n",
      "Epoch 454/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 455/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 456/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 457/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 458/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 459/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 460/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 461/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 462/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 463/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 464/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 465/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 466/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 467/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 468/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 469/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 470/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 471/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 472/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 473/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 474/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 475/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 476/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0089\n",
      "Epoch 477/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 478/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 479/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 480/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 481/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 482/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 483/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 484/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 485/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 486/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 487/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 488/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 489/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 490/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 491/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0096\n",
      "Epoch 492/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 493/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 494/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 495/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 496/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 497/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 498/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 499/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 500/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 501/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 502/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 503/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 504/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 505/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 506/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 507/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 508/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 509/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 510/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 511/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 512/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 513/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 514/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 515/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 516/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 517/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 518/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 519/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 520/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 521/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 522/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 523/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 524/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 525/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 526/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 527/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 528/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 529/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 530/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 531/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 532/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 533/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 534/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 535/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 536/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 537/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0107\n",
      "Epoch 538/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 539/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 540/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 541/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 542/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 543/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 544/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 545/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 546/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 547/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 548/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 549/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 550/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 551/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 552/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 553/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 554/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 555/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 556/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 557/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 558/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 559/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0099\n",
      "Epoch 560/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 561/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 562/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 563/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 564/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 565/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 566/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 567/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0087\n",
      "Epoch 568/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 569/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 570/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 571/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 572/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 573/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 574/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 575/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 576/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 577/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 578/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 579/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 580/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 581/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 582/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 583/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 584/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 585/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 586/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 587/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 588/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 589/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 590/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 591/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 592/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 593/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 594/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 595/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 596/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 597/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 598/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 599/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 600/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 601/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 602/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 603/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 604/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 605/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 606/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 607/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 608/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 609/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 610/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 611/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 612/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 613/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 614/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 615/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 616/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 617/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0095\n",
      "Epoch 618/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 619/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 620/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 621/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 622/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 623/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 624/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 625/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 626/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 627/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 628/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 629/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 630/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 631/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 632/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 633/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 634/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 635/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 636/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 637/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 638/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 639/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 640/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 641/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 642/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 643/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 644/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 645/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 646/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 647/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 648/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 649/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 650/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 651/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 652/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 653/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 654/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 655/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 656/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 657/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 658/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 659/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 660/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 661/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 662/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 663/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 664/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 665/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0078\n",
      "Epoch 666/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 667/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 668/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 669/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 670/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 671/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 672/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 673/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0086\n",
      "Epoch 674/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 675/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 676/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 677/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 678/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 679/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 680/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 681/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 682/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 683/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 684/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 685/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 686/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 687/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 688/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 689/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 690/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 691/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 692/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 693/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 694/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 695/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 696/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 697/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 698/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 699/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 700/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 701/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 702/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 703/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 704/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 705/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 706/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 707/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 708/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 709/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 710/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 711/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 712/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 713/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 714/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 715/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 716/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 717/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 718/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 719/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 720/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 721/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 722/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 723/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 724/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 725/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 726/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 727/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 728/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 729/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 730/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 731/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 732/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 733/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 734/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 735/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 736/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 737/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 738/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 739/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 740/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 741/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 742/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 743/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 744/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 745/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 746/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0102\n",
      "Epoch 747/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 748/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 749/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 750/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 751/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 752/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 753/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 754/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 755/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 756/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 757/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 758/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 759/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 760/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 761/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 762/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 763/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 764/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 765/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 766/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 767/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 768/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 769/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 770/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 771/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 772/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 773/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 774/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 775/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 776/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 777/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 778/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 779/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 780/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 781/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 782/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 783/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 784/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 785/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0091\n",
      "Epoch 786/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 787/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0102\n",
      "Epoch 788/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 789/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 790/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 791/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 792/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 793/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 794/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0091\n",
      "Epoch 795/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 796/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 797/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 798/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 799/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 800/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0090\n",
      "Epoch 801/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 802/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 803/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 804/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 805/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 806/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 807/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 808/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 809/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 810/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 811/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 812/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 813/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 814/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 815/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 816/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 817/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 818/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 819/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 820/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 821/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 822/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 823/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 824/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 825/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0084\n",
      "Epoch 826/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0087\n",
      "Epoch 827/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 828/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0094\n",
      "Epoch 829/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0077\n",
      "Epoch 830/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 831/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 832/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 833/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 834/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 835/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 836/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0090\n",
      "Epoch 837/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 838/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 839/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 840/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 841/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 842/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 843/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0085\n",
      "Epoch 844/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 845/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 846/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 847/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 848/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 849/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 850/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 851/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 852/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 853/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 854/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 855/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 856/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 857/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0077\n",
      "Epoch 858/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 859/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 860/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0076\n",
      "Epoch 861/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 862/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 863/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 864/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 865/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 866/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 867/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0079\n",
      "Epoch 868/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0079\n",
      "Epoch 869/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 870/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 871/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 872/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 873/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 874/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 875/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 876/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 877/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0077\n",
      "Epoch 878/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 879/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 880/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 881/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 882/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 883/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 884/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 885/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 886/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 887/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 888/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 889/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 890/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 891/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 892/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 893/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 894/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 895/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 896/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 897/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0089\n",
      "Epoch 898/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 899/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 900/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 901/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 902/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 903/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 904/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 905/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 906/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 907/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 908/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 909/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 910/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 911/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 912/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 913/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 914/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 915/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0076\n",
      "Epoch 916/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 917/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 918/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 919/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 920/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 921/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 922/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 923/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 924/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 925/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 926/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 927/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 928/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 929/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 930/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 931/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 932/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 933/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 934/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 935/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0077\n",
      "Epoch 936/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 937/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 938/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 939/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 940/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 941/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 942/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 943/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 944/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 945/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0100\n",
      "Epoch 946/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 947/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 948/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 949/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 950/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 951/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0081\n",
      "Epoch 952/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 953/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0075\n",
      "Epoch 954/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 955/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 956/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0086\n",
      "Epoch 957/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0084\n",
      "Epoch 958/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0083\n",
      "Epoch 959/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 960/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 961/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 962/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 963/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 964/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 965/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 966/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 967/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 968/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 969/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 970/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 971/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0077\n",
      "Epoch 972/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0085\n",
      "Epoch 973/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 974/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 975/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 976/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0080\n",
      "Epoch 977/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 978/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 979/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 980/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 981/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0083\n",
      "Epoch 982/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0088\n",
      "Epoch 983/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 984/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 985/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 986/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 987/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 988/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 989/1000\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.0082\n",
      "Epoch 990/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 991/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 992/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0082\n",
      "Epoch 993/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 994/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 995/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0078\n",
      "Epoch 996/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n",
      "Epoch 997/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 998/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0080\n",
      "Epoch 999/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0081\n",
      "Epoch 1000/1000\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.0079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18cd8c9208>"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit([np.array(train_a_on), np.array(train_b_on)], \n",
    "                np.array(df_onwn_train['normed_score']), \n",
    "                epochs=1000, \n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.9518681340925692\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(train_a_on), np.array(train_b_on)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_onwn_train['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons: 0.3739439827734494\n"
     ]
    }
   ],
   "source": [
    "preds = siamese_net.predict([np.array(test_a_on), np.array(test_b_on)])\n",
    "print(f\"Pearsons: {pearsonr([x[0] for x in preds.tolist()], df_msrvid_test['normed_score'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6968001 ],\n",
       "       [0.58565545],\n",
       "       [0.7384194 ],\n",
       "       [0.707824  ],\n",
       "       [0.72572374],\n",
       "       [0.86133105],\n",
       "       [0.80435646],\n",
       "       [1.0348407 ],\n",
       "       [0.81854177],\n",
       "       [0.95961344],\n",
       "       [0.8118086 ],\n",
       "       [0.84415007],\n",
       "       [0.8636528 ],\n",
       "       [0.76268256],\n",
       "       [0.8501619 ],\n",
       "       [0.6906138 ],\n",
       "       [0.58342314],\n",
       "       [0.8176853 ],\n",
       "       [0.89324695],\n",
       "       [0.85177714],\n",
       "       [0.89327675],\n",
       "       [0.69813746],\n",
       "       [0.83152074],\n",
       "       [0.822     ],\n",
       "       [0.9182694 ],\n",
       "       [0.7909725 ],\n",
       "       [0.82397556],\n",
       "       [0.8577497 ],\n",
       "       [0.80100137],\n",
       "       [0.8373728 ],\n",
       "       [0.85571647],\n",
       "       [0.8789627 ],\n",
       "       [0.56602764],\n",
       "       [0.73429066],\n",
       "       [0.6067265 ],\n",
       "       [0.85177714],\n",
       "       [0.85177714],\n",
       "       [0.88596576],\n",
       "       [0.5841074 ],\n",
       "       [0.57653   ],\n",
       "       [0.6988808 ],\n",
       "       [0.7806492 ],\n",
       "       [0.85177714],\n",
       "       [0.90966016],\n",
       "       [0.82932055],\n",
       "       [0.8623606 ],\n",
       "       [0.7787011 ],\n",
       "       [0.8296257 ],\n",
       "       [0.82480186],\n",
       "       [0.7484144 ],\n",
       "       [0.72074604],\n",
       "       [0.7700014 ],\n",
       "       [0.97158605],\n",
       "       [0.7356235 ],\n",
       "       [0.71698624],\n",
       "       [0.8453913 ],\n",
       "       [0.86333334],\n",
       "       [0.66498613],\n",
       "       [0.85498595],\n",
       "       [0.8367911 ],\n",
       "       [0.5883433 ],\n",
       "       [0.9143251 ],\n",
       "       [0.80779815],\n",
       "       [0.5904394 ],\n",
       "       [0.6807829 ],\n",
       "       [0.7129611 ],\n",
       "       [0.8871647 ],\n",
       "       [0.7510246 ],\n",
       "       [0.82984984],\n",
       "       [0.6346793 ],\n",
       "       [0.8221052 ],\n",
       "       [0.7867447 ],\n",
       "       [0.5979227 ],\n",
       "       [0.7921257 ],\n",
       "       [0.66480255],\n",
       "       [0.897081  ],\n",
       "       [0.55623674],\n",
       "       [0.8119887 ],\n",
       "       [0.798819  ],\n",
       "       [0.78110486],\n",
       "       [0.89234996],\n",
       "       [0.7887665 ],\n",
       "       [0.8534172 ],\n",
       "       [0.8697689 ],\n",
       "       [0.84152496],\n",
       "       [0.5318589 ],\n",
       "       [0.84142935],\n",
       "       [0.8197493 ],\n",
       "       [0.8252289 ],\n",
       "       [0.71450883],\n",
       "       [0.8318163 ],\n",
       "       [0.6718043 ],\n",
       "       [0.7411632 ],\n",
       "       [0.67125916],\n",
       "       [0.7810375 ],\n",
       "       [0.74360377],\n",
       "       [0.7860565 ],\n",
       "       [0.79250264],\n",
       "       [0.8645557 ],\n",
       "       [0.6755527 ],\n",
       "       [0.8396333 ],\n",
       "       [0.6390694 ],\n",
       "       [0.8060621 ],\n",
       "       [0.79969686],\n",
       "       [0.75878024],\n",
       "       [0.9377161 ],\n",
       "       [0.5404957 ],\n",
       "       [0.55794907],\n",
       "       [0.7875133 ],\n",
       "       [0.8945781 ],\n",
       "       [0.751012  ],\n",
       "       [0.4806841 ],\n",
       "       [0.87119853],\n",
       "       [0.7349924 ],\n",
       "       [0.8778718 ],\n",
       "       [0.8589025 ],\n",
       "       [0.73300636],\n",
       "       [0.7072538 ],\n",
       "       [0.76278275],\n",
       "       [0.7158439 ],\n",
       "       [0.89167905],\n",
       "       [0.83787847],\n",
       "       [0.6860353 ],\n",
       "       [0.84538573],\n",
       "       [0.70367503],\n",
       "       [0.90378207],\n",
       "       [0.771333  ],\n",
       "       [0.6438352 ],\n",
       "       [0.8340405 ],\n",
       "       [0.614468  ],\n",
       "       [0.7973354 ],\n",
       "       [0.88964605],\n",
       "       [0.81382686],\n",
       "       [0.7281353 ],\n",
       "       [0.75613314],\n",
       "       [0.8218254 ],\n",
       "       [0.74866956],\n",
       "       [0.7629408 ],\n",
       "       [0.59441173],\n",
       "       [0.7328148 ],\n",
       "       [0.6430576 ],\n",
       "       [0.88625073],\n",
       "       [0.9338288 ],\n",
       "       [0.8168319 ],\n",
       "       [0.8558386 ],\n",
       "       [0.73185754],\n",
       "       [0.8663791 ],\n",
       "       [1.0040145 ],\n",
       "       [0.75456846],\n",
       "       [0.70796365],\n",
       "       [0.80778587],\n",
       "       [0.79419565],\n",
       "       [0.42917022],\n",
       "       [0.7857985 ],\n",
       "       [0.92339796],\n",
       "       [0.73833346],\n",
       "       [0.83209527],\n",
       "       [0.7647924 ],\n",
       "       [0.9062977 ],\n",
       "       [0.7588572 ],\n",
       "       [0.3304072 ],\n",
       "       [0.7051127 ],\n",
       "       [0.8548268 ],\n",
       "       [0.7298093 ],\n",
       "       [0.8674136 ],\n",
       "       [0.8029458 ],\n",
       "       [0.59364426],\n",
       "       [0.7635006 ],\n",
       "       [0.57565665],\n",
       "       [0.6142821 ],\n",
       "       [0.9294992 ],\n",
       "       [0.63209635],\n",
       "       [0.8520407 ],\n",
       "       [0.99098504],\n",
       "       [0.7569308 ],\n",
       "       [0.80281025],\n",
       "       [0.76172894],\n",
       "       [0.8463734 ],\n",
       "       [0.8107124 ],\n",
       "       [0.00334591],\n",
       "       [0.77688456],\n",
       "       [0.3914456 ],\n",
       "       [0.7621368 ],\n",
       "       [0.9761821 ],\n",
       "       [0.9333209 ],\n",
       "       [0.9612463 ],\n",
       "       [0.5168539 ],\n",
       "       [0.86455095],\n",
       "       [0.7891778 ],\n",
       "       [0.93071395],\n",
       "       [0.8806198 ],\n",
       "       [0.8348543 ],\n",
       "       [0.9273338 ],\n",
       "       [0.9565699 ],\n",
       "       [0.57507575],\n",
       "       [0.8701906 ],\n",
       "       [0.94212514],\n",
       "       [0.6933896 ],\n",
       "       [0.8928648 ],\n",
       "       [0.52249366],\n",
       "       [0.76211727],\n",
       "       [0.6242608 ],\n",
       "       [0.876889  ],\n",
       "       [0.6929008 ],\n",
       "       [0.7219894 ],\n",
       "       [0.81995976],\n",
       "       [0.502203  ],\n",
       "       [0.6559595 ],\n",
       "       [0.94205993],\n",
       "       [1.001428  ],\n",
       "       [0.8671689 ],\n",
       "       [0.65844876],\n",
       "       [0.71758795],\n",
       "       [0.6622758 ],\n",
       "       [0.9108598 ],\n",
       "       [0.6489952 ],\n",
       "       [0.759572  ],\n",
       "       [0.7825956 ],\n",
       "       [0.975674  ],\n",
       "       [0.87603605],\n",
       "       [0.84817237],\n",
       "       [0.73634124],\n",
       "       [0.8269645 ],\n",
       "       [0.61262834],\n",
       "       [0.6542725 ],\n",
       "       [0.48356572],\n",
       "       [0.72716314],\n",
       "       [0.67619205],\n",
       "       [0.89339274],\n",
       "       [0.7820167 ],\n",
       "       [0.6402093 ],\n",
       "       [0.8259554 ],\n",
       "       [0.6182145 ],\n",
       "       [0.4451601 ],\n",
       "       [0.8335868 ],\n",
       "       [0.86165124],\n",
       "       [0.794988  ],\n",
       "       [0.7747095 ],\n",
       "       [0.784627  ],\n",
       "       [0.7992364 ],\n",
       "       [0.76449525],\n",
       "       [0.77602506],\n",
       "       [0.6989783 ],\n",
       "       [0.46661913],\n",
       "       [0.6502861 ],\n",
       "       [0.85177714],\n",
       "       [0.91944396],\n",
       "       [0.9228794 ],\n",
       "       [0.77102774],\n",
       "       [0.751278  ],\n",
       "       [0.727756  ],\n",
       "       [0.67273617],\n",
       "       [0.82775295],\n",
       "       [0.771858  ],\n",
       "       [0.91762984],\n",
       "       [0.79119754],\n",
       "       [0.6952684 ],\n",
       "       [0.86929065],\n",
       "       [0.8336576 ],\n",
       "       [0.8478093 ],\n",
       "       [0.7453719 ],\n",
       "       [1.013562  ],\n",
       "       [0.6397172 ],\n",
       "       [0.67907834],\n",
       "       [0.70112836],\n",
       "       [0.82850415],\n",
       "       [0.86725134],\n",
       "       [0.8217867 ],\n",
       "       [0.86006254],\n",
       "       [0.8145319 ],\n",
       "       [0.91327375],\n",
       "       [0.73487526],\n",
       "       [0.79189587],\n",
       "       [0.57363814],\n",
       "       [0.7613221 ],\n",
       "       [0.8104023 ],\n",
       "       [0.89709455],\n",
       "       [0.6879326 ],\n",
       "       [0.7647288 ],\n",
       "       [0.78931475],\n",
       "       [0.6363358 ],\n",
       "       [0.87049025],\n",
       "       [0.7636276 ],\n",
       "       [0.8046974 ],\n",
       "       [0.7909725 ],\n",
       "       [0.6109543 ],\n",
       "       [0.7559972 ],\n",
       "       [0.7965566 ],\n",
       "       [0.8079711 ],\n",
       "       [0.7483139 ],\n",
       "       [0.7948447 ],\n",
       "       [0.87900275],\n",
       "       [0.53680146],\n",
       "       [0.75159425],\n",
       "       [0.8656951 ],\n",
       "       [0.7046435 ],\n",
       "       [0.58850217],\n",
       "       [0.77869624],\n",
       "       [0.9436552 ],\n",
       "       [0.53772247],\n",
       "       [0.6921333 ],\n",
       "       [0.4695677 ],\n",
       "       [0.817971  ],\n",
       "       [0.7014319 ],\n",
       "       [0.86122715],\n",
       "       [0.77878606],\n",
       "       [0.84965646],\n",
       "       [0.8591582 ],\n",
       "       [0.75197715],\n",
       "       [0.89600396],\n",
       "       [0.881718  ],\n",
       "       [0.9622983 ],\n",
       "       [0.9488426 ],\n",
       "       [0.88154274],\n",
       "       [0.8570783 ],\n",
       "       [0.60394704],\n",
       "       [0.5699142 ],\n",
       "       [0.7112106 ],\n",
       "       [0.7599985 ],\n",
       "       [0.7644062 ],\n",
       "       [0.64453685],\n",
       "       [0.4233931 ],\n",
       "       [0.83943826],\n",
       "       [0.69249064],\n",
       "       [0.81105745],\n",
       "       [0.7604706 ],\n",
       "       [0.8556847 ],\n",
       "       [0.8948626 ],\n",
       "       [0.8500633 ],\n",
       "       [0.7189942 ],\n",
       "       [0.79432136],\n",
       "       [1.0556849 ],\n",
       "       [0.756616  ],\n",
       "       [0.8515619 ],\n",
       "       [0.74953306],\n",
       "       [0.9470831 ],\n",
       "       [0.9096019 ],\n",
       "       [0.8021784 ],\n",
       "       [0.76940805],\n",
       "       [0.8632147 ],\n",
       "       [0.73466796],\n",
       "       [0.929347  ],\n",
       "       [0.74267924],\n",
       "       [0.9011619 ],\n",
       "       [0.7913936 ],\n",
       "       [0.83901167],\n",
       "       [0.7066901 ],\n",
       "       [0.908726  ],\n",
       "       [0.8760486 ],\n",
       "       [0.680115  ],\n",
       "       [0.9230653 ],\n",
       "       [0.8256611 ],\n",
       "       [0.6759003 ],\n",
       "       [0.7552948 ],\n",
       "       [0.7993097 ],\n",
       "       [0.55027616],\n",
       "       [0.78135604],\n",
       "       [0.7263501 ],\n",
       "       [0.9619592 ],\n",
       "       [0.6863358 ],\n",
       "       [0.6626874 ],\n",
       "       [0.80882037],\n",
       "       [0.8146936 ],\n",
       "       [0.6908652 ],\n",
       "       [0.797787  ],\n",
       "       [0.95312184],\n",
       "       [0.7748709 ],\n",
       "       [0.69617647],\n",
       "       [0.758369  ],\n",
       "       [0.843203  ],\n",
       "       [0.86585706],\n",
       "       [0.622249  ],\n",
       "       [0.7651007 ],\n",
       "       [0.78667337],\n",
       "       [0.7651131 ],\n",
       "       [0.5492022 ],\n",
       "       [0.8496017 ],\n",
       "       [0.94179547],\n",
       "       [0.8926391 ],\n",
       "       [0.72689575],\n",
       "       [0.6944196 ],\n",
       "       [0.91997606],\n",
       "       [0.8178319 ],\n",
       "       [0.8698235 ],\n",
       "       [0.74946654],\n",
       "       [0.77897906],\n",
       "       [0.8741663 ],\n",
       "       [0.87985134],\n",
       "       [0.69270283],\n",
       "       [0.68066484],\n",
       "       [0.77257764],\n",
       "       [0.78492224],\n",
       "       [0.5928786 ],\n",
       "       [0.919868  ],\n",
       "       [0.73716325],\n",
       "       [0.8300136 ],\n",
       "       [0.6260144 ],\n",
       "       [0.8324125 ],\n",
       "       [0.95905596],\n",
       "       [0.9313142 ],\n",
       "       [0.6509704 ],\n",
       "       [0.954237  ],\n",
       "       [0.84743714],\n",
       "       [0.72777265],\n",
       "       [0.7433992 ],\n",
       "       [0.7909725 ],\n",
       "       [0.8396253 ],\n",
       "       [0.84870183],\n",
       "       [0.41653904],\n",
       "       [0.6832734 ],\n",
       "       [0.75737053],\n",
       "       [0.8188073 ],\n",
       "       [0.70956576],\n",
       "       [0.9196762 ],\n",
       "       [0.83696467],\n",
       "       [0.9211358 ],\n",
       "       [0.802893  ],\n",
       "       [0.88237906],\n",
       "       [0.9223075 ],\n",
       "       [0.6971655 ],\n",
       "       [0.821872  ],\n",
       "       [0.68250525],\n",
       "       [0.6612846 ],\n",
       "       [0.84260267],\n",
       "       [0.790496  ],\n",
       "       [0.7325117 ],\n",
       "       [0.73721945],\n",
       "       [0.86520684],\n",
       "       [0.8467567 ],\n",
       "       [0.79535145],\n",
       "       [0.879531  ],\n",
       "       [0.87439305],\n",
       "       [0.85662556],\n",
       "       [0.71260935],\n",
       "       [0.7341456 ],\n",
       "       [0.6958742 ],\n",
       "       [0.8786186 ],\n",
       "       [0.6090473 ],\n",
       "       [0.70784974],\n",
       "       [0.93103343],\n",
       "       [0.6958078 ],\n",
       "       [0.70228887],\n",
       "       [0.9105072 ],\n",
       "       [0.78429574],\n",
       "       [0.82956195],\n",
       "       [0.8251547 ],\n",
       "       [0.94477665],\n",
       "       [0.59951794],\n",
       "       [0.7408513 ],\n",
       "       [0.80589837],\n",
       "       [0.91167897],\n",
       "       [0.7942394 ],\n",
       "       [0.72877717],\n",
       "       [0.76546013],\n",
       "       [0.85177714],\n",
       "       [0.9306976 ],\n",
       "       [0.8180913 ],\n",
       "       [0.99607503],\n",
       "       [0.8296008 ],\n",
       "       [0.84930736],\n",
       "       [0.8655022 ],\n",
       "       [0.7788084 ],\n",
       "       [0.8785475 ],\n",
       "       [0.44847852],\n",
       "       [0.7164345 ],\n",
       "       [0.5829351 ],\n",
       "       [0.6443147 ],\n",
       "       [0.88443923],\n",
       "       [0.83064026],\n",
       "       [0.8369091 ],\n",
       "       [0.7337757 ],\n",
       "       [0.7755308 ],\n",
       "       [0.8494681 ],\n",
       "       [0.8230136 ],\n",
       "       [0.59403217],\n",
       "       [0.81245244],\n",
       "       [0.88041854],\n",
       "       [0.86061   ],\n",
       "       [0.7641019 ],\n",
       "       [0.90923   ],\n",
       "       [0.7682353 ],\n",
       "       [0.81820697],\n",
       "       [0.88758373],\n",
       "       [0.50769496],\n",
       "       [0.8638845 ],\n",
       "       [0.7963833 ],\n",
       "       [0.7541347 ],\n",
       "       [0.7553716 ],\n",
       "       [0.71972835],\n",
       "       [0.8986092 ],\n",
       "       [0.73336357],\n",
       "       [0.86156905],\n",
       "       [0.7883681 ],\n",
       "       [0.7299741 ],\n",
       "       [0.6888161 ],\n",
       "       [0.53789365],\n",
       "       [0.66486543],\n",
       "       [0.7322775 ],\n",
       "       [0.80746305],\n",
       "       [0.66265774],\n",
       "       [0.5884985 ],\n",
       "       [0.70774615],\n",
       "       [0.7702606 ],\n",
       "       [0.629625  ],\n",
       "       [0.7587187 ],\n",
       "       [0.7812234 ],\n",
       "       [0.7292861 ],\n",
       "       [0.6087175 ],\n",
       "       [0.8854753 ],\n",
       "       [0.7442467 ],\n",
       "       [0.79743874],\n",
       "       [0.6807545 ],\n",
       "       [0.72423375],\n",
       "       [0.82669514],\n",
       "       [0.60579145],\n",
       "       [0.76080155],\n",
       "       [0.76992804],\n",
       "       [0.74590516],\n",
       "       [0.70762944],\n",
       "       [0.7993834 ],\n",
       "       [0.6117108 ],\n",
       "       [0.80054915],\n",
       "       [0.56546956],\n",
       "       [0.84048766],\n",
       "       [0.88571924],\n",
       "       [0.79952395],\n",
       "       [0.89106023],\n",
       "       [0.684191  ],\n",
       "       [0.835029  ],\n",
       "       [0.7055923 ],\n",
       "       [0.79619116],\n",
       "       [0.79351467],\n",
       "       [0.73061776],\n",
       "       [1.0119313 ],\n",
       "       [0.96760565],\n",
       "       [0.7245431 ],\n",
       "       [0.5800141 ],\n",
       "       [0.85177714],\n",
       "       [0.81289726],\n",
       "       [0.7899005 ],\n",
       "       [0.6787855 ],\n",
       "       [0.7778475 ],\n",
       "       [0.84233207],\n",
       "       [0.9749791 ],\n",
       "       [0.906214  ],\n",
       "       [0.82284206],\n",
       "       [0.79889935],\n",
       "       [0.78086925],\n",
       "       [0.75742036],\n",
       "       [0.78672194],\n",
       "       [0.8503888 ],\n",
       "       [0.82974607],\n",
       "       [0.63902867],\n",
       "       [0.79175985],\n",
       "       [0.7296225 ],\n",
       "       [0.81996214],\n",
       "       [0.85177714],\n",
       "       [1.0007536 ],\n",
       "       [0.94645023],\n",
       "       [0.9400133 ],\n",
       "       [0.8533765 ]], dtype=float32)"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3305256639303634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.16\n",
       "1      0.60\n",
       "2      0.76\n",
       "3      0.44\n",
       "4      0.76\n",
       "       ... \n",
       "556    0.80\n",
       "557    0.28\n",
       "558    0.00\n",
       "559    0.08\n",
       "560    0.76\n",
       "Name: normed_score, Length: 561, dtype: float64"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_msrvid_test['normed_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
